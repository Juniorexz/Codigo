{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFTO/JrKQLTWzjF8ZILAFp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juniorexz/Codigo/blob/master/Redes%20Neurais_%20Deep%20Learning%20com%20PyTorch%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "v_BVncYvFH2y",
        "outputId": "11b4b9cb-3c24-4b12-8bab-2488a36f8c0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-82d05d1aafb7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Vamos aproveitar que estamos falando da história para falar também da evolução do modelo neural ao longo dos anos, porque uma coisa interessante de ver é que a história das redes neurais começa em 1940.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos aproveitar que estamos falando da história para falar também da evolução do modelo neural ao longo dos anos, porque uma coisa interessante de ver é que a história das redes neurais começa em 1940.\n",
        "\n",
        "[00:19] Então vamos ver desde o primeiro modelo neural, que foi há mais de 70 anos atrás, passando pelo perceptron, que é o neurônio que usamos até os dias atuais, até o Problema do XOR, que foi um dos principais limitadores, marcou bem a história das redes neurais, passando pelo Multi-Layer Perceptron, que é a rede efetivamente, com múltiplas camadas, até o conceito de deep learning, que se popularizou tanto.\n",
        "\n",
        "[00:46] Então o primeiro modelo neural foi proposto por dois pesquisadores, o primeiro deles um neurocientista, o Warren McCulloch, que queria simular um cérebro humano, só que de forma bem realista, ele queria um modelo computacional que fosse inspirado no funcionamento do cérebro humano. E aí ele fez uma parceria com o Walter Pitts, um lógico, um profissional de lógica computacional, que juntos conseguiram chegar em um modelo efetivo e palpável e possível de ser desenvolvido.\n",
        "\n",
        "[01:20] Só para entendermos de onde vem a inspiração, vamos entender por alto o neurônio biológico, que é como o nosso cérebro funciona. O neurônio é dividido em 3 componentes principais: Os dendritos, que são os receptores de sinal, então dos bilhões de neurônios do seu cérebro, todos eles têm receptores, que vão receber estímulos de outros neurônios, então eles recebem os sinais, o corpo processa os sinais recebidos do próprio neurônio e o axônio transmite o sinal para os próximos neurônios.\n",
        "\n",
        "[01:54] E o legal do axônio é que ele pode ou não ativar a depender do estímulo que ele recebeu, nem sempre ele vai passar a informação adiante. Se ele achar que aquela informação vai morrer ali, não vai passar adiante, ele não vai transmitir aquele sinal.\n",
        "\n",
        "[02:09] Então esses neurônios são combinados em uma rede, são bilhões de neurônios conectados formando padrões. Então quando você ouve uma música boa, sua música favorita, você vai ter um padrão de ativação, um conjunto de neurônios vai ativar. Já quando você ouve um barulho estridente, um garfo arranhando um prato, você vai ter um outro padrão de ativação, diferentes neurônios vão ativar, de formas diferentes.\n",
        "\n",
        "[02:37] E o mesmo acontece com as redes neurais, elas vão aprender padrões de ativação, conjuntos de neurônios que vão ativar de formar diferentes, a depender do estímulo que elas receberem, ou seja, da entrada delas.\n",
        "\n",
        "[02:50] Então o primeiro modelo neural era bem isso, ele tinha dendritos, ou seja, ele recebia entradas, mas vale destacar aqui brevemente que eram entradas binárias, então x varia entre 0 e 1. E ele tinha um corpo, ou seja, ele agregava essas funções, e era uma simples soma, então se temos aqui x1, x2, x3, esse corpo, esse g(x), ele fazia x1+x2+x3. E ele tinha um axônio que é essa função f, que decidia se o neurônio ia ativar ou não, conforme a saída de g.\n",
        "\n",
        "[03:38] Então a saída também era binária, ele ativava ou não ativava a depender da saída de g, então ele definia um limiar. Se g(x), por exemplo, for maior que o limiar l, o neurônio vai ativar, se não ele não vai ativar. Então era um modelo bem simples e só com o ajuste do limiar de f já era possível realizar diferentes funções.\n",
        "\n",
        "[04:01] Eu trouxe aqui exemplos mais simples, era possível realizar operações mais completas, cálculos até, usando esse tipo de neurônio. Mas os exemplos mais simples são a operação lógica do i. Então você ajustando o limiar para g(x) maior ou igual que 2, você está fazendo uma porta lógica i. Porque se x1=1 e x2 = 1, então g(x)=2, ou seja, vai atender o critério e o neurônio vai ativar.\n",
        "\n",
        "[04:35] Já, por outro lado, se um deles for igual a 0 e o outro for igual a 1, g(x) vai ser igual a 1 e o neurônio não vai ativar, porque o critério não é atendido. Então só ajustando o valor para o limiar ser igual a 2, construímos uma porta lógica i. O mesmo para porta lógica ou que é se g(x) for maior ou igual q1. Ou seja, se x1 ou x2 for igual a 1, ele vai ativar.\n",
        "\n",
        "[05:05] O Frank Rosenblatt propõe uma versão melhorada do modelo, simplesmente atribuindo pesos a cada entrada e permitindo que esses pesos sejam otimizados através de algoritmos. Então ele mudou o que tínhamos antes, que era g(x)= x1+x2, enfim, mais x3 em diante, mudou para g(x) = w1x1+w2x2 e daí por diante. Então, esses pesos foram o grande acréscimo que o perceptron trouxe e o fato de podermos otimizar os W’s foi a grande diferença que permitiu criarmos modelos tão versáteis, é só ajustar os pesos, que o mesmo modelo, o mesmo neurônio pode realizar diferentes funções.\n",
        "\n",
        "[05:59] Então diferente do anterior, que você tinha que manualmente definir valor, definir coisas, aqui nós temos o aprendizado. Então o Frank Rosenblatt introduziu pela primeira vez o aprendizado nos modelos neurais. Já existia aprendizado de máquina de outras formas, mas no contexto de redes neurais, o Frank Rosenblatt introduziu isso.\n",
        "\n",
        "[06:26] Na prática, o modelo do perceptron, a rede de perceptrons, ele era um hardware, isso que eu acho mais bacana, ele fez com que cada, cada conexão do neurônio fosse um potenciômetro, que deixava, mais ou menos, tensão passar. Então se o potenciômetro estava mais solto, ou seja, passava mais tensão, ele dava um peso maior para aquela conexão. Se o potenciômetro estivesse barrando a tensão, ou seja, pouca tensão passava, ele estava dando pouca importância para aquela conexão.\n",
        "\n",
        "[07:02] Então ele criou dessa forma uma rede capaz de detectar padrões visuais, então ele estava conectado com uma câmera e cada nova imagem que ele via, se ele visse um triângulo, ele ativava, se ele visse outras formas geométricas, ele não ativava. Então ele fez isso em um hardware que está lá no Smithsonian e que eu sou louca para conhecer, só queria compartilhar esse amor com vocês.\n",
        "\n",
        "[07:26] Em 1969, o Marvin Minky e o Seymour Papert resolveram destacar as já conhecidas limitações do perceptron, então eles queriam mostrar que o perceptron não era tudo isso e escreveram um livro falando sobre as limitações das redes neurais. O livro está linkado para quem quiser olhar os slides depois. Mas a grande limitação é que perceptrons só aprendem funções lineares, só aprendem retas. Então se você tem problema de classificação linear, que são dois conjuntos que você precisa separar, se eles são separáveis por uma reta, então um perceptron resolve.\n",
        "\n",
        "[08:04] Ou uma regressão linear que é só um conjunto e você precisa ajustar uma função a esse conjunto. Se você consegue ajustar uma reta, o perceptron resolve. Agora, se você precisar de funções mais complexas, funções não-lineares, aí o perceptron não resolve. E o que ficou marcado na história foi o “ou exclusivo”, a operação do XOR, para quem conhece já da lógica matemática.\n",
        "\n",
        "[08:33] O XOR é verdadeiro e é o que está destacado no canto superior esquerdo, quando as duas entradas são diferentes, é um “ou exclusivo”, os dois eventos não podem ser iguais. Então eles são verdadeiros quando os eventos diferem e são falsos quando os eventos são iguais. Então não existe reta que separe essas duas classes, nós precisaríamos de uma função não-linear para solucionar esse problema e perceptrons não fazem isso.\n",
        "\n",
        "[09:01] Essa época foi o início do inverno da I.A., desde a popularização do livro, do Minsky e do Papert e esse inverno durou de 69 até 86, quando surgiu o Geoffrey Hinton, com o novo algoritmo de otimização desses pesos, o Backpropagation. Então o Geoffrey Hinton propôs que se você coloca múltiplas camadas de neurônios, e daí vem o aprendizado profundo, com muitas camadas, o algoritmo dele, o Backpropagation, consegue ajustar essas funções mais complexas, essas funções não-lineares.\n",
        "\n",
        "[09:37] Então aqui está um exemplo de fronteira de decisão com uma rede de múltiplas camadas, usando o Backpropagation, que é o algoritmo do Geoffrey Hinton. Então você vê que a rede com múltiplas camadas consegue aprender uma função não-linear, essa função com a forma mais torta, uma curva. Ela consegue aprender esse tipo de função para separar as duas classes.\n",
        "\n",
        "[10:06] Então estava tudo dando tão certo, que foi postulado o Teorema da Aproximação Universal, dizendo que uma rede feed forward, que é uma rede simples, da entrada para a saída, com apenas uma camada escondida, o que significa que ela tem duas camadas, uma escondida e uma camada de saída, vamos entrar em detalhes sobre isso no curso sobre redes neurais. Uma rede neural com uma camada escondida é suficiente para representar qualquer função. Aí tem os poréns, a camada pode ser invariavelmente grande, pode falhar em aprender, porque tem requisitos específicos para que essa teoria funcione.\n",
        "\n",
        "[10:46] Mas teoricamente, ela pode representar qualquer função, inclusive as mais complexas. Se existe uma função, existe uma rede que resolve, teoricamente.\n",
        "\n",
        "[10:58] Então, apesar de ter definido que teoricamente era possível modelar qualquer função, na época não era possível validar essa afirmação. Por quê? Grandes modelos neurais são computacionalmente custosos, ou seja, não tínhamos hardware para testar essa afirmação, apesar do Geoffrey Hinton ter publicado. “Olha só, funciona. Teoricamente funciona para qualquer tipo de função.”\n",
        "\n",
        "[11:22] As pessoas continuaram falando, “Nós não temos hardware para confirmar se você tem razão.” E é necessário um grande volume de dados para treinar um grande modelo, nós não tínhamos dados. Então não era possível validar com certeza que o modelo era tão versátil e tão poderoso assim. Por isso ficou por um tempo parado também, a pesquisa, não tínhamos como testar, então não vale a pena perseguir isso.\n",
        "\n",
        "[11:45] Finalmente em 2006, quando surge o deep learning como algo popular, o aprendizado profundo, nós tínhamos não só a teoria, como hardware robusto, as GPU’s que mudaram tudo no cenário de redes neurais e abundância de dados, para conseguirmos fazer modelos profundos o suficiente e grandes o suficiente, capazes o suficiente para solucionar problemas mais complexos, como classificação de imagens, isso vale a pena visitar esse link depois, porque é uma rede rodando em tempo real no seu browser para fazer classificação de imagem.\n",
        "\n",
        "[12:23] Transferência de estilo, também vale muito a pena visitar esse projeto depois, porque é uma rede capaz de dada um vídeo original, que é um cavalo, ele consegue transferir o estilo de uma zebra para um cavalo do vídeo. Ele transformou o cavalo em uma zebra.\n",
        "\n",
        "[12:44] E modelos de linguagem. Vou ser redundante, vale super a pena visitar esse link depois, porque é um modelo capaz de completar textos. Então dado que você entra com um parágrafo na sua rede, a rede vai te dizer o complemento daquele texto. Vai contar uma história, inventar uma história sobre aquele texto de entrada que você deu.\n",
        "\n",
        "[13:08] Então estamos atualmente no ponto em que soluções com redes neurais estão no ápice, tudo é feito com redes neurais, quer dizer, as coisas mais populares hoje em dia são feitas com redes neurais, isso afeta tanto o mercado quanto a academia. Então tem se usado muito. Mas tem gente que diz que é só uma febre e que vai passar. Então eu deixo aí essa reflexão para concluirmos. O que você acha? Do que você conhece de redes neurais, do que você veio a conhecer agora, você acha que é só mais uma hype ou que tem futuro mesmo?Durante a aula, foram apresentados alguns exemplos de soluções com Deep Learning. A escolha desses exemplos foi pensada para trazer algo a mais, além de ilustrar o potencial abrangente das redes neurais. Vamos conferir a seguir cada um desses exemplos.\n",
        "\n",
        "Classificação de imagens. Vimos um gif retirado do curso CS231n - Convolutional Neural Networks for Visual Recognition (Redes Neurais Convolucionais para Reconhecimento Visual). O que você vai encontrar nessa página é uma rede neural rodando ao vivo no seu browser! O CS231n é um curso de Stanford que está disponível online em texto, ou em vídeo. É uma das principais referências online para aprender sobre redes convolucionais.\n",
        "\n",
        "Transferência de estilo. Essa área também é conhecida como tradução de imagens. Assim como traduzimos um textos do inglês para o português, podemos também traduzir uma foto tirada no verão para a sua versão correspondente do inverno. Apresentamos na aula a tradução da imagem de um cavalo para uma zebra, retirada da página do github que traz a implementação da CycleGAN, à qual foi aplicada inúmeras outras traduções. É a tradução de imagens que permite transformar o seu rosto no rosto de uma criança, ou em um gênero diferente do seu, sendo uma das aplicações que mais chama a atenção do público geral.\n",
        "\n",
        "Modelo de Linguagem. Por fim, apresentamos o GPT-2, um Modelo de Linguagem da empresa OpenAI capaz de produzir textos artificiais a partir de uma entrada inicial qualquer. Esse modelo ficou marcado pela figura de um unicórnio, visto que um dos seus primeiros textos que foi a público é sobre a descoberta de uma sociedade de unicórnios prateados. Logo após a publicação desse trabalho, a OpenAI anunciou que não iria liberá-lo publicamente, dado o potencial de repercussão negativa (por exemplo, fabricação de notícias falsas). Porém, pouco tempo depois o modelo foi totalmente liberado, e a comunidade já construiu uma ferramenta que lhe permite escrever textos com o auxílio do GPT-2. Experimente!\n",
        "\n",
        "Essas e outras aplicações mostram a versatilidade de soluções baseadas em redes neurais. Então, que tal continuar o nosso curso para dar o primeiro passo na construção de ferramentas tão poderosas quanto as apresentadas aqui?Apresentamos em aula o primeiro modelo neural, o neurônio de Mcculloch e Pitts. Ele agrega as suas entradas com uma soma e possui uma função que define se o neurônio ativa ou não com base em um limiar. Vimos em aula os seguintes exemplos:\n",
        "\n",
        "AND: ativa se a soma g(X)>=2, ou seja, x1=1 e x2=1\n",
        "\n",
        "OR: ativa se a soma g(X)>=1, ou seja, x1=1 ou x2=1\n",
        "\n",
        "Representação do neurônio Mcculloch Pitts com duas entradas, realizando as portas lógicas AND e OR\n",
        "\n",
        "Essas operações podem ser realizadas com mais de duas variáveis, podendo ou não ser necessário alterar a regra de ativação. Assuma que precisamos realizar ambas as operações lógicas com 3 entradas, como apresentado na figura a seguir.\n",
        "\n",
        "Representação do neurônio Mcculloch Pitts com 3 entradas realizando as portas lógicas AND e OR\n",
        "\n",
        "Preencha as representações acima com o limiar adequado para solucionar as operações AND e OR nesse novo cenário.\n",
        "Na aula de hoje vamos falar um pouco sobre tensores. Então vamos usar essa aula para explicar o que são tensores e, porque eles se tornaram tão importantes quando se fala de redes neurais.\n",
        "\n",
        "[00:14] Bom, se você vai trabalhar com problemas do mundo real e você quer usar uma rede neural, você quer usar deep learning, é bem necessário aprender um framework, porque por mais que seja possível aprender os conceitos de redes neurais sem usar nenhum framework, usando só NumPy, ou usando só a linguagem básica, para você implementar soluções otimizadas, não precisar reinventar a roda, é importante aprender um framework que já vai trazer bastante coisa pré-pronta para você usar e poder concentrar na parte da modelagem, na parte da criatividade, do alto nível.\n",
        "\n",
        "[00:59] Por isso que estamos fazendo esse curso com o PyTorch como base, porque eu acredito que já acelera bastante o processo de conseguir usar deep learning.\n",
        "\n",
        "[01:13] Então a estrutura de dados mais utilizada pela maioria dos frameworks, não só esses que eu trouxe aqui, é o tensor. Não por acaso essa palavra é muito ouvida quando você está estudando redes neurais. E quando você estuda qualquer tipo de framework, a palavra tensor vai aparecer bastante. Tanto é, que um dos frameworks se chama TensorFlow. O TensorFlow tem motivos para ser uma estrela.\n",
        "\n",
        "[01:39] Então a partir de agora, a princípio, o que você precisa saber é que as entradas, as saídas e as transformações de uma rede neural, são representadas através dessas estruturas de dados. É basicamente uma eterna matemática de matrizes e essas matrizes são representadas em uma estrutura tensorial.\n",
        "\n",
        "[02:02] Então vamos falar um pouco sobre tensores. Se você aprendeu estrutura de dados na aula de programação, você conheceu essas coisas com outros nomes, mas estamos aqui para simplificar. Então o primeiro conceito importante é saber que um tensor é uma generalização de coisas que já conhecemos, como, por exemplo, dadas as seguintes estruturas, aqui nós temos um escalar, um vetor com dois elementos e uma matriz com 4 elementos, 2 linhas por 2 colunas.\n",
        "\n",
        "[02:37] Como chamamos essa estrutura que tem 3 dimensões? Então você tem 4 matrizes dentro de uma outra dimensão. Como chamamos essa estrutura? Na programação, talvez você aprendeu da seguinte forma:\n",
        "\n",
        "[02:55] Você tem o a = 2 em escalar, um vetor ou um array, uma matriz ou um array 2D, você pode chamar dessas duas coisas, na introdução a programação chamamos de matriz. E no array 3D, ou já vi até gente chamar de matriz 3D. Então você simplesmente adiciona um 3D, um 4D, um 5D a estrutura da sua linguagem de programação.\n",
        "\n",
        "[03:21] Então a quantidade de dimensões d > 2, consideramos um nd-array, ou seja, um array de n dimensões. Na matemática, os nd-arrays recebem o nome de tensores ou nd-tensores. Então o tensor nada mais é do que um array n-dimensional. Então só para colocar as terminologias da computação em comparação com as da matemática, o que na programação chamamos de número, na matemática chama escalar.\n",
        "\n",
        "[03:53] O que chamamos de array é um vetor, o que chamamos de array 2D, é uma matriz e o que chamamos de array 2D, 4D, é um tensor. Então é só uma nomenclatura da matemática que foi absorvida para os frameworks.\n",
        "\n",
        "[04:08] Então por convenção, nos frameworks nós chamamos as estruturas pelo nome de tensor independente da dimensão. Então aqui temos a nomenclatura original. Número, array, escalar, vetor, matriz, tensor 3D. Essa é a nomenclatura original.\n",
        "\n",
        "[04:26] Nos frameworks vamos ter essa nomenclatura matemática, que são os tensores 0D, que são os escalares, tensor 1D, que é o vetor, tensor 2D, que é a matriz e daí por diante.\n",
        "\n",
        "[04:39] Então toda estrutura que você manipular nesses frameworks são tensores de alguma dimensão. Até os escalares são tensores de 0 dimensões. Então é importante se familiarizar com nomenclatura e não se assustar com ela, porque é uma estrutura de dados específica para trabalhar com múltiplas dimensões.\n",
        "\n",
        "[05:03] Em machine learning é muito comum trabalhar com dados em alta dimensionalidade e os tensores são as melhores estruturas para trabalhar com alta dimensionalidade. Você consegue representar de forma genérica desde um vetor até milhões de dimensões como uma rede neural pode ter, mesma estrutura de dados e consegue realizar operações de forma eficiente. Então por se tratar de uma abordagem baseada em dados, ou seja, tudo é matriz, tudo é vetor, tudo é multidimensional, o tensor virou a estrela do deep learning.\n",
        "\n",
        "[05:39] Então, como eu já falei, inclusive nomeando um dos frameworks mais famosos da atualidade. Faltou só eu dizer que o TensorFlow é da Google, então a própria Google exaltou o tensor no nome do frameworkdeles.\n",
        "\n",
        "[05:56] Nós vamos trabalhar com PyTorch ao longo do curso, eu vou justificando o porquê da escolha do PyTorch, só para não ficar essa questão no ar, sobre porque não o TensorFlow, porque os dois mais populares são o TensorFlow e o PyTorch, mas eu fiz essa escolha bem pensada para ensinar para vocês vários elementos muito bacanas do framework.\n",
        "\n",
        "[06:20] No próximo vídeo vamos conhecer a sintaxe básica do PyTorch e como manipulamos tensores, que, na verdade, é muito parecido com os arrays da programação normal."
      ],
      "metadata": {
        "id": "Id7QKu6wHPf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando Tensores(array multidimensionais)\n",
        "\n",
        "import torch\n",
        "\n",
        "lista = [[1,2,3],\n",
        "         [4,5,6]]\n",
        "\n",
        "tns = torch.Tensor(lista)\n",
        "print(tns.dtype)\n",
        "print(tns)\n",
        "\n",
        "\n",
        "tns = torch.FloatTensor(lista)\n",
        "print(tns.dtype)\n",
        "print(tns)\n",
        "\n",
        "\n",
        "tns = torch.DoubleTensor(lista)\n",
        "print(tns.dtype)\n",
        "print(tns)\n",
        "\n",
        "\n",
        "tns = torch.LongTensor(lista)\n",
        "print(tns.dtype)\n",
        "print(tns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a4iuPxIFRI2",
        "outputId": "2b03df46-3afe-4564-8f81-f832f89947f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "torch.float32\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "torch.float64\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64)\n",
            "torch.int64\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[00:00] Seja bem-vindo a nossa primeira aula mão na massa. Vamos programar bastante ao longo desse curso, então eu já estou usando um ambiente mais robusto, umas coisas mais bacanas, que é o Google Colab. É um ambiente online, onde podemos usar a infraestrutura da Google, está popularizando bastante recentemente, então se vocês olharem as configurações do notebook, eles disponibilizam GPU para usar. Então vai ser bastante útil para nós ao longo do curso.\n",
        "\n",
        "[00:28] O que vamos ver nesse notebook é a Sintaxe básico do Pytorch, então o primeiro contato com o Pytorch. E vamos aprender a manipular tensores, as operações mais básicas que tem.\n",
        "\n",
        "[00:40] Primeira coisa que vamos ver e essa é a documentação sobre tensores, se vocês quiserem acessar, o link é esse aqui < https://pytorch.org/docs/stable/tensors.html >. Primeira coisa que vamos ver são os tipos disponíveis e como nós criamos tensores. Então, basicamente, a primeira forma que vamos ver de criar tensores é convertendo a parte de listas. Em geral, os tensores vão ser convertidos através de outra coisa, pelo menos na experiência que eu tenho, eu sempre carrego as informações como array, por exemplo, e depois eu converto para tensor, é bastante útil.\n",
        "\n",
        "[01:16] A princípio vamos ver como se converte a partir de uma lista do Python. Então se eu crio uma lista normal, eu vou criar de duas dimensões para melhorar a visualização e para vermos também que essa lista obedece às dimensionalidades de uma matriz. A lista do Python não precisa ter o número de colunas igual para todas as linhas, eu poderia diminuir uma coluna na segunda linha e é totalmente plausível, mas se a minha lista obedece à dimensionalidade de uma matriz, eu consigo convertê-la para um tensor.\n",
        "\n",
        "[01:47] Para isso eu vou precisar importar a biblioteca do Pytorch e ela se chama torch. Então import torch e você tem acesso às funções. O objeto tensor, você consegue converter simplesmente chamando torch.Tensor(lista) e mandando como parâmetro a lista. Aí ele vai converter para o tipo padrão dos tensores do Pytorch que você pode consultar através do atributo D-Type.\n",
        "\n",
        "[02:13] Então se consultarmos, ele converteu minha lista para o tipo float32. Então apesar de eu ter criado uma lista de inteiros, na hora que ele converte através de uma lista, ele também vai mudar o tipo. Então print(tns). Ele mudou o tipo para ponto flutuante. Se você prefere converter de forma explícita, sem contar o tipo padrão do Pytorch, não tem problema, é só chamar FloatTensor.\n",
        "\n",
        "[02:41] Então temos vários tipos disponíveis se vocês olharem, tem floats com diferentes precisões. 32, 64, 16, tem inteiros com diferentes precisões e tem o tipo booleano também. Então tudo aqui está disponível para nós. Mas, em geral, o que nós usamos? Usamos o tensor padrão, que é o float, usamos o double, que é um ponto flutuante com maior precisão. E quando é necessário usar inteiros, aqui ele já mostrou que é o float64, e quando é necessário usar inteiros, em geral, usamos o long, que aqui ele colocou o tipo como inteiro.\n",
        "\n",
        "[03:26] E usamos o long porque ele tem uma precisão maior também para quando você precisar operar com outras coisas. Então você usa o tipo long que vai suportar inteiros maiores e que também vai operar com outras informações de maior precisão.\n",
        "\n",
        "[03:43] Esses são os que nós normalmente usamos, mas fiquem a vontade para consultar a documentação. Outras formas de instanciar tensores é a partir de arrays Numpy, que como eu falei, eu carrego muito dados a partir das funções do Numpy, de carregamento e para converter para tensor é super fácil, porque tem a função from_numpy. Então se eu crio um array qualquer, vou criar um array* aleatório, random.rand e eu coloco que eu quero um array de 3 linhas e 4 colunas, para eu converter para tensor, é só chamar de torch.from_numpy e mandar o array como parâmetro.\n",
        "\n",
        "[04:21] Então se eu imprimir o array e o tensor, os dois estão aqui. Os parâmetros das dimensões vão sem dupla. Eu criei um array aleatório e converti para um tensor. O interessante aqui é como ele trata os tipos. No caso das listas ele vai converter o tipo a depender da função que você chamar. No caso do Numpy ele vai aproveitar, ele vai preservar, melhor dizendo, o mesmo tipo do array. Então se imprimirmos o tipo do array e o tipo do tensor é exatamente o mesmo tipo, float64, torch.float64.\n",
        "\n",
        "[05:11] Se por acaso esse meu array fosse do tipo inteiro, eu vou converter para inteiro, aí o tensor também seria do tipo inteiro, ele preserva o tipo original dos dados. Tensores já inicializados é idêntico ao Numpy nesse aspecto, porque você pode criar tensores com valores preenchidos já, então você não precisa converter de ninguém, você já cria um tensor vazio, digamos assim, ou com valores aleatórios. Então isso serve para inicializar os pesos de uma rede, para criar os rótulos, serve para várias coisas que vamos ver ao longo do curso.\n",
        "\n",
        "[05:54] Eu posso criar um tensor, o tensor 1, que é preenchido com 1, que é preenchido com uns, torch.ones(2,3) e a dimensão é 2 por 3, o tns0, que é preenchido com zeros, torch.zeros(4,5) e o tnsr, que é aleatório, torch.randn(3,3). Se eu imprimir esses 3 tensores, tns1, tns0 e tnsr, eles já vão estar preenchidos, eu criei com dimensões diferentes só para facilitar a visualização. E no caso do rand, ele chama randn, porque são valores aleatórios rand a partir de uma distribuição normal. Então esses valores vão ser amostrados a partir de uma distribuição normal.\n",
        "\n",
        "[06:50] E eu posso converter esse tensor de volta para um array numpy. Por exemplo, se eu quiser salvar como array Numpy, se eu quiser fazer operações ou visualizar esses dados, eu posso converter de volta para numpy. Para isso, é só eu pegar o atributo. Vou pegar o tensor preenchido com valores aleatórios. É só para eu transformar em array eu chamo o atributo data e converto o data para Numpy. Lembra que o tenso, vou imprimir o array. O tensor ele tem vários atributos, ele tem D-Type, tem shape, assim como arrays Numpy. Nós vamos ver direito.\n",
        "\n",
        "[07:29] Então eu preciso isolar o atributo que tem os dados efetivamente do tensor, para a partir desses dados eu converter para um array Numpy. Então não adianta só eu chamar de tensor.data, ele vai continuar sendo um tensor. Eu converto para Numpy e aí ele passa a ser um array. Se eu imprimir o tipo dele, será array.\n",
        "\n",
        "[07:57] Eu acho que eu não imprimi o tipo ainda, vamos dar uma olhada, o tipo do tensor, o type do tensor é uma torch.tensor mesmo, um tipo que nós criamos, e o tipo Numpy que é a conversão que fizemos. Bom, para fazer a indexação eu vou o usar o próprio tensor de valores aleatórios mesmo, então eu vou imprimir de novo só para termos uma noção. Vou criar um tensor de valores aleatórios e vou substituir um desses valores, sei lá, vou substituir o valor da linha 0, coluna 2, substituir por -10. E se eu imprimir esse tensor de novo, ele foi modificado. Eu vou dar um /N só para visualizarmos melhor.\n",
        "\n",
        "[08:50] Ele foi modificado. Ou não! O que eu fiz? Eu modifiquei o tensor errado. É importante modificar o tensor certo. Agora ele foi modificado para -10, o que era 0.21. Então o tensor é uma estrutura mutável. E eu também posso indexar das formas que você indexa um array Numpy, indexar no sentido de acessa elementos, não sei se todos são familiarizados com o verbo. Indexar é você acessar partes do seu tensor. Para acessar uma fatia, que é uma ferramenta muito útil, eu diria, do array Numpy, você pode usar a sintaxe dos dois pontos.\n",
        "\n",
        "[09:34] Então se eu acessar, por exemplo, de 0 a 2, eu vou estar acessando uma fatia do meu tensor. Vamos acessar o tensor correto? Eu vou acessar uma fatia do meu tensor. Vou dar outro enter.\n",
        "\n",
        "[10:00] Aqui. Eu estou acessando as linhas 0 e 1 do meu tensor, porque o elemento à direita dos dois pontos é não inclusivo, então ele acessa as linhas 0 e 1 e não acessa a linha 2. Então isso aqui é uma fatia de tensor. Eu posso acessar essas fatias, eu posso dizer que eu quero todas as linhas da coluna 2 e aí ele vai me dar os elementos da coluna 2. Então ele pegou todas as linhas e na dimensão da coluna ele só pegou os elementos da posição 2. Então eu posso indexar como fatia também e o que mais? Acho que é só isso, indexar como fatia, indexar como elemento, exatamente.\n",
        "\n",
        "[10:50] E tudo que você indexa vai ser na forma de tensor. Todo elemento. Isso é importante mostrar, eu vou indexar um único elemento aqui. tnsr02. Um único elemento também vai ser um tensor. É um tensor de dimensão 0. Então eu coloco .size e ele vai dizer que o size não existe, não existe dimensão, porque esse tensor tem 0 dimensões. Porque ele é um elemento único. Então tudo que eu acesso a partir de tensores vai ser um tensor também.\n",
        "\n",
        "[11:24] Eu acessei minhas partes de tensores, e uma coisa importante destacar é que as operações básicas, soma, subtração, multiplicação, é tudo feito ponto a ponto, então cada elemento de um tensor opera com o respectivo elemento do outro tensor. Então se eu tenho o tensor de valores aleatórios e o tensor preenchido com números 1, na verdade, eu nem vou poder usá-los, porque eles não têm a mesma dimensão. Então vamos ver o shape de um e de outro.\n",
        "\n",
        "[11:59] É, a dimensão deles não bate, mas eu posso pegar uma fatia, eu vou pegar o meu tensor fatiado, vou chamar de tns mesmo. Vai ser o tensor de valores aleatórios e eu quero duas linhas e todas as colunas. Então vamos ver.\n",
        "\n",
        "[12:20] Agora as dimensões batem, agora eu consigo operar com esses tensores. Então se eu tenho aqui dois tensores de dimensões que batem uma com a outra, eu vou imprimir os próprios tensores para acompanharmos. Eu posso somar esses tensores, tns+tns1. Eu posso somar esses tensores, então ele vai somar 1+0.1 = 1.1, 1+1.0 = 2.0. Vai somar ponto a ponto. Mesmo coisa a multiplicação. A multiplicação nesse caso vai ser o mesmo tensor, porque o outro é preenchido com uns. A divisão eu posso fazer tns1 divido por tns. E vamos ver o efeito disso. E daí por diante.\n",
        "\n",
        "[13:10] Daí por diante eu posso fazer operações e elas vão ser feitas ponto a ponto. Eu estou falando isso, porque a multiplicação, se você multiplica dois tensores, que são matrizes matematicamente falando, ele vai multiplicar ponto a ponto, ele não vai fazer a multiplicação de matrizes que esperamos. Existem funções específicas para fazer produto interno.\n",
        "\n",
        "[13:34] Então no caso de matrizes de duas dimensões, eu posso simplesmente fazer a multiplicação das matrizes, eu acho que ele pede uma dupla. Como as dimensões são 2 por 3 e 2 por 3, para fazer a multiplicação de matrizes eu tenho que transpor uma das duas e aí ele vai fazer o produto interno, que é a multiplicação de matrizes que conhecemos.\n",
        "\n",
        "[14:00] Agora as duas últimas coisas são as funções para consultar o tamanho do meu tensor, eu acho que eu usei o shape, a função shape também é do Numpy, igual do Numpy e nos ajuda a consultar a dimensionalidade dos tensores. Então sabemos que é um tensor de 2 linhas e 3 colunas, consultando o shape. Mas o torch também tem a função size.\n",
        "\n",
        "[14:31] Bom, não difere nada, quase a mesma coisa da função shape, só que ele te dá um objeto. Então se eu consulto tns1.size, que é uma função, ele vai me dizer um objeto. Na verdade, também é um objeto tipo size, então literalmente não muda nada, só muda que eu posso dar parâmetros, mas também posso consultar as dimensões, então literalmente não muda nada.\n",
        "\n",
        "[14:56] Então eu posso consultar o tamanho do meu tensor e ele vai dizer que o tensor é 2 por 3, seu eu acessar corretamente, ele vai dizer que é 2 por 3. E eu posso alterar as dimensões desse tensor. Para isso eu vou criar um tensor um pouco maior para nos ajudar a visualizar. Eu vou criar um tensor aleatório, randn com duas linhas 2 por 2 por 3.\n",
        "\n",
        "[15:27] Então se eu imprimir esse tensor, ele tem 2 grandes linhas, 2 na profundidade, se isso fosse um cubo, ele tem dois nas linhas e 3 nas colunas. Começa a ficar mais difícil de visualizar, mas isso é como se fosse um cubo, mas na impressão ele aparece achatado. Então eu tenho um tensor 2 por 2 por 3. Eu posso usar a função view para redimensionar esse tensor. Isso é muito útil quando estamos trabalhando com redes neurais. Então eu posso dar tns.view e eu posso dizer, por exemplo, que eu quero achatar esse tensor, então eu posso fazer 2 x 2 = 4 x 3 = 12.\n",
        "\n",
        "[16:21] Então tns.view (12) print(tns). Faltou eu atribuir. tns recebe tns.view. Agora tudo correto.\n",
        "\n",
        "[16:35] Eu imprimo o size, eu atribuo um tensor redimensionado, a própria variável e imprimo ele novamente. Agora está certo. Então o tamanho que antes era 2 por 2 por 3, agora vai ser achatado de tamanho 12. Eu posso fazer isso também usando indexação negativa. Se eu coloco -1, ele vai saber que eu estou dizendo: “Achate esse meu tensor”. Então a indexação com -1 significa achatar.\n",
        "\n",
        "[17:05] Mas, eu também posso querer redimensionar para qualquer outra dimensão. Eu posso querer 1 por 1. Só precisa bater. Eu posso fazer 4 por 3, por exemplo, e aí vai fazer 4 linhas e 3 colunas. A única coisa que importa é que as dimensões sejam consistentes para ele poder redimensionar. Outra coisa bacana é que eu posso usar, por exemplo, se eu não sei as dimensões do meu tensor, eu posso consultar, vamos supor, eu quero manter a primeira dimensão, isso aqui é o mais comum de se fazer com redes neurais, eu quero manter a primeira dimensão e eu quero achatar o resto.\n",
        "\n",
        "[17:47] Eu posso fazer isso, então eu mantenho a primeira dimensão, ou seja, duas linhas e eu achato o resto e você pode usar a indexação negativa para isso. E por fim, o Cast na GPU. Isso é para quando você precisa colocar informação na GPU. Pegar dados e jogar na GPU, que é o hardware acelerado, que vai permitir um processamento mais rápido. O Colab te dá acesso a GPU aqui nas configurações, é maravilhoso isso. E a primeira coisa que você precisa saber, primeiro e por questão de boas práticas, é verificar se a GPU está disponível.\n",
        "\n",
        "[18:24] Então você faz isso usando o torch.cuda.is_available. Cuda é o ferramental usado para te dar acesso a GPU. Então você vê se o Cuda está disponível. Se estiver, torch.device (cuda). Se não, o device que você vai usar é o torch.device(cpu). E embaixo você pode imprimir qual é o device. E ele vai te dizer que é CPU, porque estamos usando CPU. E para você jogar a informação na GPU, aí você precisa mudar as configurações para GPU, ele vai conectar de novo, então precisamos importar tudo de novo. import.torch, vamos criar um tensor aleatório.\n",
        "\n",
        "[19:13] randn para uma dimensão só. Para você jogar a informação, basta você fazer tns.to(device) e ele vai jogar sua variável na GPU. Ele vai dizer que o dispositivo é Cuda, é GPU que está disponível e na hora que você imprimir o seu tensor, ele também vai te dar o atributo device dizendo que está na GPU. Então para nós criarmos modelos robustos, modelos maiores de redes neurais, é extremamente importante o uso da GPU e o fato do Colab fornecer isso ajuda muito a desenvolver soluções para problemas do mundo real.\n",
        "\n",
        "[19:53] Então nós vamos ver muito mais quais tipos de variáveis são colocadas na GPU e porquê, só que só nas próximas aulas.\n",
        "Nesta aula, tivemos uma visão geral de manipulação de tensores, mas ainda há um universo de possibilidades que pode ser encontrado na documentação do objeto Tensor do PyTorch.\n",
        "\n",
        "Uma operação muito útil, que não faz parte diretamente do módulo de tensores, é a concatenação. Modelos mais complexos, que envolvem fusão de informação, ou entradas que precisam ser manipuladas antes de ser enviadas para a rede, se beneficiam muito dessa operação. Para isso, o pacote torch traz a função cat que tem o seguinte padrão:\n",
        "\n",
        "tns_out = torch.cat( (tns1, tns2), dim=0 )COPIAR CÓDIGO\n",
        "Ou seja, a função recebe um objeto tipo tupla contendo o conjunto de tensores a ser concatenados, seguido da dimensão de concatenação. Os tensores devem ter dimensões idênticas, exceto na dimensão de concatenação.\n",
        "\n",
        "Outras operações para combinar múltiplos tensores pode ser encontrada nessa parte da documentação.\n",
        "\n",
        "Para trabalhar com Deep Learning no PyTorch é preciso dominar a arte de manipular tensores e transformá-los da forma que o problema necessitar. Explore amplamente a documentação do pacote torch e encontrará funções como torch.squeeze() e torch.unsqueeze() que, respectivamente, removem e adicionam dimensões de tamanho 1. Essas e outras funções vão facilitar muito o seu trabalho e te tornar um mestre na arte dos tensores![00:00] Agora nós vamos falar sobre o Perceptron. Vamos entender o que ele é, como usa no PyTorch e a princípio, eu dediquei esse primeiro vídeo para falar sobre classificação linear, ou melhor, para falar sobre modelos lineares no contexto de classificação. Por quê? Porque comentamos da limitação do Perceptron, que ele só pode modelar retas, ou, em um espaço maior, são modelos lineares. E vamos falar um pouco sobre o que são esses modelos lineares, porque basicamente é como o Perceptron funciona e como é sua limitação.\n",
        "\n",
        "[00:43] Vamos começar. A princípio nós só vamos lembrar de como é a equação da reta, como formamos uma reta a partir da equação da reta e como nós interpretamos essa reta como um classificador.\n",
        "\n",
        "[00:55] Então, legal. Aqui temos a equação que lembramos. A “x + by + c” e eu defini valores, -1, 4 e 0.4 para essa reta que vamos criar. Poderia ser qualquer reta, eu escolhi um conjunto de valores. B recebe 4 e c recebe 0.4. Então para eu conseguir visualizar a reta definida por esses valores, eu preciso dizer quem são os pontos X e Y no espaço. O que eu vou fazer é fixar o X, vou fixar os valores de X em um intervalo que eu quero e vou pegar a resposta em Y, dados os pesos da nossa reta.\n",
        "\n",
        "[01:40] Então eu só vou importar uma biblioteca, importar o Numpy para nos ajudar com esse processo e vou gerar os valores de X como um intervalo de valores, com a função linspace e eu quero um intervalo entre -2 e 4. Eu quero 50 valores. Então que temos até agora é um vetor que varia de -2 até 4 e tem 50 valores nesse intervalo. Dado que eu fixei, o eixo x, eu quero esses valores, agora o que eu preciso é pegar a resposta em Y. Quem são os valores de Y que vão definir as coordenadas dessa reta? Os pontos dessa reta.\n",
        "\n",
        "[02:21] Então para isso é só vermos que dado que temos a equação da reta “x + by + c”. Para eu descobrir o Y a partir do X, é só eu isolar o Y na equação. Lembrando que aqui é igual a 0. Eu isolo o Y e ele vai ser (-a*x – c)/b. É basicamente isso. Então é exatamente essa equação que eu vou resolver para descobrir quem é essa reta, como é essa reta no espaço. Então eu vou importar uma biblioteca de visualização, que é o matplotlib, para podermos visualizar a nossa reta. plt(x, y). Vou só colocar um grid para conseguirmos visualizar melhor a reta, sendo que, plt.plot(x, y) é a função do matplotlib.\n",
        "\n",
        "[03:12] Então essa é a nossa reta. Só para facilitar a nossa visualização, porque eu vou precisar disso depois, eu vou plotar isso mais bonito, plotando os eixos x e y no ponto 0 de cada eixo. Então, eu vou só plotar uma reta vertical em 0, variando de -1 a 1 com cor preta e o comprimento é 1, para ela ficar mais fina. E o mesmo com o eixo horizontal em 0, só que dessa vez variando de -2 até 4, porque é onde esse eixo varia, -2 até 4. Vamos ver?\n",
        "\n",
        "[03:53] Agora temos aqui os eixos, é possível vê-los e a nossa reta neles. A última coisa que eu vou precisar fazer é transformar isso em uma função, porque aqui em baixo vamos solucionar a equação da reta para diferentes pontos. Então lembrando, eu escolhi essa reta, eu poderia ter escolhido outra reta, se eu colocasse o valor de c igual a 3, por exemplo.\n",
        "\n",
        "[04:19] Eu estaria escolhendo outra reta. A reta mudou em relação à origem, mas vamos manter a reta original. E agora eu vou solucionar a equação da reta para diferentes pontos no espaço para vermos o que acontece. Para isso eu preciso transformar em uma função para podermos chamar de novo.\n",
        "\n",
        "[04:44] Então eu chamo plotline(a, b, c) que já conhecemos. E agora isso é uma função. Então vamos escolher 3 pontos para entendermos o que acontece com diferentes pontos no espaço em relação à reta. Então primeiro ponto que eu vou escolher vai ser bem em cima da reta. Vai ser o ponto com x em 2 e y em 0.4. Então o meu ponto 1 vai ser (2, 0.4). Aí eu soluciono a equação da reta para esse ponto. Então se temos ax + by + c, os valores x e y, que eu quero solucionar, são os valores do meu ponto, o valor x do meu ponto e o valor y do meu ponto.\n",
        "\n",
        "[05:36] Então transformamos isso em um retorno, atribui a um retorno e plota como fica o resultado, então plotline(a, b, c) e plota o nosso ponto, então plt.plot(p1[0], p1[1], color=b, marker=o) e eu estou alterando os atributos só para o ponto ficar mais visível para nós.\n",
        "\n",
        "[06:06] Então eu plotei um ponto exatamente em cima da reta. Mas vamos separar isso e pegar esses retornos e ver quanto eles valem. O retorno, se plotarmos... Porque isso é precisão de representação do computador. Se plotarmos com uma precisão menor, isso vale exatamente 0. Então qualquer ponto, isso é regra que aprendemos quando aprendeu equação da reta. Qualquer ponto sobre a reta, ao solucionar a equação da reta para aquele ponto, o retorno vai ser 0. Que até a equação da reta é definida por isso. ax + by + c. Vocês vão cansar da equação da reta, mas é importante lembrar. Igual a 0.\n",
        "\n",
        "[06:47] Então todos os x e y que estão sobre a reta, tem que ser iguais a 0. Então isso foi respeitado. Agora vamos escolher pontos que não estão sobre a reta. Vamos escolher um ponto 2 que está a esquerda, acima da reta. Nosso ponto 2 vai ser x=1 e y=0.6, então p2 = (1, 0.6). Vamos plotar o p2 agora com outra cor, então plt.plot(p2[0], p2[1], color=r, marker=o) vamos plotar com a cor vermelha. Só faltou eu trocar o ponto certo. Agora vai.\n",
        "\n",
        "[07:35] O ponto azul está sobre a reta, o ponto vermelho está acima da reta. Então se solucionarmos a equação para esse novo ponto, é o retorno 2 para o ponto 2, ele vai valer 1.8. Que o que interessa para nós é que é um valor positivo. Se eu colocar qualquer ponto acima da reta, vai ter um valor positivo e se eu colocar um ponto abaixo da reta, então vamos ter um p3 na posição 3, -0.4. p3 = (3, -0.4). Nós plotamos esse p3. plt.plot(p3[0], p3[1], color=g, marker=o), com a cor verde, para diferenciar.\n",
        "\n",
        "[08:25] Está aqui o p3. P3 está embaixo da reta, está inclusive bem longe. Então o que vamos ver, tem várias coisas que conseguimos ver na verdade com isso aqui. É que o p3 vai me retornar um valor negativo. Nossa, dá vontade de dar a aula inteira aqui toda de uma vez. Vai me retornar um valor negativo e um valor maior do que o retorno de p2. Porque p2 está mais próximo da reta. Então esse retorno vai dar justamente positivo, porque p2 está acima da reta e vai dar um valor que determina também o quão distante ele está da reta.\n",
        "\n",
        "[09:10] No caso de p3, eu tive um retorno negativo, ou seja, está abaixo da reta e maior em módulo, e maior, o que significa que esse ponto está mais distante da reta. Então essa reta é um classificador maravilhoso, um classificador linear maravilhoso, se você definir esses critérios, se você definir se o retorno da equação da reta for maior que 0, significa que eu estou falando de pontos que estão acima da reta. Formalizando aqui, nós temos, se o retorno for positivo, eu sei que os pontos estão para cá, acima da reta, e se o retorno for negativo, eu sei que os pontos estão para cá, abaixo da reta.\n",
        "\n",
        "[09:58] Então estou usando essa reta como uma fronteira de decisão entre duas regiões no espaço. Então eu poderia ter aqui vários pontos acima da reta e todos esses pontos teriam um retorno positivo. E eu poderia vários pontos abaixo da reta e todos esses pontos teriam valor negativo em relação à reta. Então essa minha reta é um classificador, então quando falamos de modelos lineares, estamos falando de modelos que são geometricamente falando retas em duas dimensões, ou em múltiplas dimensões chamamos eles de hiperplanos. Mas são modelos lineares, retas, planos, hiperplanos e daí por diante.\n",
        "\n",
        "[10:51] Então vamos ter sempre essas regras. Só formalizando o que já falamos, sempre temos essas regras. Se o retorno for igual a zero, significa que o ponto está sobre a reta, se o retorno for positivo, o ponto está acima da reta e se for negativo, o ponto está abaixo da reta. Por isso uma simples reta funciona como um classificador. Então quando falamos no contexto de redes neurais, antes de irmos para o texto, vamos ver a imagem.\n",
        "\n",
        "[11:19] Estamos falando disso. Dado que temos a classe gato e a classe cachorro, se conseguimos separar esses pontos no espaço com uma reta, um perceptron vai resolver. Porque, eu não sei se vocês se lembram, mas a equação do perceptron é definida nos mesmos termos de um modelo linear. Então só para entendermos bem a generalização disso, temos aqui uma reta ax + by + c, falei que vocês iam cansar disso.\n",
        "\n",
        "[11:53] Só que tem termos de redes neurais, chamamos esses multiplicadores de pesos, porque eles multiplicam pontos, multiplicam pelas coordenadas dos pontos, então eles são os pesos, que inglês chama weight, então é w1, w2, vamos entender o porquê e esse termo que soma, nós chamamos de viés, porque ele só vai deslocar essa reta no espaço, sem alterar as características angulares dessa reta. São detalhes matemáticos, mas tem razão para ter um nome diferente, então chamamos de b, que é o bias, o viés.\n",
        "\n",
        "[12:33] Então só para podermos generalizar para qualquer número de dimensões, nós não chamamos de x e y, porque em 3 dimensões seria x, y, z. Em 4 dimensões seria o quê? x, y, z, q. Teríamos que inventar letra para tratar de múltiplas dimensões, então chamamos as dimensões de x1 e x2 e colocamos um + no meio. Porque se tivermos para qualquer tamanho de espaço, de espaço cartesiano, qualquer tamanho de variável, nós vamos conseguir representar isso de forma escalável. Se for duas dimensões, x1, x2. A nossa entrada é x, tem duas dimensões.\n",
        "\n",
        "[13:19] Se for em 3 dimensões, x3, se for em 4 dimensões, x4 e daí por diante. Conseguimos definir nossa entrada em termos genéricos. Então generalizamos isso tudo para uma única equação. A equação da reta, nós generalizamos para uma única equação, que é essa.\n",
        "\n",
        "[13:41] Se a nossa entrada tem duas dimensões, eu vou ter o retorno de y, vai ser w1*x1 + w2*x2 + b o b é o nosso viés. Para 3 dimensões eu vou ter w1x1 + w2x2 + w3x3 + b = 0 e daí por diante para n dimensões. Então conseguimos colocar isso em termos de um somatório de um produto. Então wi*xi somados pelo número de dimensões que você quiser, +b.\n",
        "\n",
        "[14:24] Então essa equação é exatamente a equação do perceptron. O perceptron é o somatório de um produto de w e x mais um bias. O fato do perceptron ter essa limitação de só modelar retas é porque ele é definido por um modelo linear. Então nas próximas aulas vamos fazer um exercício bem bacana com esses conceitos.\n",
        "00:00] Nesse vídeo vamos fazer um exercício sobre classificação linear para colocar em prática o que aprendemos na última aula. Então o que vamos fazer é pegar uma distribuição dividida em duas classes e o papel de vocês vai ser dizer qual é a reta que melhor divide aqueles dados.\n",
        "\n",
        "[00:21] Primeiro, para gerar a distribuição, vamos usar a função make_classification, que inclusive já estava aberta, mas vamos seguir os links. Vamos usar a função make_classification do sci-kit learn, que é uma biblioteca de machine learning. Então o que ele vai fazer se pegarmos os exemplos embaixo, ele vai definir distribuições divididas em duas classes, duas ou mais classes, tem como fazer com mais de duas classes, linearmente separáveis ou não, depende do que você definir.\n",
        "\n",
        "[00:58] Então vamos querer algo parecido com isso, duas classes linearmente separáveis, para podermos treinar um modelo para classificar isso. Então vamos pegar. No exemplo mesmo ele mostra como gera essas distribuições. Basicamente é uma chamada simples de função, que não tem muito mistério e ele vai retornar um x, que são os dados, lembra que chamamos as dimensões de x1, x2, x3, então aqui vai guardar todas as dimensões do dado. E o y será os rótulos, então se pertence à classe 0 ou a classe 1.\n",
        "\n",
        "[01:38] Então primeiro precisamos importar a função, então from sklearn.datasets import make_classification. Então que essa função vai nos dar, se nós imprimirmos a dimensão dessas coisas, o que essa função vai nos dar são 100 amostras, isso também é um parâmetro que você pode mudar, os parâmetros que ele definiu no exemplo são número de features, número de features informativas, quantos clusters por classe, mas você pode usar os valores da sugestão do exemplo mesmo, sem problema nenhum. Por padrão, ele vai te dar 100 valores e nós definimos que são 2 features para cada valor.\n",
        "\n",
        "[02:28] Então nós temos esses valores, o array e x, são vários pares de coordenadas que vão definir pontos no espaço, aquela distribuição e o y, vale 0 ou 1 a depender da classe de cada ponto. Então vamos plotar isso, para isso eu vou precisar da biblioteca do matplotlib, então matplotlib.pyplot as plt plt.plot(x[:, 0], x[:, 1], marker=o`). Eu vou plotar do jeito que estou escrevendo para vocês verem o que vai acontecer. Coloca o marcador redondo e vamos lá. Enter.\n",
        "\n",
        "[03:20] Eu não posso plotar com a função plot normal do matplotlib, porque ele vai gerar essas retas entre os pontos que só nos atrapalha. Então o que queremos é um plot do tipo scatter, que é um plot de espalhamento. Espalhamento de dados. Então ele vai plotar só os pontos no espaço. Mas ainda precisamos ver a diferença entre as cores, por isso o scatter tem parâmetro c, onde você pode passar um vetor de classes, nesse caso temos duas classes e ele vai separar em duas cores.\n",
        "\n",
        "[03:55] Então aqui está dividido em duas cores. Só por estética, para conseguirmos visualizar melhor, eu vou colocar uma borda preta.\n",
        "\n",
        "[04:05] E vocês já devem ter reparado que cada vez que eu rodo isso de novo, ele gera uma nova distribuição. Porque é aleatória a função, é esse o propósito dela. Mas para o nosso exercício, vamos fixar a aleatorização dos dados usando a biblioteca Numpy. Como o ´scikit-learn` usa o Numpy eternamente, podemos definir uma raiz para aleatorização dos dados e assim, sempre que rodarmos, ele vai gerar exatamente a mesma distribuição. No meu computador, no computador de vocês, sempre a mesma distribuição.\n",
        "\n",
        "[04:41] Então vamos plotar um dos pontos dos nossos dados da nossa distribuição e ver que classe foi atribuída a ele. Então eu posso plotar, por exemplo, o ponto 10. Eu quero o ponto 10 da minha distribuição. Eu ploto plt.plot(p[0], p[1], marker=^, markersize=20) vou colocar um marcador diferente para conseguirmos enxergar e um markersize maior.\n",
        "\n",
        "[05:10] Vamos ver. E precisamos também imprimir a classe desse ponto, então print(y[10]).\n",
        "\n",
        "[05:21] Do lado azul são as classes que valem zero e por consequência, do lado amarelo são as classes que valem 1. Então já temos essa informação. Então vamos adiante. Agora precisamos definir um modelo de classificação que vai separar esses dados no espaço. Então o que temos que fazer é exatamente o que fizemos antes, essa função do plotline e agora sabemos que não é uma reta qualquer, é um modelo. Então vamos trocar o nome dessa função para plot model, então vamos lá def plotmodel(w1, w2, b); e sabemos agora que os pesos chamam w e o viés chama b. Então podemos trocar isso.\n",
        "\n",
        "[06:10] E agora vamos definir um modelo aleatório. Vamos lá? Eu vou inicializar isso com w1 = -3 w2 = 5. Eu estou colocando qualquer valor. E o bias igual a 3 b = 3. Coloquei valores aleatórios. Lembrando que w1 é o nosso a, w2 é o nosso b e o nosso bias é o nosso c da equação da reta.\n",
        "\n",
        "[06:40] Então eu defini um modelo e eu preciso ver o quão bem esse modelo separa a nossa distribuição. Então eu vou colocar junto do modelo, o plot da distribuição, que aí já olhamos tudo de uma vez. Opa! Trocamos o nome para plotmodel.\n",
        "\n",
        "[07:02] Então temos aqui essa reta, que não separa bem os dados, na verdade, separa muito mal os dados e agora precisamos ajustar isso para ver como conseguimos separar bem esses dados. Se estivéssemos usando um algoritmo de otimização, ele faria isso sozinho, experimentaria diferentes valores. Mas aqui, eu quero que vocês sigam a intuição, eu quero que vocês entendam o impacto de mexer nesses valores, mexer nos pesos e mexer no viés. Uma coisa eu já digo para vocês, o viés só vai deslocar essa reta no espaço. Vê onde essa reta está, a reta está cruzando o x em 1.\n",
        "\n",
        "[07:45] Então se eu coloco o b = 0, ela está cruzando agora o x na origem. Então eu a desloquei, mas ela continuou paralela à reta original. E se eu quiser mexer na rotação, eu tenho que mexer, por exemplo, em um dos pesos.\n",
        "\n",
        "[08:03] Está vendo? Ela rotacionou e foi para bem distante, vamos voltar para o que estava antes. Mas o que eu quero é rotacionar ela no espaço, vamos ver o que eu consigo fazer aqui. Eu vou fazer uma modificação estética, que é só definir os limites mínimo e máximo de cada eixo, para fixarmos o gráfico na distribuição. Então pegamos o eixo atual e pega os limites de x e faz a mesma coisa para y, pega os limites de y: y mínimo e y máximo.\n",
        "\n",
        "[08:40] E no final vamos fixar esses limites, x mínimo, x máximo, para podermos focar na região onde tem a distribuição. y mínimo, y máximo. [08:57] A reta está horrorosa, então vamos continuar rotacionando ela, eu posso deixá-la fixada no 0, porque parece que o 0 é um bom local para essa reta estar deslocada no espaço. Mas eu vou rotacionando e vou encontrando um modelo que explique.\n",
        "\n",
        "[09:16] Isso já está bem bacana. Então eu vou deslocar um pouco para o lado, para o outro lado. Eu sou meio perfeccionista, mas eu acho que está bom, vou deixar assim mesmo.\n",
        "\n",
        "[09:33] E aqui essa é uma reta que vai classificar bem esses dois dados. Então como sabemos que ela está classificando bem? Primeiro precisamos saber qual é o lado positivo e qual é o negativo dessa minha reta. Então eu preciso, eu vou dar um ponto aleatório p = (-1, 1), que é a classe azul, seria um ponto mais ou menos paralelo à reta. Eu vou plotar esse ponto no espaço, eu vou ver o retorno desse ponto, na equação da reta e vou ver se o retorno desse ponto é negativo ou positivo. Então é só eu printar print(w1 * p[0] + w2 * p[1] + b) e eu vejo quanto vale isso.\n",
        "\n",
        "[10:17] Isso vale negativo, então valores negativos em relação à reta, são da classe azul, valores positivos são da classe amarela. Então com essas informações já podemos fazer a classificação. Então vamos definir uma função classify(ponto, w1, w2, b); que vai classificar um ponto dados os pesos do modelo. Para isso só precisamos solucionar a equação da reta para esse ponto, ret = w1 * ponto[0] + w2 * ponto[1] + b. E sabemos que se o retorno for maior ou igual que zero, estamos na classe amarela, que se eu me lembro bem é a classe 1, não é isso? Azul é a classe 0 e azul é a classe 1.\n",
        "\n",
        "[11:05] Então se o retorno é maior que 0, eu retorno a classe 1 e a cor amarela, então return 1,yellowse não, `return 0, `blue retorno classe 0 e a cor azul.\n",
        "\n",
        "[11:24] Está aqui minha função. Então eu posso pegar aquele meu ponto aleatório, vou pegar um ponto diferente para vermos, p(2, -1), que é da classe amarela. E eu ploto esse ponto, na verdade, primeiro eu preciso chamar a função classe, cor = classify(p, w1, w2, b) classify ponto p em relação aos pesos do modelo.\n",
        "\n",
        "[12:02] Eu posso até plotar esse ponto no espaço, plotmodel(w1, w2, b) e plotar esse meu ponto p no espaço: plt.plot(p[0], p[1], marker=^, color=cor). Vou colocar um marcador triangular de novo e a cor que ele recebeu como retorno. Vamos ver?\n",
        "\n",
        "[12:28] Eu preciso crescer esse ponto um pouco, se não nós não conseguimos enxergar, vou colocar ele bem grande, então markersize=30.\n",
        "\n",
        "[12:36] Ele classificou corretamente, o ponto é amarelo, ele está do lado direito e a classificação funcionou. Então eu posso classificar para todos os pontos da minha distribuição. Então, eu vou inteirar nos pontos pegando o índice e o valor. Não, eu vou inteirar sabe como? No tamanho desse meu vetor de dados. Então, for x in range(len(x)); eu estou inteirando nos índices, vou até imprimir para vocês verem, print(x).\n",
        "\n",
        "[13:14] Ele vai me dar um range que vai de 0 até a quantidade de dados, que é 99. A quantidade é 100, ele vai de 0 a 99. Vou chamar de k. Então vamos classificar para pegar o quão bem o nosso modelo está se portando. Vou pegar a categoria, vou chamar a função de classificação de novo e chamar categ, _ = classify(x[k], w1, w2, b). Se a categoria for igual a categoria do vetor de rótulos, eu vou incrementar uma variável de acertos. acertos += 1. Então a acurácia no final vai ser o número de acertos em relação à quantidade total de amostras que eu tenho na distribuição.\n",
        "\n",
        "[14:24] Então print (“Acuracia: {0}” .format(acertos/len(x))) acertos divididos pela quantidade de amostras. Vamos ver como está a nossa acurácia?\n",
        "\n",
        "[14:30] 87%. Eu acertei 87% dos valores. Posso até multiplicar isso por 100 para ficar mais bonito. Pronto. 87% dos dados eu consigo classificar corretamente, usando esse meu modelo, essa minha reta e o que eu quero que vocês façam agora é alterar esse seed no início, pode alterar, por exemplo, np.random.seed(30) e agora vamos ter uma outra distribuição e vocês vão ter que ajustar os pesos w1, w2, e b para classificar bem esses dados e ver qual é a acurácia que vocês conseguem.Agora vamos falar efetivamente de Pytorch e como relacionamos tudo que já aprendemos com as funções que o Pytorch oferece.\n",
        "\n",
        "[00:12] Bom, a princípio, vamos destacar que o perceptron é a unidade fundamental de redes neurais. Já comentamos isso, mas só formalizando, redes neurais mais simples, vamos ver isso com calma depois, usam essa unidade, que vimos na aula anterior como parte fundamental. Então cada neurônio de uma rede é um perceptron como esse.\n",
        "\n",
        "[00:37] E ele recebe d, a conexão de entrada, d, a dimensionalidade da entrada. Então nesse caso temos uma entrada com 3 dimensões, x0, x1 e x2. E produz um único valor de saída, que é a transformação linear que ele faz. Então vimos que esse valor de saída, em uma classificação, nós verificamos se ele é positivo ou negativo para fazer uma classificação. Mas o que é importante aqui é que é um único valor, um único número.\n",
        "\n",
        "[01:08] Então o processamento dentro do perceptron é dividido na função de mapeamento, que é a transformação linear, o modelo linear que vimos na aula anterior e a função de ativação f, que não vamos entrar em detalhes agora, que vai ficar para os próximos vídeos.\n",
        "\n",
        "[01:29] O treinamento de um perceptron busca encontrar os parâmetros w e o viés b, que minimizam o problema em questão. Então como fizemos, se quisermos classificar 2 conjuntos de dados, precisamos encontrar os w’s e o b que vão permitir essa classificação. Se vocês lembram do nosso modelo dos vídeos anteriores, era justamente isso, nós otimizamos na mão os w’s e o b necessário para fazer a classificação desses dados.\n",
        "\n",
        "[02:05] Então o treinamento dessa unidade é isso, é experimentar com diferentes valores de w e b para solucionar o problema.\n",
        "\n",
        "[02:14] Agora vamos ver como criamos um perceptron no Pytorch, que vai ser através do módulo nn. Então além de computação tensorial, o Pytorch tem um módulo que é para construção de redes neurais. Ele chama nn de neural network, rede neural em inglês. E ele é importado dentro do pacote torch, simplesmente chamando from torch import nn. E através desse modelo conseguimos implementar diferentes camadas, esse módulo tem muitas camadas implementadas, mas estamos interessados na camada linear, que faz a transformação linear como a do perceptron.\n",
        "\n",
        "[02:57] Então utilizamos a camada linear que recebe dois parâmetros: in_features, que é a dimensionalidade da entrada e out_features, que é a dimensionalidade da saída.\n",
        "\n",
        "[03:09] Então considerando esse perceptron que temos usado nos nossos exemplos, in_features vai ser igual a 3. Eu tenho uma entrada de dimensão 3, x0, x1, x2, são 3 dimensões. E a saída é um número, então a dimensionalidade vai ser 1, meu dado só tem uma dimensão.\n",
        "\n",
        "[03:33] Na representação clássica temos uma entrada com 3 dimensões, formalizando o que eu falei, então x é igual x1, x2, x3. No caso, na verdade, é x0, x1, x2, conforme a imagem. E é uma única saída, produto da transformação linear, que estamos acostumando a chamar agora de y. Então em termos da camada linear, vamos dizer que a entrada tem 3 dimensões, 3 features e a saída tem 1 dimensão, 1 feature. Então conseguimos instanciar a camada linear, consegue instanciar esse perceptron como uma camada linear só preenchendo esses valores nos parâmetros.\n",
        "\n",
        "[04:15] Então no próximo vídeo vamos ver em termos de código mesmo o que significa e o que interpretamos nessa camada que criamos.[00:00] Nessa aula vamos implementar o perceptron, que vimos no vídeo anterior, dentro da sintaxe do Pytorch. Então a primeira coisa que temos que fazer é importar as bibliotecas. Então import torch e importar from torch import nn, que é o módulo de redes neurais, neural networks. Legal? É só para importar essas duas coisas.\n",
        "\n",
        "[00:25] E agora vamos instanciar a nossa camada linear, nosso perceptron. Então o que queremos é replicar exatamente esse perceptron e como já vimos, aqui tem o link da documentação, mas como já vimos nos slides, eu vou ter que alimentar os parâmetros que é o tamanho da entrada, dimensionalidade da entrada e dimensionalidade da saída. Então eu crio o meu ´perceptron = nn.Linear(in_features=3, out_features=1)`, você pode até se guiar pelo número de setas na imagem, a princípio, claro.\n",
        "\n",
        "[01:07] Que tem 3 setas entrando, ou seja, minha entrada tem 3 dimensões e tem 1 seta saindo da camada, ou seja, tem uma dimensão de saída. E eu posso imprimir o meu perceptron para vermos, então print(perceptron). Ele vai imprimir basicamente essa linha que eu programei em cima, porém, me dando todas as informações, inclusive as que eu omiti.\n",
        "\n",
        "[01:27] Eu não defini o bias, mas é porque por padrão ele já adota o bias. Então eu posso omitir o nome dos parâmetros e ele já vai considerar na ordem correta.\n",
        "\n",
        "[01:38] Então temos exatamente o mesmo perceptron, com 3 entradas e uma saída e podemos acessar os pesos desse perceptron, podemos ver os parâmetros que tem dentro dele. Para isso podemos usar essa função parâmetros não nomeados, que é o named_parameters ou acessar individualmente. Vamos primeiro ver pelo nome. Então ele vai retornar duas coisas, que é o nome e o tensor, for nome, tensor in perceptron.namedparameters();. Logo, embaixo eu posso imprimir `print(nome, tensor.data).\n",
        "\n",
        "[02:13] E ele vai me dizer que tem 2 tensores de parâmetro nessa minha camada. Um peso com 3 valores e um viés com um único valor, igual o que aprendemos até agora. Então esses são w0, w1, w2, que vão multiplicar com x0, x1, x2. E embaixo é o viés que é o +b da transformação linear. Por isso a camada chama linear, porque é uma transformação linear.\n",
        "\n",
        "[02:42] Também posso acessá-los pelo nome, eu vou imprimir uma linha vazia e posso imprimir aqui a minha camada perceptron print(perceptron.weight.data) e o mesmo eu faço com o viés. print(perceptron.bias.data). E aí ele vai imprimir exatamente as mesmas informações, só que eu estou acessando diretamente a partir da camada.\n",
        "\n",
        "[03:14] O legal aqui é que ainda conseguimos visualizar como é isso no espaço, para interpretarmos o que esse perceptron vai fazer. Então logo vamos aprender como usamos a camada, mas primeiro eu quero mostrar o modelo para vocês. Então já temos a equação do modelo, é w1 * x1 + w2 * x2 + w3 * x3 + b + 0. E o w2, w2, w3, conseguimos pegar dessa mesma forma, só que convertendo para Numpy.\n",
        "\n",
        "[04:00] Ele vai ter as dimensões 1 por 3, vocês estão vendo até pelos [], ele tem uma dimensão a mais, mas é porque ele está transformando 3 dimensões para 1. Então eu pego o item 0 para ter acesso aos 3 valores do tensor. E o bias, o b = perceptron.bias.data.numpy() e eu tenho o w1, w2, w3 e tenho o b. Como eu faço para plotar esse modelo, isso é só para visualizarmos, como eu ploto esse modelo a partir dos parâmetros que eu já tenho?\n",
        "\n",
        "[04:42] Então, eu acho que não vale a pena entrarmos em tantos detalhes, mas você pode usar a documentação de plot de superfícies do matplotlib ou você também pode fazer como eu fiz e perguntar: “plot plano a partir de equação matplotlib”. Plots 3d eu não faço muito comumente, então não tem problema nenhum pegar de outros códigos e eu vou explicando para vocês o que vai mudar.\n",
        "\n",
        "[05:10] Então vamos fazer. Já estamos aproveitando os imports, o import do matplotlib como estamos acostumados, Numpy como estamos acostumados e só tem esse import diferente, que é o que permite o plot 3d, o import Axes3D. Então embaixo vamos criar uma função inspirada nesse código que encontramos nesse código que encontramos nesse tech overflow. Então plot3d(weights, bias). Já sabemos que, vou até passar o perceptron todo e embaixo eu discrimino as coisas.\n",
        "\n",
        "[05:49] Dados o nosso perceptron, eu separo o peso e o bias, que é o a, b, c, d, que ele colocou aqui, é o nosso w1, w2, w3 e b. E agora ele está definindo o x1, x2, se vocês lembram da última vez que fizemos isso, também fixamos uma dimensão e calculou uma outra dimensão. Vamos fazer a mesma coisa. Estamos fixando essas dimensões, criando um mesh2d, esse é o detalhe que eu falei que não vale a pena entrar, porque o que ele está criando é basicamente uma malha, como se fosse uma matriz com esses valores preenchendo toda essa malha.\n",
        "\n",
        "[06:31] Então x1, x2, a partir do que já preenchemos em cima. E aí a partir disso ele vai inferir a terceira dimensão, que é o z, que para nós vai ser o x3, que é -b + w1 * x1 – w2*x2) / w3 esse é o nosso x3. Então o plotfigure. Ele está dizendo que a projeção é 3d, plotfigura, projeção 3d. ax.plot_surface(X1, X2, X3). E agora podemos chamar o plot3d(perceptron). Falta só executar isso. Executo a célula e agora sim conseguimos plotar. Vamos ver?\n",
        "\n",
        "[07:35] Ele plotou, mas só para ficar mais bonito, eu vou aumentar essa figura, figsize=(10, 8), vou colocar um color map que facilite a visualização e vou rotacionar isso só para ajudar a próxima explicação que eu vou dar. Então eu posso colocar o ax.view_init(azim=180), que eu posso definir qual a rotação no eixo z.\n",
        "\n",
        "[08:10] Faltou uma coisa muito importante. Para que isso seja replicável na casa de vocês, é necessário que eu coloque aqui uma raiz manual, um seed manual, porque se não o que eu fizer aqui não vai ser replicável quando vocês forem fazer. Então eu crio um seed, rodo e agora podemos plotar os novos valores do meu perceptron. Agora está plotado de um jeito que facilita que visualizemos.\n",
        "\n",
        "[08:47] Vou aumentar mais um pouco. Na verdade, não aumentou porque não coloquei fig = plt.figure(figsize=(10, 8)). Agora ele vai estar maior para conseguirmos visualizar.\n",
        "\n",
        "[08:59] Maravilha. Vou dar um “f11”. Agora temos esse plano, que é o nosso perceptron, na prática. Fizemos um plot 3d com os pesos do perceptron. Então esse é o nosso modelo. Então agora vamos aprender como usamos e o que é a saída desse perceptron. Já instanciamos o perceptron com 3 entradas e uma saída e vamos realizar o forward simplesmente chamando o perceptron como uma função em relação a alguma entrada. Então ele vai dar uma saída que é a inferência dele.\n",
        "\n",
        "[09:33] Então eu vou definir uma entrada que vai ser um torch.tensor([0, -1, 2]). Eu estou definindo a partir do que eu vi, o x=0, o y=-1 e o z=2. Vai ficar um ponto acima do plano. Aí eu faço plot 3d do nosso perceptron, plot3d(perceptron) e ploto o x, plt.plot([x[0]], [x[1]], [x[2]], marker=^, marksize=20), vou colocar um marcador maior como sempre. Vamos ver?\n",
        "\n",
        "[10:18] Vamos ver, x0, x1, x2. Ah, escrevi errado, é markersize. Agora está certo. Então eu vou colorir ele de vermelho, aqui temos onde está o meu ponto no espaço e agora vamos imprimir a saída da minha camada. Eu chamei perceptron(x), lembrando que perceptron é a nossa camada linear aqui, que eu fiz o nn linear. Então se eu imprimir o y.\n",
        "\n",
        "[10:57] Opa! Faltou uma vírgula. Se eu imprimir o y, é um valor negativo, é menos 0.2 e ele está acima do nosso plano. Se eu trouxer esse ponto mais para cá, se eu trouxer o y para o positivo.\n",
        "\n",
        "[11:18] Agora o retorno dele é positivo, porque agora ele está abaixo do plano. Então quando eu uso esse meu perceptron, fazendo essa chamada que faz a referência a instância que fizemos em cima, instanciamos em cima o perceptron e embaixo ele me deu esse resultado, que é basicamente a transformação linear que fizemos na mão. Não preciso mais fazer essa transformação na mão, eu simplesmente uso a camada e ela vai fazer a transformação de um perceptron.\n",
        "\n",
        "[11:54] Então, revendo, basicamente o que aprendemos são essas duas linhas de código. Como instanciamos uma camada linear e como realizamos o forward, que é significa a informação que vai para frente. Então definimos um x, um ponto no espaço e fez o forward, ou seja, passou na camada e o y foi a saída da camada.\n",
        "\n",
        "No notebook Classificação-Linear.ipynb, ajustamos manualmente um modelo linear para solucionar um problema de classificação. Para isso, usamos uma semente fixa ao gerar a distribuição de dados. A linha de código a seguir é o instante em que a semente é fixada:\n",
        "\n",
        "np.random.seed(46)COPIAR CÓDIGO\n",
        "Altere essa linha de código no script original, fixando uma semente no valor de 199. Em seguida, encontre os valores de w1, w2 e b que solucionam o novo problema de classificação gerado. As alterações do novo modelo devem ser realizadas nas linhas de código que precedem o seu plot, como apresentado a seguir:\n",
        "\n",
        "w1 = 5 #a\n",
        "w2 = 1  #b\n",
        "b  = -0.4  #c\n",
        "plotmodel(w1, w2, b)\n",
        "Modelos lineares.\n",
        "Retas como classificadores lineares.\n",
        "Perceptron como classificador linear.\n",
        "Camada Linear do PyTorch para criar um Perceptron.\n",
        "[00:00] Olá, pessoal! Na aula de hoje vamos falar um pouco sobre funções de ativação. Nós já mencionamos sobre a existência dessas funções, porque elas são a segunda parte do que compõe o perceptron. Então já vimos aqueles dentritos, corpo, axônio. E o axônio que vimos é a parte que decide se vai ativar ou não a depender da saída do que foi calculado no perceptron.\n",
        "\n",
        "[00:26] Então vamos dar uma olhada agora no que é essa função de ativação e para que ela serve. Bom, para primeiro entendermos de forma abstrata, imagina que o papel do neurônio é ativar ou não ativar, a depender do estímulo que ele recebe. Então, por exemplo, você tem um neurônio na sua cabeça que ativa sempre que você ouve uma música boa. Vamos dizer, hipoteticamente falando que ele é o neurônio das boas sensações.\n",
        "\n",
        "[00:59] Esse neurônio não vai ativar se você bate o dedão em uma quina, porque ele não foi especializado nisso, não foi especializado em te fazer sofrer e fazer você sentir aquele sofrimento de bater o dedão em uma quina. Então você tem aquele neurônio que vai ativar caso o padrão que ele foi treinado, que ele foi especializado aconteça e que não vai ativar nos outros casos. A ideia basicamente é essa, quando falamos de perceptrons e redes neurais, a ideia é você treinar aquele neurônio para só ativar em determinados momentos.\n",
        "\n",
        "[01:37] Então basicamente é isso, em termos práticos, ele vai ativar quando ele enxergar um padrão de estímulos e o papel de função da ativação é definir se ele vai ativar ou não e qual a força de ativação. Isso vai depender do resultado do acúmulo das entradas.\n",
        "\n",
        "[01:57] Então, comparando agora de forma prática, ao nível de código mesmo, o que isso significa. Quando fizemos o nosso primeiro modelo linear, fizemos aquela função classify, que solucionava a equação da reta e tomava uma decisão. Então se o retorno da equação da reta fosse maior que 0, nós dizíamos que era categoria 1, se não ele dizia que era categoria 0. Isso no nosso código ficou dessa forma. É bem simples. Resolve a equação da reta para um dado ponto e avalia o retorno se está de um lado da reta ou se está do outro lado da reta.\n",
        "\n",
        "[02:35] Isso nada mais é que uma função linear. Então é daí que vem o termo função de ativação. Essa própria tomada de decisão que fizemos, é uma função de transformação do retorno do perceptron, o acúmulo de entradas do perceptron e ele vai transformar esses possíveis valores de retorno em valores de ativação, que em geral vão estar dentro de um intervalo mais definido.\n",
        "\n",
        "[03:07] Então o que temos aqui é que para f de x, que é a saída do perceptron, para f de x menor do que 0, essa era a nossa regra, ele vai transformar qualquer valor negativo em 0. Já para f de x maior do que 0, que nosso casso são os valores da outra classe, f de x maior do que 0, ele vai transformar esses valores do lado esquerdo em 1. Então é como se fosse um interruptor mesmo. Para valores negativos esse interruptor desliga, para valores positivos esse interruptor vai ligar.\n",
        "\n",
        "[03:41] Então passando adiante. A função limiar, como eu falei, é esse interruptor, que ou é 0 ou é 1. Se quisermos algo mais específico, podemos usar funções como a sigmóide, que faz algo parecido com o limiar, só que ao invés de dar uma opção binária para você, ele vai te dar um intervalo entre 0 e 1 e assume qualquer valor dentro desse intervalo. Então para qualquer possível valor que o seu perceptron pode assumir, ele vai conseguir transformar para um valor dentro do intervalo 0 e 1.\n",
        "\n",
        "[04:19] Então ao invés de você ter aquele degrau forte que tínhamos na imagem anterior, vamos ter agora uma função mais curva, uma não-linearidade mais suave que dado, por exemplo, um f de x menor que 5, ele vai assumir valor 0. Um f de x maior que 5, ele vai assumir o valor 1 e qualquer coisa no meio desses dois momentos, ele vai assumir valores que vão variar suavemente entre 0 e 1.\n",
        "\n",
        "[05:05] Então é interessante você ter esse tipo de variação mais suave, por quê? É interessante para, por exemplo, entender a certeza do seu perceptron.\n",
        "\n",
        "[05:18] Então você tem esses 3 estados, exemplo que eu falei, mas lembrando que ele pode assumir qualquer valor nesse intervalo, mas dado que você treinou o seu perceptron para identificar um padrão, se ele ativar com o valor 1, significa que ele tem certeza que ele enxergou aquele padrão. Se ele ativar com o valor 0, ele tem certeza que ele não enxergou aquele padrão. Mas se ele ativar com 0.5, 0.4, 0.6, significa que ele não sabe bem se ele enxergou o padrão ou não.\n",
        "\n",
        "[05:51] Então ao invés de você ter um degrau, uma função degrau que não vai te dar essa informação da certeza, a sigmóide já dá um pouco mais dessa informação.\n",
        "\n",
        "[06:01] Então temos várias funções de ativação, que eu coloquei aqui mais populares, mas cada uma delas traz uma vantagem matemática, uma vantagem de convergir mais rápido, de otimizar melhor, mas aí penso que não vale a pena entrarmos nesses detalhes da matemática. Mas temos a própria limiar, que não se usa, sinceramente, ela é muito simples para acrescentar esses modelos, tem a sigmóide, que já é bastante utilizada, vamos saber em que condições usar cada uma delas. Tem a tangente hiperbólica, que diferente da sigmóide, ela vai variar entre 1 e 0.\n",
        "\n",
        "[06:50] Então o valor máximo dela vai ser 1 e o valor mínimo vai ser 0. Então ela vai variar entre esses valores, mas ela vai fazer mais ou menos a mesma coisa que a sigmóide, só que permitindo valores negativos.\n",
        "\n",
        "[07:04] E temos também a Relu, que é bastante popular, e quando estivermos falando de redes com mais de uma camada, vamos entender melhor o papel da Relu, mas basicamente o que ela faz, ela é linear para valores positivos e simplesmente corta valores negativos os transformando em 0.\n",
        "\n",
        "[07:25] Então, isso tem vantagens matemáticas, na hora de calcular as derivadas isso tem uma vantagem, mas também vamos entender quando é melhor usar essa função. Tem outras também, mas essas são ativações mais populares que se usa. E o que vamos ver mais para frente é que quando usamos redes com mais de uma camada, essas funções de ativação são essenciais. Então elas não são um acréscimo opcional ao seu perceptron. Nós vamos ver as condições em que elas são obrigatórias e vamos ter que fazer a melhor escolha de uma função de ativação.\n",
        "\n",
        "[08:02] Graças as funções de ativação conseguimos solucionar problemas mais complexos. Então eu vou mostrar para vocês, na prática, qual é o impacto de colocar e tirar uma função de ativação. Mas no próximo vídeo vamos conhecer na prática as funções mais populares.\n",
        "\n",
        " DISCUTIR NO FORUM\n",
        "[00:00] Olá, pessoal! Na aula de hoje vamos ver na prática como se usa as funções de ativação no Pytorch e para isso não vamos precisar de nenhum módulo diferente dos que estamos usando, vai ser só o torch normal e o módulo nn, que é o de redes neurais. Se vocês forem à documentação do nn, vocês vão ver que eles têm várias camadas, tudo que tem escrito layer é camada de rede neural, então tem camadas convolucionais, pulling padding e tem ativações não-lineares. São essas ativações que estamos interessados.\n",
        "\n",
        "[00:34] Então tem dois links, um grupo de ativações lineares e os outros grupos não-lineares. Então se vocês clicarem na parte de camadas no documento, tem todas as ativações não-lineares, inclusive algumas que não comentamos, porque são várias. Mas é a partir desse pacote que é simplesmente “nn.” e o nome da função de ativação que conseguimos usar as funções.\n",
        "\n",
        "[00:59] Então vamos com calma. Vamos primeiro fazer os imports dos pacotes. Aqui, como eu falei, só vamos precisar do torch normal e do nn.\n",
        "\n",
        "[01:12] Agora para fazermos na prática o perceptron com as funções de ativação, vamos reaproveitar o exemplo que fizemos na aula de classificação linear. Então só relembrando, trabalhamos com essa distribuição de dados para classificar essa distribuição pelas duas classes que estão apresentadas.\n",
        "\n",
        "[01:49] Então o que precisamos só é fazer a mesma distribuição de novo e para isso podemos usar o mesmo seed, porque o que é interessante da aula de hoje é que vamos fazer isso só com o Pytorch e usando funções de ativação. E nós copiamos nossa função plotmodel, que foi a nossa solução para o problema de classificação da aula. Então está tudo aqui. Vamos ver.\n",
        "\n",
        "[02:22] Está tudo aqui. O nosso modelo que corta relativamente bem. Tinha até um teste que fizemos para ver qual era o retorno do nosso modelo, mas o que estamos interessados é como faz isso no Pytorch. Então vamos lá.\n",
        "\n",
        "[02:37] Primeira coisa que vamos fazer é o que já fizemos quando aprendemos como instancia uma camada linear, então nesse caso vamos instanciar, vou colocar a nossa camada, vou chamar de perceptron igual fizemos na última aula. Vai ser perceptron = nn.linear e o tamanho da entrada é definido pela dimensionalidade do seu dado. Nesse caso temos dados em duas dimensões, dimensão x1 e x2.\n",
        "\n",
        "[03:07] Então um ponto aqui é uma coordenada x1, x2. Então a nossa entrada tem tamanho 2. Como fizemos uma solução de equação linear que nos retorna um único valor, a dimensionalidade da saída é 1, porque isso é um perceptron simples. Então se formos colocar em termos do perceptron, como ele funciona, é um perceptron com duas entradas e uma saída, bem simples, que dá uma saída e recebe duas entradas. Essas entradas são as coordenadas x1, x2, dado que a x1 seria embaixo e em cima seria a x2.\n",
        "\n",
        "[03:54] Então é um perceptron que recebe duas entradas, faz o somatório e dá uma saída que interpretamos para fazer a classificação. Então um perceptron normal, como eu falei, nós podemos se guiar pelo número de traços, ele recebe duas entradas e dá uma única saída. Vamos lá.\n",
        "\n",
        "[04:16] Temos o nosso perceptron e agora vamos colocar depois desse perceptron, uma ativação sigmóide, que foi aquela que vimos que varia entre 0 e 1, transforma qualquer intervalo de valores em um intervalo de 0 e 1, só que de forma a informar qual é a certeza daquela ativação. Se for 0.5, significa que não tem tanta certeza assim. Então, é só chamar sigmoide = nn.Sigmoid(). Os nomes das funções sempre vão ser em inglês, então vocês podem consultar a documentação de ativações não-lineares.\n",
        "\n",
        "[04:56] Então agora temos uma camada linear, que é o nosso perceptron e uma ativação sigmóide. E o que vamos fazer é substituir os pesos, lembra que nós já vimos como consulta os pesos de uma camada, então eu posso imprimir print(perceptron.weight.data). Ele vai iniciar aleatoriamente, com pesos aleatórios. E eu posso imprimir o bias também. E quando eu rodar de novo, vai ser outro conjunto de pesos, porque cada vez que eu iniciar, ele vai gerar novos pesos aleatórios.\n",
        "\n",
        "[05:32] Só que o que eu quero é replicar exatamente o que fizemos anteriormente. Então eu vou pegar esses pesos da nossa equação da reta e vou colocar no meu perceptron. É como se eu já tivesse treinado ele. Como não chegamos na parte da otimização ainda, eu vou usar os pesos que otimizamos na mão.\n",
        "\n",
        "[05:49] Então eu posso colocar perceptron.weight e atribuir um novo valor a ele. É só eu criar um objeto do tipo nn.parameter, é o parâmetro. Então se eu colocar perceptron.weight = nn.Parameter(torch.Tensor([5, 1[)) e dentro eu vou colocar o meu tensor. Qual é o tensor que queremos? Queremos o com os pesos w1 5 e w2 1, então colocamos como float e ele já vai converter. E o perceptron.bias = nn.Parameter(torch.Tensor([-0.4])) vamos colocar só um tensor, -0.4.\n",
        "\n",
        "[06:35] Agora temos o nosso perceptron com os pesos do modelo que treinamos na mão, porque somos muitos bons e treinamos os modelos com o poder da nossa mente. Precisa ter uma dimensão a mais, isso são detalhes que só vamos pegar melhor quando falarmos de redes com mais perceptrons, mas por hora só precisa saber que o vetor de pesos tem que ser uma matriz, tem que ter duas dimensões, então temos que colocar uma dimensão a mais no primeiro perceptron. Mas calma, que quando falarmos de múltiplas camadas e múltiplos perceptrons na mesma camada isso vai fazer mais sentido.\n",
        "\n",
        "[07:18] Eu refiz os pesos e o viés, lógico, se eu imprimir ele vai ter os novos pesos e o novo viés. E é um modelo definido por essa reta.\n",
        "\n",
        "[07:34] E agora podemos usar com a função de ativação, para ver o que isso significa em termos práticos para nós. Então o que vamos fazer agora é usar esse perceptron para ativar para diferentes pontos e ver como esses pontos se comportam. Eu já selecionei na mão quais são os pontos, então dessa distribuição de dados que eu fixei com o seed, se vocês se lembram bem, eu fixei essa distribuição, então ela é sempre a mesma. Então eu escolhi alguns pontos estratégicos para entendermos como funciona a função de ativação.\n",
        "\n",
        "[08:13] Eu coloquei até os índices desses pontos que eu selecionei. Então vamos lá. Nós vamos trabalhar com esses pontos, for k, idx in enumerate([17, 21, 43, 66]);, eu quero esses pontos. E dentro eu vou transformar esse ponto da minha distribuição em um tensor, então x = torch.Tensor(X[idx]). Lembrando que o x é a variável que guarda os dados da minha distribuição e y é o que guarda os rótulos da classe. Então eu selecionei um ponto dessa lista de pontos, e agora vou usar a minha rede.\n",
        "\n",
        "[09:09] Então já sabemos também como usa, eu pego o retorno ret = perceptron(x) a chamada do meu perceptron em relação a x. E o retorno final, que é o retorno ativado, é só eu chamar a sigmóide, que eu chamei de sigmóide em português mesmo e passo agora o que foi retornado do perceptron act = sigmoide(ret). Então presta atenção que eu alimentei o x para o meu perceptron, ele está na variável retorno e passei a variável retorno para a função de ativação sigmóide. Então eu estou ativando a saída do perceptron. Aí ativei.\n",
        "\n",
        "[09:46] E agora eu vou fazer o seguinte, vou comparar o que a ativação retornou, o que o perceptron retornou e o que uma função linear faria, que o que temos feito até agora. Então act_limiar = 0 if ret.data < 0 else 1. Então ele vai retornar 1 se o retorno for positivo e retornar 0 se o retorno for negativo. Uma função limiar simples.\n",
        "\n",
        "[10:24] Então agora vamos plotar essas coisas. Eu só vou criar um rótulo do plot, vocês vão entender o que é isso. Mas eu só vou criar um rótulo para conseguirmos comparar as 3 coisas. Então a variável que se chama retorno vai ter o float formatado, então label =ret: (:5.2f).format(ret.data.numpy()[0]), isso tudo já vimos nas aulas anteriores, eu estou selecionando os dados do tensor, transformando para Numpy e pegando os valores da dimensão única.\n",
        "\n",
        "[10:58] Eu quero isso, eu quero também o retorno do limiar, vai ser limiar: (4.2f).format(act_limiar) + act: (:5.2f).format(act.data.numpy()[0])`. Então eu tenho 3 strings para compararmos os resultados dos 3 modos que fizemos a captura dos valores.\n",
        "\n",
        "[11:48] Então agora é só eu plotar essas coisas. Então eu vou fazer plt.plot(x[0], x[1], marker=) vou plotar esse ponto com um marcador diferente para ele ficar diferente. Só que agora eu vou criar uma lista de marcadores e uma lista de cores para que cada ponto fique com uma cor diferente e podermos comparar. Então eu quero os marcadores um triângulo para cima, um triângulo para baixo, um para a direita e um para a esquerda. E as cores vão ser vermelho, verde, azul e cinza.\n",
        "\n",
        "[12:28] E embaixo plotamos uma única figura com todos esses pontos. plt.figure(figsize=(8, 6)), vamos fazer uma figura maior para conseguirmos enxergar. E no plot eu pego o marcador e a cor que eu quero. marker=markers[k], color=colors[k]. Se a visualização não ficar muito boa, eu faço o restante das coisas. Só tenho que plotar a legenda plt.legend() e apresentar esse gráfico plt.show().\n",
        "\n",
        "[13:07] Por enquanto eu só plotei os pontos. Agora eu preciso plotar o modelo e o restante dessa distribuição para compararmos.\n",
        "\n",
        "[13:15] Eu vou plotar a distribuição é só chamar o ´plotmodel. Vou usar os mesmos pesos de antes, entãoplotmodel(w1, w2, b)podemos usar o w1, w2, e b, porque copiamos os valores, não tem problema. Vou aumentar um pouco os marcadores. Vamos ver se está legal agora. Agora está mais legal. Faltou só a legenda aparecer, porque eu não coloquei o *label* nos plots.label=label`. Vamos lá. Agora sim. Vou dar uns espaços em branco para não ficar confuso.\n",
        "\n",
        "[14:08] Agora vamos ver o que aconteceu. Quando eu peguei o ponto vermelho, que está bem distante de nós, o retorno do meu perceptron é -18, uma função limiar me diria que o retorno é 0, porque esse ponto está de um dos lados da classificação e a função de ativação também está retornando 0, porque esse ponto está bem distante, então ele retornou 0, que está no extremo da função sigmóide. -1.54 é esse ponto verde, ele está próximo do nosso modelo, mas ainda está negativo. Aí a função limiar vai retornar 0 também.\n",
        "\n",
        "[14:55] Ou seja, o que acontece é que tanto faz se o ponto está bem próximo da fronteira ou se o ponto está bem distante, a função limiar vai retornar a mesma coisa e você não tem a menor noção se esse ponto pode ser um erro do seu modelo, pode ser um ponto que está muito próximo da fronteira de decisão, ou se ele realmente faz parte da distribuição, porque ele está tão distante, que só pode fazer parte.\n",
        "\n",
        "[15:21] E o azul a mesma coisa. O -1.48 está próximo também, o limiar retorna 0 e a função de ativação retorna 0.19, dizendo que ele está nas proximidades. E o último é esse bem distante cinza, que o retorno retorna 16, o limiar vai dar 1, finalmente esse pertence a outra classe e a ativação também vai dar 1, porque com certeza esse ponto pertence à classe amarela.\n",
        "\n",
        "[15:50] Então o que eu vou fazer agora é puxar o meu modelo mais para a esquerda, para ele ficar em cima dos pontos que eu selecionei. Então se eu puxo ele mais cá, ele está passando à esquerda da origem agora, então eu tenho que alterar no perceptron, eu posso até colocar w1, w2 e b, que aí eu não preciso ficar mudando os valores na mão.\n",
        "\n",
        "[16:15] Aí agora ele mudou o valor do bias e deslocou a minha reta. Então se eu rodar esse novo modelo, o que vai acontecer é que agora vai dar exatamente 0.5 para os valores que estão em cima da reta, na zona de confusão.\n",
        "\n",
        "[16:32] Então uma coisa que precisamos prestar bastante atenção é que isso é muito intuitivo, essa função de ativação é muito intuitiva, para mostrar pontos que estão em cima da zona de confusão e que não podemos confiar na inferência. Ou temos que tomar cuidado com a inferência naquela região, porque o próprio modelo não tem certeza da classe, devido à região que o ponto está. Então é muito mais intuitivo que a função limiar, que ela agora vai dizer que todos esses pontos valem 1, porque eles estão do lado da classe amarela, mas, na verdade, estão na zona de confusão e não temos certeza que classe eles pertencem.\n",
        "\n",
        "[17:13] A mesma coisa aqui, se eu pegar a função da tangente hiperbólica, e agora eu troco a ativação. Eu vou chamar de activation, que é mais genérico. Vou trocar a ativação para tangente hiperbólica.\n",
        "\n",
        "[17:26] O que vai mudar é que agora ele permite valores negativos e poderemos classificar a saída do nosso modelo pelo sinal. Então se for negativo é uma classe, se for positivo é outra classe. E o que estiver próximo de 0, está na zona de confusão. De novo, muito mais interpretável do que uma decisão binária simples.\n",
        "\n",
        "[17:47] E por fim, tem a Relu, que ela não é adequada para esse tipo de problema, mas só para vermos como ela se comporta.\n",
        "\n",
        "[17:57] Ela só vai atribuir zero para quem for negativo, mas esse ponto distante que assume valor 18, mantém o valor 18. E esses que assumem valor 0 12, 0 06, ele mantém os valores. Então ela é uma função que só trunca valores negativos e preserva o valor original dos pontos com retorno positivo.\n",
        "\n",
        "[18:20] Então cada função vai ter o seu papel, é bem simples de usar e nas próximas aulas vamos dar uma olhada nos diferentes problemas, como você cria arquiteturas, como você soluciona problemas diferentes e essas funções vão fazer muito mais sentido.\n",
        "No notebook Ativações.ipynb geramos uma distribuição aleatória correspondendo a um problema de classificação binária. Em seguida, comparamos a saída original do perceptron com diferentes funções de ativação: limiar, Sigmoide, ReLU e Tanh. A comparação foi feita para 4 diferentes pontos da distribuição aleatoriamente gerada, correspondendo aos índices [17, 21, 43, 66].\n",
        "\n",
        "Selecione diferentes pontos da distribuição para conhecer mais a fundo o comportamento das funções de ativação que experimentamos. Deve-se rodar o script Ativações.ipynb em sua forma original, alterando somente a penúltima célula na seguinte linha:\n",
        "\n",
        "for k, idx in enumerate([17, 21, 43, 66]):\n",
        "[00:00] Olá, pessoal! Agora que já entendemos bem o perceptron, vamos passar para redes mais completas, com mais de um neurônio. Até o momento falamos que um perceptron só, só consegue modelar uma única reta ou um único hiperplano. Trabalhamos com 2 ou 3 dimensões, mas é escalável para quantas dimensões a sua entrada tiver. Apresentamos, como exemplo ilustrativo, o problema de separar cachorro de gato.\n",
        "\n",
        "[00:30] Tenho uma reta no espaço de duas dimensões que separa cachorro de gato com um único perceptron. E se você tiver um problema de múltiplas classes? E para falar de múltiplas classes, que o negócio começa a ficar um pouco mais interessante, eu trouxe os meus bebês da vida real, esses 3 cachorrinhos são da família, essas 3 calopsitas também são da família.\n",
        "\n",
        "[00:56] Agora vamos classificar os meus bebês, só os gatos que são da internet, porque eu não tenho gatos. E se você tiver um problema como esse? Ainda estamos em duas dimensões, mas agora queremos separar 3 classes entre si. Um único perceptron, por mais que o problema seja linearmente separável, não vai conseguir, porque não existe uma única reta que você consiga separar essas 3 classes.\n",
        "\n",
        "[01:24] nesse caso começa a magia das redes neurais. A coisa mais intuitiva que sabemos até agora, seria treinar múltiplos perceptrons especializando cada um em um padrão. A ideia seria treinar um classificador de cachorro, que seria o classificador azul, treinar um classificador de gato, que seria o vermelho e treinar um classificador de calopsita, que é o verde. Eu teria 3 retas, 3 perceptrons, cada perceptron é uma reta e eu especializaria cada perceptron em uma das classes. Super intuitivo.\n",
        "\n",
        "[2:11] É basicamente isso que fazemos no mundo real, no mundo das redes neurais, só que agora vamos ver uma representação um pouco melhor para começarmos a visualizar as redes como elas se conectam. A representação que vamos usar é em camadas. O que sabíamos até agora era 1 neurônio com entradas e saídas. Agora vamos ver um neurônio com uma camada de entrada, que vai ter a quantidade de dimensões que a sua entrada tiver e até agora uma camada de saída, com um neurônio.\n",
        "\n",
        "[02:49] A representação teria essa cara. Toda vez que eu falar da camada de input, eu estou falando da camada que traz as dimensões do meu dado. Já cansamos de falar que x1, x2, são os eixos de um plano cartesiano mesmo e os valores x1, x2, são os valores de cada uma das dimensões. São as coordenadas. Não precisa ser só um plano cartesiano, na verdade, podemos falar de atributos, de idade e formação escolar, para classificar pessoas que podem entrar na faculdade ou não, não sei.\n",
        "\n",
        "[3:33] Então isso são os dois atributos de entrada e o ponto azul é uma camada de saída, que tem um único neurônio. Isso equivale a um perceptron, que equivale a duas entradas, que aceita entradas com 2 dimensões e emite uma saída só, o perceptron só emite uma saída.\n",
        "\n",
        "[03:54] O modelo que propomos, com 3 perceptron, sendo cada perceptron especializado, seria representado dessa forma. Uma camada de entrada com duas dimensões e uma camada de saída com 3 neurônios.\n",
        "\n",
        "[04:09] Portanto, teríamos essa rede neural. Essa já é a representação de redes neurais. Uma camada de entrada com duas dimensões, estamos em um plano 2D e 3 perceptrons na camada de saída. Todas essas conexões, na verdade, são as conexões da primeira dimensão, com todos os neurônios e da segunda dimensão com todos os neurônios também.\n",
        "\n",
        "[04:36] O neurônio azul, que é o classificador de cachorro, vai acumular as informações das duas dimensões de entrada e emitir uma saída, essa saída que vamos interpretar como classificador de cachorro, o mesmo para o vermelho, que vai receber 2 entradas e emitir uma saída dessa vez se referindo aos gatos e o verde vai fazer o mesmo para as calopsitas.\n",
        "\n",
        "[05:01] Agora vamos visualizar uma ferramenta muito bacana, que é da Universidade de Stanford e que nos ajuda a entender como funciona esses modelos com múltiplos perceptrons. O que vamos ver são classificadores lineares que solucionam a equação da reta como fizemos manualmente, só que aqui conseguimos fazer essa interpretação como se fossem perceptrons.\n",
        "\n",
        "[05:25] Ele está colorindo de azul a fronteira de decisão dessa reta azul, de verde a fronteira de decisão da reta verde e de vermelho a fronteira de decisão da reta vermelha. Aqui eu tenho meu peso, w1, w2 e o meu viés. Vou mexer na reta azul. Se eu mexo no viés, essa reta só se desloca no espaço, sem rotacionar. Se eu mexo no peso, qualquer um dos pesos, eu estou rotacionando essa reta. Eu posso fazer isso com qualquer uma das retas, a verde ou a vermelha. E cada vez que eu mexo nos pesos, cada vez que eu ajusto os pesos, eu estou criando uma nova fronteira de decisão.\n",
        "\n",
        "[06:10] Quando trabalhamos com mais de um perceptron, nós precisamos avaliar a saída dos 3 perceptrons, nesse caso aqui, e comparar quem emitiu uma ativação mais forte. Isso nós vamos ver em maiores detalhes. Essa ferramenta permite que você simplesmente comece um treinamento, tem um botão “start update”. E ela vai ajustar os pesos para você, ela vai definir as fronteiras de decisão que separam esses 3 dados.\n",
        "\n",
        "[06:40] Ela ajustou, já podemos parar, já está com 100% de acurácia e agora ela definiu essas 3 retas, esses 3 modelos que conseguem classificar distribuições com múltiplas classes. Vamos voltar para o nosso slide.\n",
        "\n",
        "[07:01] Para problemas de classificação com múltiplas categorias temos que aprender a interpretar a saída. Antes de falarmos de softmax, que é uma função de ativação, vamos pensar em uma coisa simples, vamos pensar que queremos interpretar a saída azul, vermelho e verde, eu não tenho essas cores, vou usar preto para todos. E vamos dizer que eu não tenho ativação nenhuma nesses neurônios.\n",
        "\n",
        "[07:33] A fronteira de decisão do azul disse 2, porque ele está do lado positivo do azul. No vermelho disse -1, está do lado negativo do vermelho. E no verde disse 1.5, está do lado positivo do verde. Isso significa o quê? Que o classificador de cachorros ativou mais forte. Se ele ativou mais forte é mais provável que essa imagem de entrada de 2 dimensões seja de um cachorro. Só que o temos de ativação até agora, o verde também ativou com uma força parecida, talvez seja uma calopsita. Mas o que com certeza não é, é um gato, porque a ativação foi negativa, ou seja, está do outro lado da fronteira de decisão.\n",
        "\n",
        "[08:23] Isso é uma forma simples de interpretar uma rede neural com 3 perceptrons. Porém, também podemos usar a função softmax, que vai fazer isso de uma forma muito mais elegante. Ela vai pegar essas saídas, quaisquer que sejam elas, e transformar em uma distribuição de probabilidades.\n",
        "\n",
        "[08:44] Se temos saídas igual a 2, -1 e 0.5, vamos lá, linha a linha do código. Temos o módulo nn e o torch normal. Instanciamos uma função de ativação no softmax, que podemos chamar de camada de ativação também, uma softmax. E eu inventei uma saída, com os valores 2, -1 e 0.5. Eu estou imaginando que a saída da minha rede foram esses 3 valores. Vou passar na softmax e se eu imprimir esse output ativado, ele transformou em valores que somam 1, porque essa é a principal característica de uma distribuição de probabilidades.\n",
        "\n",
        "[09:34] Ele vai me dar que para a classe cachorro a ativação é muito mais forte que para a classe gato e para a classe calopsita. Olhando para isso como uma distribuição de probabilidades eu consigo avaliar melhor. Sempre que você ativar sua saída com a softmax, a saída vai somar 1 exatamente e você consegue avaliar a saída de cada neurônio como uma força de ativação. Qual neurônio ativou mais forte. Esse é o neurônio da classe predita, não sei se a rede está bem treinada e está prevendo direito, mas essa seria a predição dela para uma dada entrada.\n",
        "\n",
        "[10:14] Na camada linear. Agora vamos começar a alterar o parâmetro out_features, porque agora temos mais de uma saída. O in_features continua dependendo da dimensionalidade da entrada, só que agora o outr_features, vamos pensar nele como a quantidade de neurônios da camada de saída. Se eu tenho um único perceptron, eu vou instanciar uma camada linear com duas features de entrada, estamos falando de x1, x2 e uma feature de saída, porque eu tenho 1 neurônio na saída, um perceptron.\n",
        "\n",
        "[10:49] Para o nosso classificador de gato, cachorro e calopsita, a camada linear vai ter 2 features, mas agora vai ter 3 features de saída, 3 dimensões de saída, porque agora eu tenho 3 neurônios na saída. O que eu comentei com vocês que o número de entrada de features é o tamanho da sua entrada, quantos atributos, quantas dimensões você tem, sempre pensa no out_features como a quantidade de neurônios.\n",
        "\n",
        "[11:17] E no próximo vídeo vamos falar um pouco sobre múltiplas camadas. Essa aula vamos falar toda a teoria primeiro e no final vamos fazer uma prática bacana.\n",
        "[00:00] Olá, pessoal! Agora vamos falar de redes neurais com mais de uma camada, está começando a ficar interessante. Só que antes de falarmos do MLP, que é o Multi-Layer Perceptron, precisamos rever tudo que já fizemos só para termos uma noção de qual é o novo passo que vamos dar.\n",
        "\n",
        "[00:21]Até agora falamos de problemas que podem ser resolvidos com 1 ou múltiplas retas. No caso de ser resolvido com uma reta só, vem toda aquela questão do inverno da IA: “Ah, só modela retas, não serve para nada!”.\n",
        "\n",
        "[00:37] E aqui de novo múltiplas retas, já vimos isso como uma rede neural, com uma camada de saída e 3 perceptrons na camada de saída. Inverno da IA, “olha só, a rede neural não modela funções complexas, só modela retas!”. E aí que entra os problemas não-linearmente separáveis. Já falamos por alto deles, falamos do problema do ou exclusivo, mas vamos retomar. Muitos problemas do mundo real, na verdade, a maioria dos problemas do mundo real possui distribuições mais complexas e não são solucionáveis por transformações lineares.\n",
        "\n",
        "[01:15] Eu trouxe um exemplo só porque eu achei interessante mesmo, uma representação 2D de ressonâncias magnéticas. E o problema aqui é classificar qual o equipamento que coletou a ressonância magnética. Conseguimos dois grupos claros, só que não conseguimos separar com retas. Claro que se você usar infinitas retas, eventualmente você vai conseguir classificar, mas não é tão viável e tão fácil assim solucionar esse problema com retas.\n",
        "\n",
        "[01:48] O Scikit-Learn, nós fizemos o make classification com o scikit-Learn que dava um problema linearmente separável, mas se você quiser, lá também tem exemplos de problemas não-linearmente separáveis como os círculos, o make circles e é simples, ele dá o exemplo de como você usa a função, é só chamar a criação no dataset e plotar o dataset, o mesmo com o make moons, que é outra função não-linearmente separável, você precisa de uma função não-linear para separar esses dados.\n",
        "\n",
        "[02:22] E vamos ver justamente como solucionar esse tipo de problema, que uma distribuição tão simples como essa, uma rede com uma única camada não consegue resolver.\n",
        "\n",
        "[02:32] Temos nosso clássico problema do XOR e agora vamos tentar simular um problema para solucionar. Lembrando o problema do XOR com 4 pontos só é verdadeiro quando os dois pontos têm estados diferentes. Se A é 0 e B é 1, ele é verdadeiro, mas se A e B tem o mesmo estado, ou 0 ou 1, ele é falso. Então, ele cria essa distribuição que não é linearmente separável e isso que foi o grande barulho do nvernoinverno da IA, que fez as pessoas pararem de investir durante 20 anos em redes neurais.\n",
        "\n",
        "[03:12] Se trocarmos os estados de A e B por sinal, conseguimos criar uma distribuição como essa. Se para uma dada coordenada, tanto o A for negativo, quanto o B for negativo, que é o quadrante azul -1 e 0, portanto, é falso. Se A e B forem positivos, que é o quadrante de cima, então também é falso. Mas se A e B tiverem sinais diferentes, é verdadeiro. Criamos duas classes que simulam o problema do XOR de uma maneira mais complexa, com vários pontos no espaço, que facilita treinamos um modelo para tentar separar essas duas classes.\n",
        "\n",
        "[03:58] Só que, se usarmos um único perceptron, uma única reta, não conseguimos resolver esse problema. Por quê? Ele vai ajustar de forma a maximizar a acurácia e até consegue uma acurácia que olhando para ela parece até boa, ele consegue classificar 72% das amostras. Mas isso porque ele está simplesmente jogando fora um cluster de dados, por exemplo, nessa primeira solução superior esquerda, ele está simplesmente ignorando o fato de que tem amostras azuis desse lado da fronteira e está classificando tudo que está abaixo da reta como classe amarela e tudo que está acima como classe azul.\n",
        "\n",
        "[04:42] O mesmo ele faz nas outras 3 soluções, ele simplesmente escolhe um canto, onde ele consegue pelo menos classificar uma parte dos dados e define uma reta naquela região e sempre ignora que existem amostrar, ignora não, ele não tem mais o que fazer, não tem capacidade de solucionar o problema melhor do que isso.\n",
        "\n",
        "[05:05] Neste caso entra o Geoffrey Hinton com as contribuições que ele fez em 1986 sobre o algoritmo backpropagation e a não-linearidade em camadas intermediárias. Vamos entender tudo isso. Primeiro as camadas intermediárias. Essas camadas, isso significa agora que temos uma rede com não só uma camada de entrada e uma camada de saída, como também o que chamamos de camada escondida, que é a hidden layer. Se você vê camada intermediária, camada escondida ou hidden, significa camadas que não são de entrada nem de saída, são as camadas do meio da sua rede.\n",
        "\n",
        "[05:50] Temos uma rede neural com 1, 2, 3, 4, 5, neurônios, 5 perceptrons na camada escondida e 1 perceptron na camada de saída. Essas camadas, as camadas intermediárias, precisam ter ativações não-lineares. Ao final desse vídeo eu vou mostrar uma prática bem bacana que mostra a obrigatoriedade dessas ativações e aqui precisamos que essas camadas passem por ativações não-lineares e a camada de saída nós usamos aquela ativação que nos ajuda na interpretabilidade da classificação. E isso tudo também tem valor de ajudar a rede otimizar mais rápido, mas aí são os detalhes matemáticos.\n",
        "\n",
        "[06:35] Precisamos de camadas escondidas e precisa de ativações não-lineares nas camadas escondidas. E a outra grande contribuição do Hinton foi o algoritmo do backpropagation, que só para reforçar a ideia do que falamos até agora, temos o fluxo de forward, que é quando a entrada passa na rede e gera uma saída e tem o fluxo de backward, que é o fluxo para trás. Esse fluxo de backward é o cálculo do gradiente e a atualização dos pesos que vamos ver na aula de como a rede aprende. Mas essa também foi uma grande contribuição do Geoffrey Hinton, que vamos deixar em stand-by para conversar melhor depois.\n",
        "\n",
        "[07:18] Para o problema do XOR nós vamos experimentar uma arquitetura recomendada pelo Geoffrey Hinton.\n",
        "\n",
        "[07:25] Temos o problema do XOR, lembrando que essa distribuição aqui nós fizemos o ou exclusivo com sinais. E arquitetura, só para definirmos aqui, é o padrão de conectividade da sua rede, é a estrutura da rede. Vamos definir uma arquitetura com uma camada escondida com 6 neurônios e ativações não-lineares ReLu. E uma camada de saída com 1 neurônio, com ativação sigmóide. Por que 1 neurônio? Porque nós só temos 2 classes. Se trata de um problema binário, onde eu posso interpretar que se a saída for 0 é uma classe e se a saída for 1 é a outra classe.\n",
        "\n",
        "[08:03] E fica essa arquitetura, que é o que temos mostrado ao longo desse vídeo, só que com 6 neurônios na camada intermediária. E todas essas conexões, nada mais são do que cada uma das dimensões sendo interpretada por cada um dos neurônios da nossa rede. O que acontece? Nós passamos as entradas para essa camada escondida, ela vai transformar o meu dado e passar esse dado transformado para a camada de saída. Eu vou falar disso mais lá na frente, o perceptron continua sendo uma transformação linear, ele continua só sendo capaz de modelar retas.\n",
        "\n",
        "[08:44] Passamos por uma transformação não-linear, com as ativações não-lineares, que permitem que ele aprenda funções mais complexas. Então, vamos lá. Só a questão de nomenclatura, vocês vão encontrar isso como camadas totalmente conectadas, porque elas são totalmente conectadas, todas as dimensões conectam com todos os neurônios. Vocês podem encontrar como fully connected, como dense, porque ela é densa ou como Linear, que é o caso do Pytorch.\n",
        "\n",
        "[09:16] Eem outras frameworks, ela pode chamar fully connected FC, em outro pode se chamar dense, mas todos se referem a mesma coisa, que são redes neurais com a unidade fundamental, que é o perceptron e conectando todo mundo com todo mundo. Na verdade, o da camada anterior está totalmente conectado com a camada seguinte.\n",
        "\n",
        "[09:39] Veja que não tem conexão direta da entrada com a camada de saída. É hierárquico isso, precisa passar pela camada intermediária e a camada intermediária se comunica com a camada de saída. É bem hierárquico, é bem de iteração mesmo. Um depende da resposta do outro.\n",
        "\n",
        "[10:00] Em termos de código, isso ficaria dessa forma. Temos uma camada linear com duas dimensões de entrada, x1, x2 e 6 dimensões de saída, porque eu tenho 6 neurônios, significa que eu tenho 2 pontos na camada de entrada e 6 pontos, que são os neurônios na dimensão intermediária, passo uma ReLu, que é uma ativação não-linear e crio uma outra camada linear com 6 dimensões de entrada, eu tenho 6 pontos na camada anterior e uma dimensão na saída, que o meu perceptron. E coloco uma sigmóide no fim para ativar não-linearmente a saída da classificação.\n",
        "\n",
        "[10:50] O resultado da fronteira de decisão, olha só que bacana, consegue classificar praticamente 100%, infelizmente, olha esses pontos amarelos, eles estão na parte errada da fronteira de decisão. Mas é uma fronteira de decisão muito melhor do que conseguimos fazer com uma única reta e aqui vocês estão vendo que ela é uma função mais complexa, ela faz curva, uma única função que se encontra no infinito e faz curvas.\n",
        "\n",
        "[11:24] Agora vamos ver a Demo Interativa para encerrar essa aula, para vocês conseguirem ver a importância da ativação. Temos uma sintaxe que você só precisa definir uma lista de camadas. Fizemos praticamente a mesma coisa, fizemos uma camada de entrada com 2 dimensões, uma camada FC, fully connected, que é a mesma coisa da linear, com 6 neurônios e uma ativação ReLu e uma camada fully connected de saída com 3 neurônios e uma softmax na ponta.\n",
        "\n",
        "[11:59] Nós fizemos com 2 neurônios aqui por limitação mesmo do framework, mas a forma mais simples de implementar isso é com um único neurônio, mas dá para fazer com 2 também. E uma softmax porque agora estamos falando de mais de um neurônio na camada de saída, então a softmax vai transformar em uma distribuição de probabilidades. Vamos lá? Vamos dar uma olhada nessa ferramenta que é muito bacana.\n",
        "\n",
        "[12:26] O que vamos ter? Ele vai criar situações, que podemos definir qualquer situação por padrão ele criou distribuições circulares. Está vendo? 6 neurônios, 2 neurônios. Está até a ativação tangente hiperbólica, vamos simular exatamente o que queremos. 2 dimensões de entrada, 6 neurônios, queremos ReLu na intermediária, queremos nenhuma ativação na outra camada, porque temos a softmax na ponta, e é exatamente o que vimos mais cedo.\n",
        "\n",
        "[13:04] E olha só, ele consegue dividir esse problema que é não-linearmente separado. Agora o que eu vou fazer é tirar a ativação. Eu vou tirar a ativação da camada intermediária e vamos ver o que acontece. O que temos agora é uma camada intermediária sem ativação não-linear, uma camada de saída e uma softmax que simplesmente vai transformar a saída da rede em uma distribuição de probabilidades.\n",
        "\n",
        "[13:32] Olha só, ele só consegue modelar retas, ele não vai conseguir modelar algo diferente e obviamente ele não consegue solucionar um problema como esse. Porque esse problema é completamente não-linearmente separável. Ele até está tentando ajustar isso com uma reta, mas ele não vai conseguir, simplesmente porque eu tirei a ativação. Se eu der um “ctrl + Z” e colocar a ativação de novo, ele consegue solucionar o problema.\n",
        "\n",
        "[14:01] O papel da função de ativação não-linear na camada intermediária é extremamente essencial para aprender funções não-lineares, se não você não consegue solucionar a maioria dos problemas do mundo real e é por isso que surgiu o inverno da IA.\n",
        "\n",
        "[14:24] Descobriu-se que essa arquitetura hierárquica consegue aproximar funções mais complexas, não-lineares. Isso porque, e isso é como interpretamos hoje em dia, as características aprendidas na camada intermediária são linearmente separáveis. Essa camada faz um aprendizado de características para que a camada de saída consiga dividir isso linearmente.\n",
        "\n",
        "[14:49] O potencial de aprendizado hierárquico, essa ideia de aprendizado hierárquico que deu origem a famosa área de deep learning, deep porque é profundo, tem várias camadas, learning porque é machine learning, mas é legal pensarmos nisso como aprendizado hierárquico.\n",
        "\n",
        "[15:06] Só uma comparação bem breve para terminarmos essa aula. O machine learning tradicional tem duas etapas. A etapa de extração de características, onde o programador diz quais características são relevantes. No caso do cachorro eu posso procurar por pontas de orelha, por círculos, equivalente aos olhos. E aí eu vou treinar um modelo de classificação provavelmente, que não precisa ser rede neural, pode ser qualquer outro modelo, que vai me dizer que isso é um cachorro.\n",
        "\n",
        "[15:38] Só que o que eu faço com o deep learning é as duas coisas ao mesmo tempo, eu crio uma rede profunda que vai aprender características e treinar um modelo de inferência ao mesmo tempo. Portando, é isso que é o deep learning: o aprendizado de características para realizar inferências.\n",
        "\n",
        "[15:57] Só relembrando o teorema da aproximação universal, ele vai fechar o potencial, ele vai definir o potencial de uma rede neural, que uma rede neural com apenas uma camada escondida é suficiente para representar qualquer função. É aí que está o “Q” da coisa. Ela vai aprender as características, vai transformar o espaço da sua entrada, para que a última camada consiga separar isso de forma linear. No próximo vídeo vamos meter a mão na massa e fazer um MLP no Pytorch.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zrV8F2MdN_gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[00:00] Olá, pessoal! Nessa aula vamos ver no Pytorch como definimos uma arquitetura. Porque até agora só trabalhamos com uma única camada ou no máximo uma camada e uma ativação. Precisamos começar a pensar, a medida que vamos crescendo a nossa rede, colocando mais camadas na nossa rede, como transformamos toda uma arquitetura em um único objeto, porque não faz sentido cada camada ser uma variável solta no universo, se não fica extremamente confuso.\n",
        "\n",
        "[00:32] Antes de começarmos, vamos primeiro definir quais são os dados que vamos trabalhar. Nesse notebook não vamos treinar nem testar nada ainda, mas é importante sempre trabalharmos com dados mesmo para termos noção do que é cada dimensão.\n",
        "\n",
        "[00:50] Vou pegar um exemplo de dado de distribuição não-linearmente separável, porque já que vamos fazer uma rede com múltiplas camadas, ela consegue solucionar esse tipo de problema. Só falta importar o matplotlib, então: ìmport matplotlib.pyplot as plt` e rodamos a nossa distribuição.\n",
        "\n",
        "[01:14] Está aqui a nossa distribuição de dados não-linearmente separáveis. Primeira estrutura que vamos conhecer é a sequential. Ela é do mesmo módulo nn, que estamos acostumados a usar até agora, vamos fazer os imports de sempre, import torch e from torch import nn, que é o pacote de neural networks.\n",
        "\n",
        "[01:38] Ele é um contêiner, onde você pode colocar múltiplas camadas e na hora de fazer o forward ele vai passar hierarquicamente por cada camada que você colocou. Se eu crio a minha rede como um objeto tipo sequential, tudo que eu tenho que fazer é preencher aqui dentro quem são as camadas. Posso ter uma camada net = nn.Sequential(nn.linear() depois preenchemos o parâmetro e só separado por vírgulas eu coloco quem são as minhas outras camadas.\n",
        "\n",
        "[02:11] Quero uma linear, uma ReLu e outra linear, que são os requisitos básicos para uma rede capaz de aprender funções mais complexas. Eu preciso ter uma camada escondida, que é a primeira, a nossa hidden, eu preciso ter uma ativação não-linear, a ReLu, logo em seguida da camada escondida e essa camada de output que é a camada de saída. Esse par “camada escondida e ativação não-linear” é essencial para o aprendizado como vimos no vídeo anterior, o aprendizado de funções mais complexas.\n",
        "\n",
        "[02:49] Vou imprimir print(net). Ainda não temos, porque eu ainda não preenchi os parâmetros da camada. Quem é o número de entradas, quem é o número de saída e se lembramos bem, isso significa a dimensionalidade do meu dado, porque essa camada é a primeira camada da arquitetura e a saída disso vai ser um hiper parâmetro e vamos ver o porquê.\n",
        "\n",
        "[03:17] O que sabemos da nossa entrada? É uma entrada em 2 dimensões. Eu tenho as dimensões x1 e x2, o tamanho da minha entrada são 2 dimensões. Já sei que a primeira camada da minha rede vai ter entrada de tamanho 2. Outra coisa que eu já sei é que a camada de saída está relacionada com o meu problema.\n",
        "\n",
        "[03:42] Se esse é um problema de classificação binária, já vimos que podemos treinar um único perceptron para interpretar a saída dele como quanto mais próximo de 1 é uma classe, quanto mais próximo de 0 é outra classe, ou se usarmos os valores crus da saída do perceptron, maior que 0 está de um lado da reta, menor que zero está do outro lado da reta.\n",
        "\n",
        "[04:07] Posso fazer isso usando um único perceptron, que vai ser a saída da última camada. Então in_features ainda não preenchemos, mas já sabemos que a saída da última camada tem que estar relacionada com o problema. Então: out_features vai ter output size, que é um único perceptron de classificação.\n",
        "\n",
        "[04:29] Ativação linear não leva parâmetros, só resta sabermos quem é essa dimensão. Essa dimensão, nós que vamos escolher a depender da capacidade que desejamos, da complexidade do problema. Porque o que temos até agora é a seguinte rede. Sabemos que a entrada tem 2 dimensões, sabemos que é x1, x2, vamos ter uma camada hidden, que não definimos ainda e vai ter na ponta uma camada de output, que vai fazer a classificação.\n",
        "\n",
        "[05:10] É a entrada, hidden e a saída. O tamanho da camada escondida é um hiper parâmetro, é o programador, o analista, o arquiteto de redes que vai escolher, porque quanto mais parâmetros você colocar, mais capaz a sua rede vai ser de aprender padrões complexos. Faz todo sentido. Quanto mais parâmetros você tem, mais graus de liberdade você tem.\n",
        "\n",
        "[05:34] Posso colocar 4 neurônios, porque foi o que coube na imagem. E vamos ter essas conexões. Agora chegou a hora de desenhar linhas retas, que é o meu maior nêmesis. Vamos ter essas conexões em uma rede totalmente conectada. Vamos lá? E essas conexões.\n",
        "\n",
        "[06:02] Temos essa rede, que é o que estamos fazendo embaixo. Se eu for seguir literalmente o que essa rede está fazendo, o meu hidden size seria 4, que é o tamanho da camada escondida. Coloco a saída da primeira camada, que é hidden_size = 4 e consequentemente a entrada da camada de saída, também é 4. E isso é um padrão que vamos seguir sempre.\n",
        "\n",
        "[06:30] A saída de uma camada tem que ser exatamente o mesmo tamanho da entrada da próxima camada, a menos em casos específicos que não vamos abordar nesse curso. Por regra de ouro, a saída de uma camada é o tamanho da entrada da próxima.\n",
        "\n",
        "[06:48] Se imprimirmos isso agora, agora sim, temos a nossa rede que é exatamente essa que eu desenhei lindamente para nós. Só que eu vou apagar esse desenho e vou colocar um tamanho um pouco maior, que seria o tamanho que eu usaria para solucionar aquele problema. Pode ser 8, 12, 16, mas 4 está pouco.\n",
        "\n",
        "[07:11] Só um pacote de visualização que vale a pena vocês conhecerem é o torchsummary, ele vai dar um pouco mais de informações a cerca da sua rede, por exemplo, quantos parâmetros ela tem. Eu só preciso passar por parâmetro a rede e qual é o tamanho da entrada. É requisito do pacote.\n",
        "\n",
        "[07:34] E ele vai nos dar quantos parâmetros vai ocupar. Sabemos que com 8 neurônios na camada escondida eu tenho 24 parâmetros e na camada de saída, como eu só tenho um perceptron conectado a 8 neurônios de saída, ela tem 9 parâmetros. Tenho 33 parâmetros é uma rede bem pequena.\n",
        "\n",
        "[07:54] Na hora de fazer o forward com os nossos dados, quais são os passos que temos que lembrar? Temos que transformar a entrada em um tensor, vou chamar de tensor para não esquecermos. tensor = torch.Tensor(X1);. E vamos passar esse tensor para a nossa rede. Vou transformar do jeito que está, depois eu vou transformar, porque o tipo desse dado, vou deixar para vocês verem o que vai acontecer.\n",
        "\n",
        "[08:30] E na hora que eu for fazer a predição é só chamar a minha rede e passar o tensor de entrada. Um detalhe é que, na verdade como isso se trata de um array numpy, a função correta é from numpy. Se fosse uma lista eu podia chamar o torch,Tensor. Acabei me confundindo. No caso de arrays numpy, como o nosso dado, eu chamo o from_numpy, então: pred = net(entrada). Agora eu só vou imprimir print(pred.size(])para vermos o tamanho da saída.\n",
        "\n",
        "[09:05] Deu erro. Por quê? Deu erro porque eu errei o nome, mas agora eu vou mostrar para vocês o erro correto.\n",
        "\n",
        "[09:13] Deu erro porque o nosso dado é um float 64, o array numpy é um float 64. E por padrão, as redes são carregadas em float 32. Quando você carrega dados do Numpy, às vezes ele vai carregar como float 64 e é só escrever .float para corrigir isso. Na hora que eu fizer o .float ele vai converter float 32 e aí eu vou conseguir fazer o forward na rede.\n",
        "\n",
        "[09:42] Nós tivemos uma saída de 300 por 1. Por quê? Porque eu imprimir o shape da minha entrada, eu tenho 300 amostrar com 2 dimensões. Essas 2 dimensões passaram na minha rede e se transformaram em 300 predições. A predição só tem uma dimensão, que é ou uma classe, ou outra classe, a saída vai ser a predição das 300 amostras. Posso passar para a minha rede múltiplas amostras de uma vez e mais para frente vamos ver como isso é importante. Mas para 300 dados com 2 dimensões, eu tenho 300 predições de um único perceptron na camada de saída. Faz todo sentido.\n",
        "\n",
        "[10:24] No próximo vídeo vamos ver a segunda forma de agruparmos as camadas em um objeto só, que é através da classe nn.modulo.\n",
        "\n"
      ],
      "metadata": {
        "id": "o8xgT8RBcDEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos falar mais um pouco de como nós criamos redes neurais maiores, com mais camadas. E como esse vídeo é uma sequência direta do vídeo anterior, vamos só lembrar que dado e que problema estamos usando. Estamos falando de um problema de classificação, onde os dados têm uma distribuição não-linearmente separável. Não existe reta que separe essas duas classes de forma satisfatória.\n",
        "\n",
        "[00:27] Nesse caso, precisamos de uma rede com pelo menos uma camada escondida. Já aprendemos como faz isso com a estrutura do sequential, agora vamos aprender usando a classe nn.modulo. Se você for na documentação do módulo, ele vai te dizer que o módulo é a classe principal, a classe base para modelos neurais. O seu modelo deve ser uma subclasse dessa classe. O que precisamos fazer é implementar nesses moldes a nossa rede neural.\n",
        "\n",
        "[01:09] Nós podemos copiar, mas eu vou preferir fazer do zero esse, porque faz diferença, mas o que sabemos é que a nossa rede vai ser uma classe, que vamos nomear, vamos definir e no caso precisamos definir dessa forma, que se trata de uma subclasse da classe módulo. Essa classe módulo implementa uma rede neural de uma forma geral e os detalhes nós implementamos na subclasse.\n",
        "\n",
        "[01:35] Vamos definir a classe “minha rede”, porque a classe é minha, eu posso chamar de batata, posso chamar do que eu quiser. E vou indicar que ela é uma subclasse da classe módulo. Os métodos obrigatórios que eu tenho que implementar é o método init, sempre acompanhado do self, então: def __init__(self), para falar que é um método pertencente a classe. E o forward sempre acompanhado do self, para indicar que é um método da classe.\n",
        "\n",
        "[02:04] No caso do forward ele também tem um parâmetro obrigatório, que é o parâmetro. Você pode chamar de x, pode chamar de input, pode chamar do que você quiser. Mas eu prefiro chamar de x mesmo para facilitar, que fazemos o mapeamento de x para y, eu pessoalmente acho intuitivo. Esses dois são os métodos obrigatórios.\n",
        "\n",
        "[02:26] No init, como é uma sublasse, precisamos inicializar a classe superior, a classe mãe, digamos assim. Essa chamada de super é obrigatória e no lugar de model, vamos colocar o nome da nossa rede, que é minha rede. Então: super(MinhaRede, self).__init__() vamos inicializar a classe super.\n",
        "\n",
        "[02:53] Agora, o que fazemos no init e o que fazemos no forward? No init vamos definir arquitetura e no forward vamos fazer o forward mesmo, que é processar, gerar uma saída a partir do x. Que é o processamento, é o forward mesmo. Vamos definir a mesma arquitetura que fizemos no vídeo anterior, que são duas camadas lineares intercaladas por uma ReLu. Vou ter a camada escondida, self.hidden = nn.Linear(). self.relu = nn.relu() e self.output = nn.linear().\n",
        "\n",
        "[03:49] Precisamos chamar self.hidden, self.relu, essas coisas assim, para indicar que esse é um atributo da classe também, acessível de qualquer lugar da classe. Se eu quiser acessar esse self.hidden e eu vou precisar disso embaixo, eu posso acessar porque eu sei que um atributo da classe, um objeto da classe.\n",
        "\n",
        "[04:13] No método init, precisamos colocar os parâmetros da camada linear, que já sabemos que é o input_size e o hidden_size, que é o hiper parâmetro escolhido por quem definir essa rede. E na camada de saída é hidden_size, output_size. Se você não se lembra como essas coisas são definidas, vale a pena voltar no vídeo anterior, que fizemos bonitinho do que se tratam essas coisas.\n",
        "\n",
        "[04:40] Mas aqui vale lembrar que é hierárquico, portanto, roda primeiro a camada linear, depois a ReLu, depois a outra linear e quem vai definir isso no caso da classe é você na hora do forward. Definimos a arquitetura da nossa rede, mas esses valores ainda não existem. Podemos definir esses valores como parâmetros desse método init. Então, input_size, hidden_size e o output_size são valores que quem definir essa rede depois, vai dizer quem são esses valores. Essa classe fica genérica e consegue fazer redes de diferentes tamanhos.\n",
        "\n",
        "[05:17] Na hora de fazer o forward, só vamos passar a entrada na rede e retornar a saída. A minha variável hidden vai receber self.relu, a ativação da saída do hidden. Vai rodar de dentro para fora, primeiro o self.hidden, isso vai dar uma saída e a saída do self.hidden vai passar na ativação. E a camada de saída, a variável de saída vai ser self.output a partir do resultado no hidden.\n",
        "\n",
        "[05:50] E eu retorno esse output. Pronto! Acabei a minha classe. Na hora de instanciar, se você colocar net = MinhaRede e passa por parâmetro qual é o input_size, output_size, hidden_size, ele vai chamar implicitamente o método init. Aqui ele chama implicitamente o método init, você precisa passar quem quer passar do hidden a saída.\n",
        "\n",
        "[06:20] Nós já sabemos do vídeo anterior que dado esse problema que estamos tratando com esses dados, podemos usar esses hiper parâmetros, input_size, output_size, hidden_size, que é um dado com 2 dimensões, nós queremos uma rede com 8 neurônios, isso é uma escolha que estamos fazendo e sabemos que é um problema de classificação binário que eu consigo resolver com um único neurônio.\n",
        "\n",
        "[06:48] Instanciei a minha rede, eu posso imprimi-la, ela é exatamente a mesma rede do vídeo anterior e vamos agora fazer o forward, que também vai ser idêntico ao forward do vídeo anterior. É completamente transparente na hora de você chamar a sua rede, não importa se você definiu a sua rede como sequential ou como uma classe, na hora de fazer o forward é idêntico.\n",
        "\n",
        "[07:19] Fizemos para 300 amostras de 2 dimensões, 300 predições de classes binárias em uma dimensão só. A última coisa que falta olharmos é como usamos essas coisas na GPU. Para isso, a primeiro coisa é editarmos as configurações do notebook para GPU, lembrando, ele vai reconectar, ou seja, você vai perder toda a sua sessão, você precisa rodar tudo de novo. Vamos rodar de novo os dados.\n",
        "\n",
        "[07:55] Vamos rodar de novo a definição da nossa classe, só que agora vai mudar como instanciamos a rede e vai mudar também como fazemos o forward, porque agora estamos fazendo na GPU.\n",
        "\n",
        "[08:11] E agora vamos fazer aquela verificação que já conhecemos que é se a GPU está disponível. if torch.cuda.is_available(); se a GPU está disponível, portanto, o device que eu vou usar é um objeto device = torch.device(cuda). Se não é um objeto torch.device(‘cpu’). Imprimimos quem é o device e ele vai dizer que estamos usando agora.\n",
        "\n",
        "[08:48] Pronto, agora estamos usando o cuda. Quando formos instanciar a rede, a única coisa que muda é que agora precisamos subi-la na GPU, isso é feito na função to. net.to(device) e eu passo como parâmetro o dispositivo que eu quero. Aí eu dou enter.\n",
        "\n",
        "[09:10] Agora minha rede está na GPU, o mesmo serve para o tensor. No caso do tensor, quando eu subo ele na GPU, tensor = tensor.to(device) se eu imprimo esse tensor, ele vai indicar na impressão que ele está na GPU. Se eu imprimir esse tensor, no final ele vai dizer que é um tensor que está no dispositivo GPU.\n",
        "\n",
        "[09:36] Agora eu consigo fazer o forward na rede e ele vai se comportar normalmente, porque tanto a rede quanto a entrada estão na GPU. Se eu não colocar os dados na GPU, vai dar erro e ele vai me dizer que esperava por um objeto do tipo cuda. Porque a rede está na GPU e o seu dado não está. Colocando o dado na GPU fica tudo certo.\n",
        "\n",
        "[10:03] Aprendemos como vamos criar uma rede de múltiplas camadas como uma classe e como usamos a GPU para o nosso processamento."
      ],
      "metadata": {
        "id": "Hb_KWyVkiXSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No notebook Arquitetura.ipynb implementamos uma arquitetura neural utilizando o módulo nn.Sequentual, e também a classe nn.Module, para um problema de classificação com duas classes, onde cada amostra possuía dois atributos. Considere agora o seguinte conjunto de dados para classificação de flores a partir de atributos estruturais:\n",
        "\n",
        "Distribuição de 3 dos 4 atributos do dataset Iris que possui três classes de flores. \n",
        "\n",
        "Implemente, usando tanto a implementação do nn.Sequential quanto o nn.Module, uma arquitetura que suporte o treinamento para esse problema de classificação. Sugiro uma arquitetura com uma camada linear escondida com seis neurônios, uma ativação ReLU e uma camada linear de saída.\n",
        "Problemas multi-classe.\n",
        "Multi-Layer Perceptron (MLP).\n",
        "Implementando uma arquitetura com nn.Sequential.\n",
        "Implementando uma arquitetura com nn.Module."
      ],
      "metadata": {
        "id": "JCZva9Jziatm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbQ1QDk_VeFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wjq8RuwBVeBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7xxApDtVd9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDvm6koYVd6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DyhpTsvFRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feSm8_jQFRAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIOcGeFSFQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wXZN5AQPFQ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDqd9V3JFQ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PVYvce8EFQbY"
      }
    }
  ]
}