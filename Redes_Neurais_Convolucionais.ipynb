{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp8szZLlwpIB4MQ0U1SH7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juniorexz/Codigo/blob/master/Redes_Neurais_Convolucionais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWNh-LMfuiHH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "] Olá e muito prazer eu me chamo Camila Laranjeira, esse aqui é o curso de Redes Neurais Convolucionais, antes de falarmos sobre o curso é importante mencionar que esse curso ele depende fortemente de conhecimentos que estão em outros dois cursos que é o de introdução a redes neurais e treinando uma rede neural.\n",
        "\n",
        "[00:22] Nesse curso principalmente o de introdução a redes neurais, vamos aprender os elementos essenciais de uma rede que é uma conexão o que que são camadas que são conhecimentos que vão ser assumidos como você já está familiarizado com eles, então é importante que você pelo menos tenha conhecimento base de redes neurais, então recomendo fortemente assistir os dois cursos que são uma introdução a esse.\n",
        "\n",
        "[00:51] Nesse curso de Redes Convolucionais vamos cobrir todo conhecer específico de uma Rede Convolucional. Como por exemplo, o que que é uma imagem como que olhamos para uma imagem de forma matemática, a representação base dela.\n",
        "\n",
        "[01:10] Vamos conhecer as diferentes representações de Redes Convolucionais para que vocês quando vejam uma rede representada em algum livro, em algum blog, você já entenda o que é cada elemento ali representado.\n",
        "\n",
        "[01:23] Vamos explicar o que é o “aprendizado Hierárquico” e por que no caso de Redes Convolucionais, é especialmente interessante você ter uma rede mais profunda, os benefícios disso, daí por diante nós vamos conhecer as principais aplicações da Rede Convolucional, explorando os datasets que o Torchvision provê, então vocês vão conhecer a ferramenta essencial que é o Torchvision que traz vários modelos prontos, vários conjuntos de dados prontos e ao melhor amigo de quem usa PyTorch para imagens .\n",
        "\n",
        "[01:58] Depois vamos conhecer a convolução desde a convulsão 1D, fazendo classificação de sinais 1D com a convolução, vai ser bem divertida essa prática. Vamos passar também pela convolução 2D que é efetivamente o que nós usamos para imagens, qual que é o comportamento de uma convulsão 2D.\n",
        "\n",
        "[02:20] Qual é a saída de cada filtro convolucional, o que é um filtro convolucional, como que ele interage com a imagem, vamos conhecer também a camada efetivamente do “PyTorch”, cada um dos parâmetros desta camada vocês vão se familiarizar com a importância desse parâmetro.\n",
        "\n",
        "[02:40] Vamos conhecer outras camadas como a camada de pulling e a camada de batch normalization, aqui no pulling vamos entender a segunda operação mais importante da convolução, vamos implementar tanto pulling quanto o batch normalization são essenciais para o treinamento de um bom modelo.\n",
        "\n",
        "[03:02] Implementaremos aqui esse bloco convolucional completo com todas as camadas, as mais importantes, todas que nós vamos aprender nesse curso, aí vamos chegar na parte que eu acho a mais interessante para quem decidiu fazer esse curso que é treinar uma CNN do zero.\n",
        "\n",
        "[03:20] Então aqui, faremos tudo do zero, desde a implementação na de uma arquitetura convolucional até o treinamento completo de uma rede. Esse conhecimento que está aqui do treinamento de uma rede, ele é todo esse curso aqui de Treinando uma rede neural Deep Learning com PyTorch, mas vamos tomar o cuidado de revisar esse conteúdo na hora de fazer o treinamento da rede convolucional.\n",
        "\n",
        "[03:51] E por fim para ser a cereja do bolo, nós vamos conhecer outras estratégias de treino usando uma rede convolucional como extrator de características e fazendo Fine-Tuning que é um dos usos mais comuns de um CNN.\n",
        "\n",
        "[04:08 ] Então nós iremos aprender que no mundo de Redes Convolucionais no processamento de imagens é comum e recomendável que você aproveite conhecimento treinado em outros datasets, então você não precisa treinar o seu modelo do zero, não precisa se basear só no seu conjunto de dados na hora de treinar numa Rede Convolucional.\n",
        "\n",
        "[04:32] Você também pode aproveitar redes pré-prontas modelos pré treinados para que seu ponto de partida seja de uma rede já inteligente, uma rede já é treinada. Uma última recomendação é: faça os exercícios e usem o fórum, porque é interessante que vocês tenham certeza de que aprenderam e os exercícios foram pensados para você validar o seu aprendizado.\n",
        "\n",
        "[05:00] Eu tomei muito cuidado na criação dos exercícios, para que ele aborde parte do conteúdo que costumam ser os mais difíceis de aprender, então eu recomendo que vocês façam os exercícios .\n",
        "\n",
        "[05:12 ] Vai ser um prazer estar aqui com vocês ao longo desse curso e eu espero que vocês aproveitem muito, bom aprendizado para você."
      ],
      "metadata": {
        "id": "7aj8OiWx9ceZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdE4G_fq9pQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Olá pessoal, vamos começar então a falar um pouco sobre Redes Neurais Convolucionais, começando a partir dos conceitos gerais que basicamente nós falaremos desde a motivação de porque se usa esse tipo de rede, até os elementos que compõem a arquitetura os principais, e terminando essa aula com os principais problemas que nós resolvemos com a convolução com a Rede Convolucional.\n",
        "\n",
        "[00:28 ] Começando com motivação, vamos primeiro entender o que é uma imagem propriamente falando em termos computacionais, que nada mais é que uma matriz de pixels, são números organizados numa matriz.\n",
        "\n",
        "Imagem com título \"Motivação\" e subtítulo \"Imagem -> Matriz de pixels (números), contendo uma fotografia em preto e branco de uma mulher usando chapéu e olhando por cima do ombro, seguido de uma seta que aponta para a mesma imagem com uma malha quadriculada sobreposta com valores de duzentos a zero nas linhas do eixo vertical e zero a duzentos no horizontal.\n",
        "\n",
        "[00:41] Nesse caso, uma matriz quadrada de dimensões 256x256. que em outras palavras significa que são 246 números por 256 números, é uma matriz quadrada de números que nós vamos ver como isso se transforma numa imagem.\n",
        "\n",
        "[01:00] Então se nós pegarmos um trecho dessa imagem, o olho da Lena nós conseguimos ver nesse pedaço pequeno, os quadrados certos que são os pixels, que definem a imagem.\n",
        "\n",
        "[01:14] E em termos simples esses pixels, esses quadrados são a intensidade da cor daquele elemento, daquele quadrados. Então se eu tenho uma matriz, nesse caso 15 por 15, que a região do olho em um determinado elemento tem valor de 0.1, está perto de 0 então representa algo mais próximo do preto, 0.9 está mais perto do 1 representa algo próximo do branco e 0.5, 0.6 está na faixa dos cinza.\n",
        "\n",
        "[01:48] Vamos relembrar então o “Multilayer Perceptron” que nós já falamos aqui na Alura, no curso de introdução a Redes Neurais com PyTorch. Recomendo muito assistir, vai ser essencial para esse curso, e o Multilayer Perceptrons ele é a rede mais tradicional, a rede do Deep learn mais tradicional, e ele basicamente é composto por camadas de neurônios.\n",
        "\n",
        "[02:14] Camadas de uma dimensão vetores, de neurônios totalmente conectados, quando nós vamos definir o MLP a quantidade de conexões relativa ao número de neurônios da camada anterior e o número de neurônios da camada atual.\n",
        "\n",
        "[02:32] E o número de conexões, cada conexão lembrando é um peso vai ser dado pelo produto desses dois valores ´Pesos = Ni-1*Ni´que eu preciso conectar todo mundo com todo mundo, se eu tenho três neurônios de um lado, quatro neurônios do outro, eu vou ter 12 conexões ligando essas camadas.\n",
        "\n",
        "Imagem com título \"MultiLayer Perceptron - MLP\" com o texto \"Relembrando: ao definir uma rede, a quantidade de conexões é relativa a:\" seguido dos itens \"Número de neurônios na camada anterior\" e \"Número de neurônios na camada atual\". Abaixo, há a fórmula \"Pesos\" igual a \"N\" no índice \"i\" menos um vezes \"N\" no índice \"i\". Ao lado, há três círculos numerados de um a três em sequência vertical representando os neurônios da camada \"i-1\", todos ligados por linhas a outros quatro círculos ao lado numerados de um a quatro representando os neurônios da camada \"i\".\n",
        "\n",
        "[02:49] No caso da nossa imagem da Lena que é 256 por 256, se nós fossemos tentar usar um MLP, nós temos na mão aqui essa imagem que é uma matriz quadrada, só que o MLP só aceita entradas 1D. Ele trabalha dessa forma, a arquitetura é feita dessa forma.\n",
        "\n",
        "[03:07] Então, nós teríamos que linearizar essa imagem ficando com n ao quadrado valores, tudo bem, nós linearizamos, nós conseguimos usar no MLP, só que aí vem uma outra questão, é quantos neurônios a próxima camada vai ter?\n",
        "\n",
        "[03:24] Cada problema vai ter a sua complexidade específica, mas em geral para você conseguir uma boa representação, a camada subsequente tem que ser diretamente proporcional ao tamanho da camada anterior, porque não faz muito sentido, eu ter n ao quadrado pixels que é 256 por 256 e a próxima camada eu ter 10 neurônios.\n",
        "\n",
        "[03:47] Vou perder muita informação, eu não vou conseguir aprender boas características dessa minha entrada.\n",
        "\n",
        "[03:55] Então eu teria que, por exemplo, ter uma camada subsequente igual 1 n ao quadrado Pixel, lógico que no mundo real não precisa ser igual, mas tem vezes que até maior do que o tamanho da entrada a camada.\n",
        "\n",
        "[04:08] Então se nós supormos uma situação intermediária, onde a camada subsequente é do tamanho da entrada, nós teríamos duas camadas de N ao quadrado pixels em uma camada e n ao quadrado neurônios na outra camada.\n",
        "\n",
        "[04:24] Para conectar essas coisas, teríamos um total de n à quarta conexões, cada conexão é um peso, então n a quarta peso, lembrando a nossa imagem tem 256 por 256, então que nós teria 236 a quarta pesos que são 4 bilhões de pesos que ocupa 17GB de memória.\n",
        "\n",
        "Imagem com título \"MLP em Imagens\" com o texto \"Quantos neurônios a próxima camada deve ter?\" Abaixo, há cinco círculos numerados de um vírgula um a \"n\" vírgula \"n\", sendo o do meio com reticências dentro, em sequência vertical representando o número de pixels, todos ligados por linhas coloridas a outros cinco círculos iguais ao lado representando o número de pixels. Ao lado deste esquema, há os itens \"Total de pesos: N elevado a quatro\", \"256 elevado a quatro\", \"4 bilhões de pesos\" e por fim \"17GB de memória\".\n",
        "\n",
        "[04:45] Só para você armazenar essa rede, nós não estamos falando de treinar, não estamos falando do gasto que você vai ter carregando coisas além da rede, só a rede são 17 GB de memória. Por isso, não usamos LMP para imagens, tem essa restrição, você teria que usar muitos parâmetros para conseguir uma boa representação.\n",
        "\n",
        "[05:06] Então daqui para frente vamos conhecer as Redes Neurais Convolucionais, que se baseia na operação da convolução, por isso que elas chamam convolucionais, em inglês Convolutional Neural Networks, então quando eu falar CNN eu tô falando das redes Neurais Convolucionais.\n",
        "\n",
        "[05:23] O número de parâmetros não vai ser mais vinculado ao tamanho da entrada, nós vamos conhecer o Kernel da convolução e o tamanho desse Kernel não depende do tamanho da entrada e resolve o problema de muitos parâmetros.\n",
        "\n",
        "[05:38] Por fim, uma coisa que se tornou essencial, inclusive para muitas aplicações, é que a Rede Convolucional interpreta a imagem de forma intuitiva.\n",
        "\n",
        "[06:10] Então nós vamos conhecer um pouco melhor, não tanto quanto eu gostaria, mas as características são apreendidas por essas redes, são algo que conseguimos interpretar visualmente, inclusive as Redes Convolucionais são as mais usadas quando se tenta produzir algum tipo de interpretabilidade da metodologia baseada em Deep learning, elas facilitaram nessa visualização."
      ],
      "metadata": {
        "id": "x5Bv2-yD9qc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora nós vamos falar um pouco sobre Arquitetura da CNN, bom, relembrando mais uma vez o MLP,eu acho importante nós fazermos essas comparações para entender o papel da CNN e quando usá-la.\n",
        "\n",
        "[00:16] Ele recebe um vetor, nós já comentamos isso, um vetor de uma dimensão e as camadas são totalmente conectadas, ou seja, cada neurônio vai se conectar à todos os elementos da entrada, que foi o que comentamos agora pouco do problema de você ter uma dimensionalidade muito grande nas imagens e isso tem inviável para o uso no MLP.\n",
        "\n",
        "[00:40] As imagens, nós ainda nem comentamos de imagens coloridas. Elas são volumes. Então no caso das imagens das RGB que é vermelho, verde, azul. Elas são volumes de três canais: um canal azul, um canal verde, um canal vermelho e possuem altíssima dimensionalidade, porque só essa imagem da Lena 256 x 256, se nós considerarmos os três canais ainda, totalizam mais de 190.000 pixels que se nós fossemos colocar numa rede totalmente conectada, totalmente inviável.\n",
        "\n",
        "[01:17] A CNN e nós já vamos começar a nos familiarizar com a representação, ela é apta a trabalhar com volumes 3D. Então nós vamos ver da CNN, sempre uma representação com profundidade também.\n",
        "\n",
        "[01:33] Então não só ela consegue trabalhar com matrizes, que são as matrizes quadradas ou não, como também ela lida com a questão da profundidade, nós vamos ver em breve como ela faz isso.\n",
        "\n",
        "Imagem com título \"representação CNN\" com o texto \"Consegue trabalhar com volumes 3D (altura, largura e profundidade)\" seguido do item \"Profundidade: canais de cor, mapa de características\". Abaixo, há um retângulo em perspectiva e em posição vertical, o qual projeta uma pequena área para um cubo em seguida, que por sua vez projeta uma pequena área para outro retângulo menor em perspectiva e posição horizontal. Abaixo das figuras, há as legendas de \"convolução\" e \"pooling\".\n",
        "\n",
        "[01:46] Essa profundidade quando nós estamos falando da entrada que aqui esse primeiro cubo seria a imagem, a partir da segunda camada, esses cubos são mapas de características; Lembrando que Redes Neurais Profundas, elas vão transformar a imagem em características isso é verdade para qualquer rede.\n",
        "\n",
        "[02:08] Então se nós transformarmos uma imagem a partir de uma camada de Rede Convolucional o que ela passa a ser é um mapa de características, também em três dimensões, só que aqui não estamos mais falando de canal de cor, estamos falando de mapa de características.\n",
        "\n",
        "[02:25] E os neurônios também são organizados em 3D e possuem um campo de visão limitado, então aqui nós estamos vendo as duas principais operações de uma CNN, que a convolução e o pooling\n",
        "\n",
        "[02:40] Então a convolução vai transformar a imagem em características e o Pooling é um processo “sub-amostragem” que nós vamos conhecer melhor, então a CNN vai justamente ficar alternando entre “transformação” e “subamostragem’, até o final.\n",
        "\n",
        "[02:59] Uma outra representação que podemos usar, também é uma representação em três dimensões, então só que dessa forma em vez de nós desenharmos cubos, desenhamos como se fossem as lâminas da representação, então é como se em vez de fazer esse cubo, eu separasse cada mapa de características, então aqui eu transformaria uma entrada que nesse caso eu fiz ela só com um canal em seis mapas de características através da convolução.\n",
        "\n",
        "Imagem com título \"representação CNN\". Abaixo, há um quadrado chamado \"Entrada\" com uma pequena área quadrada projetada para outro quadrado chamado \"conv1\" sobreposto a uma sequência de outros, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos chamados \"Pool 1\". Em seguida, há um sinal de igualdade seguido de um retângulo em perspectiva e em posição vertical, o qual projeta uma pequena área para um cubo em seguida, que por sua vez projeta uma pequena área para outro retângulo menor em perspectiva e posição horizontal. Abaixo destas últimas figuras, há as legendas de \"convolução\" e \"pooling\".\n",
        "\n",
        "[03:31] E quando nós compomos uma rede completa do começo até o final a tendência, que nós vamos entender melhor porque, é que quanto mais profunda a rede mais mapas de características a representação vai ter.\n",
        "\n",
        "[03:48] Então aqui nós passamos de seis mapas de características, para eu suponho que é 12, para o que eu suponho que é 18. Mas eu não lembro quando eu fiz esse desenho.\n",
        "\n",
        "[03:58] Então nós estamos alternando entre “transformação” e “subamostragem’, usando um filtro que tem um campo de visão limitado que nós vamos ver direito mais para frente como funciona.\n",
        "\n",
        "[04:11] Então alterna entre “transformação” e “subamostragem” e as camadas mais comuns são a Convolucional, que nós já falamos diversas vezes de transformação.\n",
        "\n",
        "[04:19] O pulling que a “subamostragem” e camadas totalmente conectadas que são essas aqui, azuis do final, que nada mais são do que as camadas que nós vamos usar para fazer uma inferência, por exemplo uma classificação, uma regressão, nós usamos camadas tradicionais totalmente conectadas, só que vão operar em cima de um conjunto de características em Alta Dimensão, nós vamos ver mais para frente o que que significa.\n",
        "\n",
        "[04:51] Vão operar com características melhores do que a imagem original. E essa forma de você compor a Rede Convolucional, produz um aprendizado hierárquico, por quê? Porque existe uma hierarquia de processamento, primeiro você passa na primeira camada, depois você passa na segunda, depois você passa na terceira,\n",
        "\n",
        "[05:12] Então aprendizado hierárquico, aprende uma característica em cima dessa característica você aprende outra e daí por diante. Então as primeiras camadas, como elas acessam diretamente a imagem, mais diretamente, elas vão aprender características de baixo nível, enquanto as últimas camadas vão aprender características de alto nível.\n",
        "\n",
        "Imagem com título \"Aprendizado Hierárquico\". Abaixo, há um quadrado chamado \"Entrada\" com uma pequena área quadrada projetada para outro quadrado chamado \"conv1\" sobreposto a uma sequência de outros, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos chamados \"Pool 1\". Em seguida, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\". Abaixo, há o texto \"Múltiplas camadas produzem um aprendizado hierárquico\" com uma lista de dois itens, \"Primeiras camadas: características de baixo nível\" e \"últimas camadas: características de alto nível\".\n",
        "\n",
        "[05:34] Então basicamente teríamos que pensar que essa entrada aqui é um objeto, acho que esse bicho aqui é um ganso, então vamos supor que é um. Aqui nós aprenderíamos nas primeiras camadas retas, quinas, blobs de cor, na camada intermediária nós aprenderíamos coisas de mais alta frequência, algo que talvez pudesse parecer um olho aqui embaixo.\n",
        "\n",
        "[05:59] E nas últimas camadas nós aprenderíamos formas mesmo, tipo a cabeça de um cisne ou ganso talvez, uma colmeia de abelhas, enfim aprenderíamos formas mais próximas dos objetos que nós conhecemos do mundo real.\n",
        "\n",
        "[06:17] Então essa hierarquia de características que é desde o baixo nível, até o médio e até o alto. Mais cedo na rede nós aprendemos só retas por exemplo, no nível intermediário nós aprendemos os olhos de um animal por exemplo, e no alto nível nós aprendemos a cabeça inteira do animal, digamos assim.\n",
        "\n",
        "[06:36] Essas imagens que nós estamos vendo aqui, são filtros reais, são camadas de pesos mesmo real da rede, vamos ver mais para frente e maiores detalhes mas é como se essas camadas estivessem buscando esse tipo de padrão. Então as camadas iniciais procurariam por retas, e as finais procurariam pela cabeça do cisne efetivamente.\n",
        "\n",
        "Mesma imagem anterior. Os quadrados de \"Conv1 e \"Pool 1\" possuem uma chave que indica para uma imagem de pouca definição, chamada \"Baixo nível\". Os quadrados \"Conv2\" e \"Pool2\" possuem uma chave que indicam para uma imagem com uma definição melhor chamada \"Médio Nível\". Por fim, os quadrados \"Conv3\" e \"Pool3\" possuel uma chave indicando para uma imagem de boa definição, chamada \"Alto nível\".\n",
        "\n",
        "[07:05] Então é isso, esse aprendizado que você tem que tirar dessa aula de que a CNN vai transformar hierarquicamente a imagem em características de cada vez mais alto nível e só no final vão fazer a inferência, que é a classificação a regressão em cima de características de alto nível."
      ],
      "metadata": {
        "id": "F08f4lxM9qYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Nessa aula nós vamos conhecer um pouco as principais aplicações de Redes Neurais Convolucionais, no caso os principais problemas com imagens que essas redes conseguem resolver.\n",
        "\n",
        "[00:14] Primeiro de todos é a classificação, que é o problema mais comum, sempre que nós falamos de Redes Neurais Deep, nós vamos ver também detecção de objetos e Segmentação Semântica que nós vamos saber já o que é.\n",
        "\n",
        "[00:29] Então para nós conhecer esses problemas, uma proposta que eu acho que pode ser uma forma interessante, é nós vermos Datasets referentes a esses problemas, então veremos um Dataset de classificação, um de detecção e outro de segmentação.\n",
        "\n",
        "[00:44] Primeiro de tudo vamos fazer os importes aqui o pacote torch nós sempre importamos que o pacote principal do “PyTorch” que faz todas as operações básicas Contensores e no caso, das operações com imagens nós vamos estar sempre importando também algumas coisas do “TorchVision” que é o pacote torch que tem modelos, datasets monte de coisa relacionada à imagens.\n",
        "\n",
        "[01:12] Então, hoje nós vamos precisar dos datasets e nós também vamos precisar das transformações, vamos ver mais lá na frente porquê. Então para representar problema de classificação iremos ver o dataset mnist, é um dataset de dígitos escritos na mão, como estão vendo aqui que possui 10 classes são dígitos de 0 a 9.\n",
        "\n",
        "Imagem contendo quadrados com desenhos dos números feitos aparentemente à mão de zero a nove e numerados de zero a nove.\n",
        "\n",
        "[01:35] Então o dado nesse caso é a imagem, vamos receber como entrada a imagem e o rótulo o objetivo do treinamento, seria um inteiro, qual é a classe que aquele dígito pertence, então o rótulo aqui seria 0, rótulo aqui seria 1, dado uma imagem qualquer um inteiro correspondente.\n",
        "\n",
        "[01:57] Para nós aprendermos como é que carrega, é super simples, pegamos da documentação, vai estar sempre dizendo como é que carrega cada “Dataset” vai ter um conjunto de parâmetros diferente, pode ter um conjunto de parâmetros diferentes, nós só precisamos associar essa chamada datset.mnist a uma variável e preencher os parâmetros.\n",
        "\n",
        "[02:21] Então os parâmetros que vamos precisar são: onde é que eu vou salvar esse modelo, pode salvar aqui na pasta que nós estamos, no colab. Estamos salvando nos computadores da Google, eu quero um conjunto de teste, então eu digo aqui que eu não quero conjunto de treino, e vem a parte das transformações que vamos fazer no dado e no rótulo.\n",
        "\n",
        "[02:47] O rótulo vou deixar como inteiro mesmo, mas o dado eu vou aproveitar para mostrar para vocês a transformação mais básica que fazemos quando carregamos os dados que a transformação para “tensor” como para passar numa rede, para passar em uma camada precisamos transformar para “tensor” você pode fazer essa transformação já no carregamento do dado.\n",
        "\n",
        "[03:11] Eu não fiz nenhuma transformação no rótulo e, eu quero que ele baixe os dados, eu não tenho ele salvo aqui, então ele vai baixar esses dados para mim baixou, agora eu tenho acesso a dados aqui que, por exemplo, eu posso indexar. Eu quero o dado 0 do meu dataset.mnist.\n",
        "\n",
        "[03:31] A tela vai se encher agora, está vendo? Ele colocou aqui o array representando a imagem, e lá no final ele colocou o rótulo que é 7. Essa imagem representa um 7, então eu posso carregar isso de forma mais organizada, colocando numa tupla dado dado, rótulo = MNIST, porque essa é a forma que ele vai me fornecer essas informações, como eu tenho duas informações ele retorna as duas numa tupla.\n",
        "\n",
        "[04:41] Eu poderia fazer de várias formas, dado recebe mnist e aí depois eu quebraria em dado e rótulo. Enfim, eu posso fazer qualquer forma, eu prefiro já separar a tupla aqui na atribuição, e agora eu posso imprimir qual é o tipo do meu dado, qual é o tipo do meu rótulo print (type (dado), type (rotulo)).\n",
        "\n",
        "[04:23] E primeiro imprimir dado, rotulo = MNIST= [0], eu tenho um “tensor” e um inteiro, se eu imprimir agora o size desse meu dado, e eu vou imprimir também o rótulo de uma vez, ele vai mostrar que é 28x28 por 1, que é esse lembrete que eu anotei aqui para não esquecer de falar que o padrão do “PyTorch” é o canal primeiro.\n",
        "\n",
        "[04:50] Então no caso da imagem, a dimensão do canal é referente a quantas dimensões de cor a imagem tem, no caso do “Mnist” ele é preto e branco, então ele só tem um tamanho de profundidade. Ele só tem um canal de cor, então aqui na hora que nós formos imprimir isso aqui, eu vou usar o “Matplotlib” para imprimir algumas amostras.\n",
        "\n",
        "[05:20] Eu vou fazer um subplot para eu poder imprimir várias imagens na mesma figura, uma imagem de tamanho 15x4 por exemplo. Então vou imprimir os 10 primeiros elementos do dataset Mnist, dado, rotulo = MNIST[i].\n",
        "\n",
        "[05:40] E agora eu posso imprimir no eixo da minha figura: axs.[i].imshow.(dado [0], cmap = ‘gray’), colormap preto e branco, e eu também quero colocar o rótulo no título, então eu vou setar o título e vou colocar a stringer do rótulo que eu chamei de rótulo mesmo axs.[i].set_title(str(rotulo).\n",
        "\n",
        "[06:21] O que eu esqueci? O ‘s’ em fig, axs = plt. [ (1, 10, figsize = (15, 4)). Então eu imprimi os 10 primeiros elementos do “dataset” e o rótulo no título. Então como é que deve ser a última camada de uma rede cujo objetivo é classificar os dados de Mnist? Essa última camada vamos lembrar dos MLPS ela tem que ter o número de neurônios correspondente ao número de classes.\n",
        "\n",
        "[06:42] Então vamos supor que eu tenho aqui a minha imagem de entrada, eu vou ter aqui a minha Rede Convolucional, eu vou representar como um grande retângulo, vou ter a minha Rede Convolucional com arquitetura que eu posso definir a vontade, só que o que tem que vir no final com certeza, é uma camada de MLP totalmente conectada com 10 neurônios então vou ter que um total de 10 neurônios um para cada categoria do meu dataset.\n",
        "\n",
        "[07:14] Vou ter que o neurônio que vai ativar se o dígito for 0, se o dígito for 1, se o dígito for 2 e daí por diante.\n",
        "\n",
        "[07:20] Então, assim se resolveria um problema de classificação. No próximo vídeo vamos ver a detecção e a segmentação.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Qw2G8jZ9qVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos agora para o problema de detecção, um lembrete muito válido é que esse curso é fortemente relacionado com curso de Introdução a Redes Neurais com “PyTortch” e treinando redes neurais com “PyTortch”.\n",
        "\n",
        "[00:15] No primeiro, nós explicamos redes totalmente conectadas, no segundo explicamos como você carrega dados, como você treina os modelos, então vale muito a pena voltar nesses cursos.\n",
        "\n",
        "[00:27] Mas para entendermos do problema da detecção, eu vou mostrar aqui os dados e os rótulos referentes a esse problema. Vamos usar esse script aqui para ilustrar os problemas e para que nós entendamos os tipos de aplicação que podemos usar SNLs.\n",
        "\n",
        "[00:43] Então vamos conhecer agora o “Pascal VOC” que é um dataset que fornece rótulos de detecção, então aqui no “Pascal VOC” ele nos mostra que ele fornece rótulos de detecção para 20 classes diferentes para classes como: bicicletas, ônibus, pessoa uma vaca aqui, cadeira.\n",
        "\n",
        "[01:14] Então basicamente tem animais, objetos e meios de transporte que consideramos tudo como se fosse objeto, o que nós estamos interessados, no caso da detecção, vamos ver essa imagem que são os rótulos, que são essas caixas delimitadoras, que dizem onde é que está o objeto, então tem aqui um retângulo que diz que a pessoa está delimitada dentro dessa região da imagem.\n",
        "\n",
        "Imagem contendo uma mulher de casaco e mãos juntas em visão frontal em um gramado. Atrás dela, há uma ovelha marrom olhando diretamente para a câmera. Ambas as figuras estão selecionadas por uma área retangular e outra quadrada de linhas amarelas que as separam do fundo.\n",
        "\n",
        "[01:40] Nós vamos entender como são esses rótulos agora na hora de carregar, vamos de novo aqui na documentação, copia da documentação, de novo: cada dataset vai ter um conjunto de parâmetros diferentes e nós vamos carregar aqui agora os dados do VOC.\n",
        "\n",
        "[02:00] Igual fizemos com o “Mnist” só que agora tem outro conjunto de parâmetros para preencher, lembrando que esses valores que já vem aqui definidos, são os valores padrão que a biblioteca considera, então eu posso simplesmente omitir alguns valores se eu não quero modificar eles.\n",
        "\n",
        "[02:19] Eu carrego aqui o treino, eu preciso que baixe o dataset e salve aqui no nosso computador que nós estamos pegando emprestado da Google, eu quero fazer a minha transformação na imagem de novo transformando ela para tensor para vermos como é eu não quero mais fazer nenhuma outra transformação, só a transformação da imagem mesmo.\n",
        "\n",
        "[02:40] Vamos carregar o “VOC”, beleza, que ele está baixando “Pascal VOC” e vai salvar aqui na minha variável, uma vez que dataset foi carregado, agora nós vamos fazer a mesma coisa que é ver o tipo e a dimensionalidade dos dados só para entendermos o que é o dado e o que é o rótulo.\n",
        "\n",
        "[03:00 ] De novo o dado aqui vai ser um tensor porque transformamos ele em tensor, mas o rótulo por enquanto é uma surpresa, então primeiro precisamos carregar o dado e o rótulo aqui do VOC de um elemento qualquer do VOC e ele vai dizer que é um tensor indicionário.\n",
        "\n",
        "[03:16] Então vamos lá, primeiro que tensor que é esse? Esse Tensor nada mais é do que a nossa imagem, então vamos ver o tamanho desse Tensor ele é 3 x 442 x 500, significa que a imagem é 442 x 500 e o número de canais de cor é 3.\n",
        "\n",
        "[03:39] Então na hora de plotar isso, aqui tem uma ressalva que eu estou fazendo, por padrão o “PyTortch” adota o Channel first que o canal primeiro é que fica C x H x W, já as bibliotecas de visualização não, elas adotam o channel last que é o canal por último.\n",
        "\n",
        "[03:58] Então nós precisamos fazer aqui uma operação que chamamos de “permuta”, então vamos fazer aqui uma “permuta” nesse dado nas dimensões. E o que eu quero agora? Quero jogar a primeira dimensão para o final, vou colocar a dimensão 0 para o final e as dimensões 1 e 2 para o início. Então aqui agora eu consigo plotar essa imagem colorida sem precisar ignorar o canal de cor da última vez.\n",
        "\n",
        "[04:27] Se eu plotar aqui a imagem, deixa eu colocar uma figura um pouco maior, mas eu vou colocar 8 x 6 por exemplo, plt.figure(figsize(8, 6)).\n",
        "\n",
        "Imagem de um equitador montado em um cavalo preto pulando um bostáculo em um campo de areia. Ao lado no eixo vertical da imagem, há a numeração de trezentos a zero de cima para baixo.\n",
        "\n",
        "[04:46] Beleza, agora tenho aqui a imagem de uma pessoa num cavalo que tem 442 X 500, como vimos agora a pouco, esse é o nosso dado, já o rótulo é um dicionário de várias coisas, por quê? Porque cada imagem vai ter mais de um objeto, vai ter diferentes informações que são relevantes para o dataset.\n",
        "\n",
        "[05:10] Mas nós estamos interessados nessa informação aqui, que é o bndbox? É a caixa delimitadora que nós falamos agora pouco. E o que é esse bndbox que é essencial para entendermos o que é o problema de detecção?\n",
        "\n",
        "[05:24] São os valores Xmax Xmin ymax Ymin, que definem essa caixinha delimitadora. Então nós temos quatro coordenadas que vão definir um retângulo, então onde é que está cada uma dessas coordenadas?\n",
        "\n",
        "[05:45] Aqui está o xmax eymaxe aqui está oxminymin`, com essas duas informações eu consigo definir um retângulo, porque o retângulo ele obedece a alguns critérios, que é de paralelo e de mesmo tamanho aqui, paralelo de mesmo tamanho aqui.\n",
        "\n",
        "[06:07] Então só preciso dessas duas coordenadas e quatro valores para definir esse meu retângulo. Cada quatro valores aqui representam as coordenadas de um único objeto.\n",
        "\n",
        "[06:24] Para nós não perdermos tempo com como que carrega esses dados do “Pascal VOC”, já deixei implementado aqui o plot do retângulo.\n",
        "\n",
        "[06:36] Então se nós rodarmos esse código ele vai me mostrar aqui a imagem da pessoa no cavalo, e lembrando que pegamos aqui o objeto 0, dentro e todos os objetos são dois no total, que tem aqui no bndbox do objeto 1 e o bndbox objeto 2. Objeto 1 é o cavalo então o bndbox correspondente vai definir o retângulo onde está o cavalo.\n",
        "\n",
        "Imagem de um equitador montado em um cavalo preto pulando um bostáculo em um campo de areia. Ao lado no eixo vertical da imagem, há a numeração de trezentos a zero de cima para baixo, e no eixo horizontal há a numeração de zero a quatrocentos. Ainda, somente a figura do cavalo está selecionada por um retângulo de linhas vermelhas.\n",
        "\n",
        "[7:00] Como deve ser a última camada de uma rede, cujo objetivo é detectar objetos do “Pascal voc”? Dessa vez, ao invéz de classificação nós vamos estar resolvendo um problema de detecção, então vai ser um problema de regressão, porque eu preciso descobrir qual é o valor do Xmin eu tenho aqui quatro coisas para descobrir xmin ,xmax e ymín, Ymax.\n",
        "\n",
        "[07:32] Então vou ter 4 neurônios no final, um referente a cada variável que eu quero fazer regressão e cada neurônio a intensidade da ativação dele tem que corresponder ao valor dessa coordenada, desse valor de coordenada. Então, de novo, eu vou ter aqui a minha imagem de entrada, vou ter que me a Rede Convolucional que vai aprender características, a partir da minha imagem.\n",
        "\n",
        "[07:58] E a partir dessas características que eu vou aprender, eu vou conectar aqui com os meus neurônios para fazer a regressão de quatro valores, é só uma revisão básica dos problemas de classificação e regressão com Redes Neurais de novo recomendo ver a aula de Treinando modelos.\n",
        "\n",
        "[08:18] E para finalizar a segmentação é um problema que dentre eles as pessoas menos conhecem dentre os três que nós vimos, mas é muito comum o uso de Redes Convolucionais para esse tipo de problema. Aqui tem uma imagem do próprio “Pascal voc” porque ele fornece dois tipos de rótulo: os rótulos de detecção que é o que nós viu agora, e os rótulos de segmentação que são rótulos mais densos.\n",
        "\n",
        "[08:44] Ou seja, em vez de definir um o retângulo onde está essa bicicleta eu vou definir o mapa Pixel a pixel, que define os pixels onde estão a bicicleta, chamamos de máscara. A máscara de pixels que define a região onde está a bicicleta em um nível de detalhe muito mais rico.\n",
        "\n",
        "[09:10] “Pascal voc” fornece esse tipo de rótulo para alguns dados e para carregar é só carregar em vez do voc detection o voc segmentation.Então eu vou carregar aqui mais rapidamente para nós não gastarmos muito tempo com isso, voc segmentation\n",
        "\n",
        "[09:30] Vou chamar de VOCs, aqui é a mesma coisa que nós fez agora pouco eu acho até que não precisava nem eu ter mudado nada, é que são as transformações que eu possa apagar. De novo Transformers para `tensor’, com ‘transform=transform.ToTensor’\n",
        "\n",
        "[09:56] Eu não vou baixar o treino dessa vez que da última vez demorou muito, então eu quero só a validação aqui que eu quero, só a validação 2012. Beleza, vamos baixar os dados como já baixamos o “Pascal voc” lá em cima, ele usou o mesmo arquivo que tínhamos baixado só que só baixando os rótulos, indexando os rótulos.\n",
        "\n",
        "[10:21] Aqui vai ser muito mais simples de entender, porque tanto o dado quanto o rótulo do “Pascal voc” para segmentação são imagens, então porque que são imagens? Vamos pegar um exemplo aqui, se nós printarmos o tipo, nesse caso aqui eu cometi um erro que eu deveria ter transformado também o Target porque os dois aqui são imagens.\n",
        "\n",
        "[10:47] Não vou precisar mais baixar porque já está baixado, ele só vai fazer a transformação do target, nesse caso os dois são imagens porque um é a imagem original e o outro é a máscara que define onde está o objeto, como é uma máscara densa ela é de mesmo tamanho que a imagem, vamos ver o tipo do dado, tipo do rótulo e já imprimimos também o tamanho dessas coisas, para facilitar rótulo.size().\n",
        "\n",
        "[11:20] Para vermos que nesse caso de segmentação, o dado e o rótulo vão ser de mesmo tamanho, os dois são tensores que eu converti os dois, o dado é uma imagem 366 x 500 com três canais de cor, uma imagem colorida e o rótulo é do mesmo tamanho da imagem só que com um canal de corpo que é uma máscara binária igual 1,onde tem um objeto, igual a zero onde não tem objetos.\n",
        "\n",
        "[11:49] Então se eu plotar aqui, eu vou precisar fazer a permuta no dado, porque lembrando a visualização considera o canal por último, então vou fazer a permuta para jogar dimensão do canal para o final, vou plotar aqui o meu dado, ‘dado = dado.permute(1, 2, 0)’, uma figura maior como sempre, plt.figsize=(8, 6) e ‘plt.imshow(dado)’.\n",
        "\n",
        "[12:16] Vou plotar o dado e vou plotar o rótulo, ignorando a dimensão do canal e com mapa de cor preto e branco que se não ele imprime verde, então vamos ver essa aqui a imagem e esse aqui é o mapa de segmentação.\n",
        "\n",
        "[12:35] Nesse caso, do “pascal voc” ele dá o mapa de segmentação das bordas aqui, mas você já pode inferir a partir disso uma máscara é dentro desse polígono, dentro dessa estrutura definida pelas bordas, então tudo que está dentro dessas bordas, também é Pixel que pertence ao objeto que nesse caso é o avião.\n",
        "\n",
        "Fotografia em visão diagonal de um avião da companhia Lufthansa estacionado em uma pista com equipamentos ao redor e uma escada de metal na porta. Ao fundo, há outros aviões estacionados ao longe. Abaixo, há outra imagem somente com o contorno do avião com linhas brancas em fundo preto. Ambas as imagens possuem graduação de trezentos e cinquenta a zero a cada cinquenta pontos no eixo vertical, e zero a quatrocentos no horizontal a cada cem pontos.\n",
        "\n",
        "[13:01] Então nesse caso, não vamos tentar dizer como é a rede que tenta fazer segmentação, porque nesse caso são redes totalmente convolucionais, ou seja depende de conhecimento que nós ainda não adquirimos.\n",
        "\n",
        "[13:15] A premissa que vocês já sabem, como que os MLPs funcionam porque vai ser essencial para compreender o restante desse curso que foi que nós fez para entender e classificação e detecção, são problemas de problemas classificação e regressão da detecção na última camada vai ser um MLP.\n",
        "\n",
        "[13:36] No caso da segmentação a última camada também vai ser convolucional, então mais para o final do curso vocês vão entender como resolver segmentação com redes convolucionais."
      ],
      "metadata": {
        "id": "E9d_5pbi9qRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TSvCp2mL9qAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No script Datasets.ipynb, carregamos o dataset de classificação MNIST e imprimimos seus 10 primeiros elementos.\n",
        "\n",
        "Acrescente uma célula no script para carregar dessa vez o CIFAR10. Plote com o matplotlib uma quantidade N de amostras tal que todas as classes do conjunto estejam impressas na tela.\n",
        "\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "De acordo com a ordem de carregamento dos dados do CIFAR no Torchvision, é preciso carregar pelo menos 26 imagens para que todas as 10 classes do dataset estejam visíveis, produzindo o código a seguir.\n",
        "\n",
        "data = datasets.CIFAR10('./',\n",
        "                      train=False,\n",
        "                      download=True,\n",
        "                      transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "dado, rotulo = data[0]\n",
        "print(type(dado), type(rotulo))\n",
        "\n",
        "# Channel First: Padrão do torch\n",
        "print(dado.size()) # C x H x W\n",
        "\n",
        "fig, axs = plt.subplots(1,26, figsize=(28, 4))\n",
        "\n",
        "for i in range(26):\n",
        "  dado, rotulo = data[i]\n",
        "  # Lembre-se que imagens são carregadas na dimensão C x H x W\n",
        "  # é preciso permutar as dimensões para plot.\n",
        "  axs[i].imshow(dado.permute(1,2,0))\n",
        "  axs[i].set_title(str(rotulo))\n",
        "\n",
        "plt.show()COPIAR CÓDIGO\n"
      ],
      "metadata": {
        "id": "iVUy54eP-Z9f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8saK2vMm-QeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets no PyTorch\n",
        "No script Datasets.ipynb, conhecemos as aplicações mais comuns para CNNs através dos datasets fornecidos no pacote Torchvision do PyTorch. Vale a pena explorarmos um pouco melhor essa biblioteca de datasets, e a importância desses repositórios de dados.\n",
        "\n",
        "A primeira dúvida que pode pairar na sua cabeça é:\n",
        "\n",
        "Eu irei trabalhar com meus próprios dados, por que precisaria dos dados do Torchvision?\n",
        "\n",
        "Pois saiba que esses datasets são especialmente úteis para auxiliar na solução de outros problemas. As 3 principais utilidades para os datasets clássicos da literatura são:\n",
        "\n",
        "Conhecendo os problemas clássicos. Como fizemos na nossa aula, os datasets do Torchvision nos ajudam a conhecer os principais problemas de visão computacional, a área relacionada a sistemas que tentam compreender informação visual (imagens, vídeos, etc.). Pra quem tá começando agora, carregar e conhecer os datasets disponíveis podem abrir sua cabeça para os diferentes usos de uma CNN.\n",
        "\n",
        "Testando a qualidade de modelos. Ainda que o seu objetivo seja trabalhar com um dataset muito específico, talvez até privado, é interessante avaliar a sua abordagem de solução em um dataset já consolidado na literatura. Por serem datasets amplamente utilizados, existe uma referência de bom desempenho para esses dados. Caso o problema que deseja resolver se aproxime de alguma das aplicações contidas no Torchvision, vale a pena o test-drive do seu modelo nesses dados.\n",
        "\n",
        "Pré-treinando modelos. Dentre os datasets do Torchvision, alguns são o que chamamos de large scale (grande escala), ou seja, conjuntos com uma quantidade significativa de dados e portanto capazes de treinar modelos mais robustos. Esses dados são ideais para o pré-treinamento de modelos, que se tornam capazes de generalizar para muitos outros problemas através de simples ajustes. Um exemplo muito comum são os modelos treinados no ImageNet que veremos mais à frente nesse post. O Torchvision possui uma biblioteca com os principais modelos pré-treinados no ImageNet para classificação de imagens.\n",
        "\n",
        "Já conhecemos a ponta do iceberg que representa os problemas com imagens: classificação, detecção e segmentação. O PyTorch traz inúmeros datasets relacionados a esses problemas, vamos conhecer alguns deles a seguir. Além das aplicações clássicas, há uma infinidade de outras informações que podem ser extraídas a partir de imagens. Também comentaremos algumas dessas ao final.\n",
        "\n",
        "Classificação\n",
        "MNIST e FashionMNIST: Imagens em escala de cinza representando dígitos escritos à mão (MNIST), e peças de roupas (FashionMNIST). Trata-se de um dataset mais simples, pois as informações são apresentadas de forma controlada (centralizada e frontal), além de ter o fundo removido, destacando somente a imagem relevante. Há também variações contendo caracteres japoneses (KMNIST) e também letras (EMNIST)Variações do MNIST\n",
        "\n",
        "CIFAR: Um dataset de imagens minúsculas (tiny images) como referido pelos próprios autores. São imagens naturais, ou seja, fotografias do mundo real (com ruído, fundo, variação de luz, etc.), sendo portanto muito mais desafiador que o MNIST. Possui duas variações: CIFAR10, com 10 categorias de objetos e CIFAR100, com 100 categorias.Variações do CIFAR\n",
        "\n",
        "ImageNet: Um dos datasets mais importantes da história da visão computacional, traz mais de um milhão de imagens anotadas que variam entre 1000 categorias de objetos. Modelos treinados nesse dataset são capazes de generalizar para outros problemas, como já extensivamente demonstrado por pesquisadores. O PyTorch inclusive traz uma biblioteca com os principais modelos de classificação de imagens pré-treinados no ImageNet.ImageNet\n",
        "Além de objetos, o Torchvision tem também datasets para classificação de cenas (LSUN) e imagens de celebridades (CelebA) com múltiplos rótulos de classificação, como a identidade dos indivíduos ou atributos do rosto (franja, óculos, etc.).\n",
        "\n",
        "Detecção\n",
        "Em geral, datasets de detecção e segmentação oferecem também o rótulo com a categoria daquele elemento, por ser uma anotação mais simples de ser adquirida (apenas um identificador, um número, que representa a imagem inteira). O contrário não é verdade, poucos datasets de classificação oferecem rótulos com maior nível de detalhe como bounding boxes para detecção ou máscaras para segmentação.\n",
        "\n",
        "SVHN: Apesar deste ser um dataset majoritariamente de classificação, decidi trazê-lo para esta seção pois ele também fornece rótulos de detecção. Da mesma forma que o MNIST original, este também é um dataset de classificação de dígitos, com um diferencial: são imagens naturais provenientes do Google Street View. A imagem a seguir mostra exemplos das imagens acompanhadas dos rótulos de detecção.\n",
        "\n",
        "\n",
        "\n",
        "CocoDetection: COCO (Common objects in context, objetos comuns em contexto) é uma competição que fornece um dataset de grande escala com múltiplas modalidades de rótulo, dentre eles a detecção. A página do desafio traz inclusive uma tabela de classificação com o desempenho dos melhores participantes, servindo de referencial para novas metodologias.\n",
        "\n",
        "PascalVOC - Detection: O Pascal Visual Object Classes também foi uma popular competição com múltiplas modalidades que consolidou um dos principais datasets de imagem. A imagem a seguir demonstra resultados de classificação e segmentação de um trabalho acadêmico.\n",
        "Segmentação\n",
        "Como vimos na seção de detecção, os datasets COCO e PascalVOC também oferecem rótulos de segmentação, no entanto o Torchvision fornece esse tipo de rótulo somente para o PascalVOC. Além dele, há também um dataset muito importante para a área de segmentação:\n",
        "\n",
        "Cityscapes: contendo imagens de cenas urbanas, este dataset oferece uma variedade robusta de imagens em diferentes cidades europeias, com diferentes condições de clima. Ele oferece rótulos para os objetos mais comuns nesse tipo de cena, como árvores, humanos, veículos, placas de trânsito, entre outros. Visite a página do dataset para maiores detalhes.\n",
        "\n",
        "\n",
        "Note que quanto mais denso o rótulo (classe, bounding box, máscara de segmentação), mais escassa é a quantidade de datasets e amostras por dataset, dada a maior dificuldade de produzir tais rótulos.\n",
        "\n",
        "Outras aplicações\n",
        "Mas há também problemas como Image Captioning (legendagem de imagens), que mistura o uso de CNNs com outro tipo de rede, responsável pela geração do texto da legenda. Veja na imagem a seguir um exemplo do dataset CocoCaption, também encontrado no Torchvision. Nessa mesma categoria de problemas, o Torchvision traz também o SBU.\n",
        "\n",
        "\n",
        "\n",
        "Uma parte importante (e difícil) da visão computacional é o trabalho com vídeos. Um dos principais desafios nessa área é o Reconhecimento de ações. É um problema de classificação para vídeos, onde o rótulo faz referência à principal ação sendo realizada no vídeo. Para essa aplicação, o Torchvision possui três datasets:\n",
        "\n",
        "Kinetics-400\n",
        "HMDB51\n",
        "UCF101\n",
        "Os números no nome de cada dataset fazem referência à quantidade de categorias de ação anotadas. A imagem a seguir apresenta exemplos retirados dos três datasets.\n",
        "\n",
        "Faça bom proveito dessa rica biblioteca de dados! Seja para pré-treinar modelos, estudar ou experimentar uma nova abordagem, vale a pena recorrer primeiro para bases de dados já consolidadas.\n",
        "\n",
        "Representação numérica de uma imagem\n",
        "Motivação do uso de redes neurais convolucionais (CNN)\n",
        "Representações visuais de uma CNN\n",
        "Principais usos da CNN\n",
        "Datasets do Torchvision\n",
        "\n",
        "\n",
        "Olá, nessa aula agora vamos começar a entender como funciona operação da \"convolução\" e apesar do curso focar em imagens onde é usada a convolução \"2D\" nós vamos entender futuramente, para entender a base da operação nós vamos usar primeiro a convolução \"1D\".\n",
        "\n",
        "Imagem com título \"Convolução\" seguido do texto \"Matematicamente falando:\". Abaixo, há uma fórmula matemática em que \"h\" de \"x\" é igual a, abre parênteses, \"f\" vezes \"g\", fecha parênteses, vezes \"x\", sendo igual ao somatório de \"f\" de \"u\", vezes \"g\", abre parênteses, \"x\" menos \"u\", fecha parênteses, em que \"u\" minúsculo varia de zero a \"U\" maiúsculo.\n",
        "\n",
        "[00:21] Então vamos primeiro falar matematicamente o que é a convolução. De forma simples: é operação entre duas funções que resultem uma terceira função Então vamos ter um ´f (x)’ e um ‘g(x)’, duas funções que quando operados vão resultar num ´h(x)´ é o que está aqui ´h(x)´ produto de duas funções.\n",
        "\n",
        "[00:49] Essa operação, ela é representada por um asterisco então a operação que guia a convolução nós vamos representar sempre por um asterisco, e ela é o somatório entre duas funções sendo uma invertida e deslocada.\n",
        "\n",
        "[01:13] Então o que que acontece é esse final aqui da operação, vamos operar \"f\" com \"g\" a partir de uma soma de valores que vai ser o deslocamento de uma dessas funções, vamos ver direito agora como funciona ilustrando em 1D.\n",
        "\n",
        "[01:31] Então vamos supor que nós temos as nossas funções ´f´ e ´g´ e nós vamos convoluir Essas funções, pensando de forma prática se você mostra um conjunto de valores, essas funções elas vão gerar dados.\n",
        "\n",
        "[01:45] Então mais para frente nós vamos falar de imagens como funções, vamos falar de camadas como funções, então você pode imaginar que você que isso aqui é um dado, a função ´f´ é um dado e a função ´g´ é o filtro que você vai passar nesse dado, mais para frente nós vamos dar nome aos dois.\n",
        "\n",
        "[02:04] Então o que vamos fazer? Vamos fazer o produto dessas funções, sendo que uma delas vai ser invertida, no caso invertemos a função ´g´, e vai ser deslocado, ou seja a função que foi invertida vamos deslocar ao longo da função de entrada que é a ´f´.\n",
        "\n",
        "[02:24] Então invertemos a função ´g´ e vamos deslocar ela ao longo da função ´f´ e vamos ter como resultado a nossa função ´h´, que é essa função aqui resultante.\n",
        "\n",
        "Imagem com título \"Convolução\" seguido do texto \"Somatório do produto entre funções, sendo uma delas invertida e deslocada\". Abaixo, há uma fórmula em que \"h\" de \"x\" é igual a, abre parênteses, \"f\" vezes \"g\", fecha parênteses, vezes \"x\" entre parênteses. Em seguida, há um subtítulo \"Convolution\" com uma representação de uma linha chamada \"f\" que inicia horizontalmente, sobe, vira a direita e desce para continuar a linha no mesmo nível do início, formando três arestas de um quadrado, e abaixo há a linha \"g\" que inicia horizontalmente, sobe e desce em diagonal para finalizar horizontalmente no mesmo nível do início, formando duas arestas de um triângulo. Ao lado destas linhas, há suas respectivas versões espelhadas. Ao lado dessas linhas, há uma representação gráfica de uma linha que representa a operação \"f\" vezes \"g\" com uma linha que inicia horizontal, faz uma curva brusca crescente e desce em curva para se alinhar horizontalmente, formando uma figura de uma onda. Há três setas de cores distintas, uma no meio da curva ascendente, outra maior no topo da curva e outra bem pequena quase no final da onda antes de alinhar horizontalmente. Abaixo, estão indicadas as sobreposições das linhas \"f\" e \"g\" descritas antes, em que há o alinhamento central perfeito entre elas na indicação da seta do topo da curva \"f*g\", sobreposição deslocada para a esquerda próximo a primeira seta do início da onda, e sobreposição deslocada para a direita no final da curva.\n",
        "\n",
        "[02:37] Basicamente, isso já conseguimos ver desse exemplo que aliás é o exemplo que está no \"Wikipédia\" de convolução, que a operação da convolução, esse filtro que a nossa função ´g´ que estamos passando na imagem ele está fazendo um casamento de padrões.\n",
        "\n",
        "[02:56] Então por que que a função resultante tem seu pico aqui no meio? Porque é nesse ponto onde o casamento de padrões entre as duas funções alcança o seu ponto máximo, o melhor casamento possível entre essas duas funções porque ela se sobrepõe completamente.\n",
        "\n",
        "[03:16] Já na seta indicada em amarelo aqui da esquerda, temos um casamento bom entre as duas funções, ela está um pouco deslocada para esquerda, a função ´g´, mas não é tão bom quanto o casamento aqui do meio, e quando essa função é mais deslocada para frente, aí o resultado, o casamento entre essas duas funções se torna cada vez pior, vai decrescendo.\n",
        "\n",
        "[03:43] Então basicamente, a convolução ela é a operação entre duas funções: um dado e um kernel, um filtro, depois entraremos nas definições melhores, onde esse filtro vai ser invertido e deslocado ao longo do dado de entrada.\n",
        "\n",
        "[04:03] No próximo vídeo vamos ver na prática o que isso significa.\n",
        "\n",
        "\n",
        "\n",
        "Vamos colocar a mão na massa aqui e ver como é que funciona a convolução numa situação onde o sinal e o filtro só têm uma dimensão, e também uma coisa importante de falar sobre a convolução \"1D\", é que essa função só se desloca em uma dimensão, então uma coisa que veremos mais para frente nas convoluções com mais dimensões, é que ela também vai se deslocar em várias dimensões, no caso da \"1D\", ela só se desloca na dimensão do sinal.\n",
        "\n",
        "[00:30] Então vamos assumir o seguinte problema: você decidiu coletar dados de acelerômetro do celular, então você pediu para as pessoas colocarem o celular no bolso e saírem andando com ele, e aqui eu trouxe até um trabalho real que fez uma análise desse tipo e mostrou que a amplitude do sinal do acelerômetro fica mais ou menos como se fosse uma senóide ruidosa, então cada ciclo da senóide é um passo que a pessoa dá.\n",
        "\n",
        "Imagem com um gráfico \"a)\" com eixo vertical de \"amplitude\" graduado de zero vírgula cinco a dois a cada meio ponto, e eixo horizontal de \"sample\" variando de zero a duzentos e cinquenta a cada cinquenta pontos. A linha do gráfico apresenta diversos picos negativos e positivos que abrangem a extensão dos eixos. Abaixo, há outra representação gráfica chamada “b)” em cores mais quentes e frias relativa aos valores que representam, cujo eixo vertical \"scale\" varia de trinta a um a cada vinte e cinco pontos, e eixo horizontal \"sample\" variando de zero a duzentos e cinquenta a cada cinquenta pontos, em que encontramos cerca de quinze zonas \"quentes\" em formato vertical no gráfico e próximas aos valores quinze e dez do eixo vertical.\n",
        "\n",
        "[00:56] Então vamos simular aqui essa senóide ruidosa e vamos supor que nós queremos localizar os intervalos crescentes do sinal, e faremos isso só com filtro de convolução \"1D\".\n",
        "\n",
        "[01:09] Então vamos lá, aqui eu já deixei import matplotlib.pyplot as plt, import numpy as np, que vai ser o que vamos acabar sempre usando aqui, e vamos fazer a convolução a partir da biblioteca dos \"scipy\", que é o \"Python Científico\", from scipy.signal import convolve, vamos simular esse sinal, essa senóide ruidosa.\n",
        "\n",
        "[01:41] Primeiro vou criar um eixo \"X\" aqui, um intervalo de valores entre \"0\" e \"100\". Eu quero \"100\" valores, então ele vai se gerar 0,1,2,3, daí por diante, e o meu \"y\" vai ser uma senóide, eu vou aumentar um pouco a amplitude dela, uma senoide desses dados de \"X\", só que eu vou multiplicar um ruído aqui, só para gerarmos um cenário parecido com a amplitude do acelerômetro. x = np.linspace(0, 100, 100) ey = 10 * np.sin(x) * np.random.rand(x.shape[0]).\n",
        "\n",
        "[02:13] Então vamos plotar esse sinal aqui, vamos aumentar a figura um pouco para enxergarmos bem, plt.figure(figsize=(12, 4) e plt.plot(x, y). Então fizemos aqui a nossa senóide ruidosa, e vamos, em cima dessa senóide, criar um filtro convolucional que vai conseguir detectar os intervalos crescentes desse sinal.\n",
        "\n",
        "Gráfico com eixo horizontal graduado de zero a cem a cada vinte pontos, e eixo vertical graduado de menos dez a dez ponto zero a cada dois pontos e meio. A linha do gráfico apresenta diversos picos negativos e positivos, cerca de dezoito negativos mais expressivos e dezesseis positivos na abrangência dos eixos.\n",
        "\n",
        "[02:38] Então vamos pegar aqui, para facilitar eu deixei uma função que só imprime a função de, no caso essa senoide aqui, como se fosse imagens, já para nós colocarmos um pé no mundo das imagens e entender como é que se funciona, já nas convoluções \"1D\" também.\n",
        "\n",
        "[02:59] Então pegaremos um pequeno trecho do sinal, vamos pegar de \"5\" a \"15\", talvez, vamos pegar um pequeno trecho do sinal e eu vou usar essa minha função para plotar o sinal como uma imagem, sinal = y[5:15] e show (sinal, 'sinal'), vou plotar ele também no gráfico do mesmo jeito que eu fiz aqui em cima, plt.plot(sinal) só para compararmos as duas representações.\n",
        "\n",
        "Gráfico plotado com título \"Sinal\", eixo horizontal variando de zero a dez a cada cinco pontos, e eixo vertical sem valores. A linha inicia na metade, desce até próximo de dois e meio do eixo horizontal, sobe novamente até o topo do gráfico e depois desce vertiginosamente até o eixo horizontal. Sobreposto ao gráfico, há valores em vermelho ilegíveis.\n",
        "\n",
        "[03:44] Eu vou tentar deixar as figuras do mesmo tamanho plt.figure (figsize = (10, 4)), porque eu acho que vai ser melhor. Estou querendo mostrar para vocês que cada ponto nessa nossa imagem está quase perfeito, cada ponto nessa nossa imagem equivale a um ponto do sinal então se eu tenho aqui \"- 1.4\" representado em cinza ao mesmo tempo eu tenho \"5.8\", \"7.1\" que praticamente é branco, significa que é o ponto máximo desse meu sinal.\n",
        "\n",
        "[04:28] Então eu tenho aqui em cima nessa representação com cores, algo que se assemelha a representação em pixels de uma imagem, então conseguimos ver a altura do sinal pela cor dessa imagem, se é cinza, cinza escuro, é alto mas não é o ponto máximo, digo: se é cinza escuro é baixo, se é cinza claro, é alto porque está tendendo a 1, se é preto é o ponto mínimo, se é branco é o ponto máximo.\n",
        "\n",
        "Imagem do output com uma barra nomeado \"sinal\" dividia em dez partes com tons diferentes de cinza, em que os valores mais altos como cinco ponto oito e sete ponto um possuem tons mais claros próximos ao branco, e valores mais baixos como menos cinco ponto oito e menos seis ponto dois possuem os tons mais escuros próximos ao preto. Abaixo, há um gráfico de eixo horizontal graduado de zero a dez a cada dois pontos, e eixo vertical graduado de menos seis a oito a cada dois pontos, em que a linha inicia na localização de zero do eixo \"x\" e menos dois do eixo \"y\", sobe até próximo do três do eixo \"x\" e seis do \"y\", desce com algumas variações até seis do eixo \"x\" e menos seis do eixo \"y\", e despois sobre vertiginosamente em linha reta até o final do gráfico.\n",
        "\n",
        "[05:01] Então como que a intensidade da cor se refere a valores. É a intensidade do sinal, digamos assim, então aqui temos a intensidade do sinal representada de duas formas diferentes, e o que vamos fazer é identificar os intervalos crescentes nesse pedaço do sinal e depois no sinal completo.\n",
        "\n",
        "[05:26] Temos aqui basicamente dois intervalos crescentes um bem acentuado e outro um pouco mais leve e teoricamente é pro nosso código conseguir identificar os dois, claro, com graus diferentes, se queremos procurar intervalos crescentes, ele vai dizer que esse é um pouco menos crescente do que esse aqui que é um sinal bem tenso.\n",
        "\n",
        "[05:48] Então no contexto de processamento de imagens o \"Kernel\" é um filtro convolucional, que vimos quando mostramos os sinais sendo operados um com o outro, então vamos ter um dado de entrada e um filtro, o que temos que fazer agora que já temos o dado é criar um filtro que simula o padrão procurado, um \"Kernel\" que se aproxime do padrão procurado.\n",
        "\n",
        "Kernel\n",
        "No contexto de processamento de imagens, o kernel é um filtro convolucional. De forma prática, é uma matriz n-dimensional que será operada com o dado através de uma convolução.\n",
        "\n",
        "Pode-se dizer que a convolução mede a semelhança entre os dois sinais.\n",
        "\n",
        "Precisamos portanto propor um kernel que simule o padrão procurado: intervalos crescentes.\n",
        "\n",
        "Mas lembre-se que a convolução opera as funções após inverter o kernel.\n",
        "\n",
        "[06:15] Se encontrarmos intervalos crescentes no nosso sinal, temos que produzir um \"Kernel\" que também carregue esse padrão com ele, que também seja um intervalo crescente, justamente porque a operação da convolução vai medir a semelhança entre dois sinais, então meu filtro tem que ter o sinal que eu quero encontrar no meu dado.\n",
        "\n",
        "[06:38] Mas lembre-se que a convolução opera as funções após inverter o \"Kernel\", então se eu quero encontrar intervalos crescentes, eu vou fazer aqui um kernel = np.asarray [1. 0, -1], show(kernel, 'Kernel'), ele só vai conseguir fazer esse plot com arrays.\n",
        "\n",
        "Imagem do output com título \"Kernel\" contendo uma barra dividia em três valores com seus respectivos tons de cinza, em que o menor valor é um ponto zero de cor branca, o valor do meio é zero ponto zero de cor cinza médio, e o último valor de menos um ponto zero possui a cor preta. O eixo horizontal possui os valores zero, um e dois relativo aos tons.\n",
        "\n",
        "[07:10] Então vamos lá, aqui eu tenho meu \"Kernel\", que vai simular um intervalo decrescente, ele está fazendo isso, certo? Porque que eu fiz ele decrescente se eu quero procurar intervalos crescentes no meu dado? Porque, o que ele vai operar efetivamente quando eu chamar a função convolução é esse Kernel invertido.\n",
        "\n",
        "[07:32] Então o kernel_invertido, se chamar função flip do NumPy, ‘kernel_invertido = np.flip(kernel)’, vai fazer exatamente essa inversão da convolução, show (kernel_invertido, ‘Kernel Invertido’). Então aqui agora se eu inverter esse meu \"Kernel\", eu tenho exatamente o sinal crescente que eu gostaria de procurar no meu dado.\n",
        "\n",
        "[08:05] Então assim, já criamos o dado, já criamos o \"Kernel\" que vai fazer o casamento que nós precisamos, então vamos agora é deslocar ele ao longo do sinal e entender a operação.\n",
        "\n",
        "[08:18] Então vamos lá, eu fiz aqui que a famosa gambiarra que na verdade é um artifício de programação, para conseguirmos deslocar a imagem para conseguiremos fazer as contas \"Pixel a Pixel\" na hora de fazer esse cálculo, claro que depois vamos usar a função convolve para fazer o cálculo automaticamente, mas primeiro faremos na mão, então vamos lá.\n",
        "\n",
        "[08:43] Aqui é o nosso artifício de código para nós poder deslocar o \"Kernel\" e operar com a imagem original show (kernel_ deslocado, ‘Kernel’) show (sinal, 'Sinal'). E eu deixei essa variável de controle para eu ir deslocando o sinal pouco a pouco, então vamos lá. Primeiro não vou deslocar nada para começarmos a fazer nossas contas, vamos ao que interessa como que funciona convolução.\n",
        "\n",
        "[09:19] A operação da convolução basicamente é um produto de ponto-a-ponto, depois você vai somar e vai ter o resultado, então como estamos fazendo de um modo de operação sem adição de zero, sem nenhum outro artifício que a convolução permite, temos que centralizar o filtro na posição correspondente do sinal, então se o meu filtro de tamanho \"3\" o primeiro pixel que eu posso operar é esse pixel aqui, porque eu preciso sobrepor o meu \"Kernel\" no trecho do sinal que eu quero operar.\n",
        "\n",
        "[09:58] Se eu quisesse operar com esse pixel, eu teria que sobrepor o meu \"Kernel\" aqui, mas não existe valor aqui, então eu não consigo fazer essa operação. Então o primeiro pixel que eu consigo operar é o \"0.9\" e aqui eu consigo fazer a minha primeira operação ponto a ponto, então eu faço? \"-1 * - 1.4 = 1.4\". Agora, \"1.4 + 0 + 5 = 6,4\" então aqui é o primeiro sinal resultante aqui.\n",
        "\n",
        "[10:38] Se formos deslocando esse \"Kernel\", vamos lá deslocamento agora um ´u=1´, desbloqueei, agora nós vamos operar com esse pixel ‘-0.9’, nesse intervalo aqui’[-0.9, 5.0, 5.8]’ ponto a ponto.\n",
        "\n",
        "[10:56] Então vai ser \" -1 - 0.9 + 0 + 5.8 = 6.7\", o pixel resultante vai ser \"6.7\", então o que estamos fazendo aqui nessas contas basicamente estamos criando um sinal resultante que vai ser a nossa ativação, que o primeiro pixel é a operação fizemos aqui que se eu não me engano deu \"6.4\", o segundo pixel resultante é a operação que fizemos agora, que fizemos pixel a pixel que deu \"6.7\".\n",
        "\n",
        "[11:48] O terceiro pixel é a operação que faríamos aqui deslocando pouco, aí faria pixel com pixel aqui e daria \"-5 + 2.8\" que vai dar \"2.2\", e assim por diante, se deslocarmos mais um pouco aqui eu desloco até o \"6\", deslocar mais um pouco para frente até o 4 para facilitar, estaríamos fazendo o quarto pixel, que é centralizando o \"Kernel\" com esse pixel e fazer a operação aqui de novo.\n",
        "\n",
        "[12:37] Então basicamente vamos fazendo isso ponto a ponto e gerando um sinal resultante que é a função resultante que comentamos lá no slide.\n",
        "\n",
        "[12:47] Então vamos seguir adiante para fazermos agora automaticamente, porque só fizemos na mão para entender como funciona, é um produto ponto a ponto somado no final, isso para cada deslocamento do nosso \"Kernel\", então se eu fizer aqui resultado, vou chamar de ativação que é a palavra certa.\n",
        "\n",
        "[13:07] A minha função de ativação final, meu mapa de ativação, meu mapa de características vai ser a operação de convolução do sinal com o \"Kernel, então ele vai convoluir o meu sinal de entrada com meu \"Kernel\" e eu vou colocar o modo válido aqui só para ele fazer a operação exatamente como fizemos sem usar nenhum artifício extra, e vou mostrar aqui o resultado dessa ativação, com ´ativacao = concolve(sinal, kernel, mode=’valide’)´ e ´show(ativação, ‘Mapa de Ativação’)´.\n",
        "\n",
        "Imagem do output com título \"Kernel\" contendo uma barra dividia em sete valores com seus respectivos tons de cinza e numeradas de zero a seis, em que as quatro primeiras partes com  valor \"nan\" e a última parte com um ponto zero estão na cor branca, o valor mediano de zero ponto zero em cinza na sexta parte, e o menor valor em preto é menos um ponto zero na quinta parte. Abaixo, há outra barra chamada \"Sinal\" divida em dez partes com o mesmo padrão de tons de cinza e valores numerados de zero a nove, sendo o valor mais alto sete ponto um na última parte em cor branca, e o menor valor menos seis ponto dois e menos cinco ponto oito nas partes sete e seis respectivamente.\n",
        "\n",
        "[13:58] Exatamente como nas nossas contas o resultado da convolução entre o sinal e o \"Kernel\" primeiro pixel \"6.4\", segundo \"6.7\", terceiro \"- 2.2\", e daí por diante, temos aqui um mapa de ativação, que é o resultado da convolução entre um sinal e um \"Kernel\".\n",
        "\n",
        "[14:18] Então o que temos aqui, para conseguirmos visualizar isso pelo que ele é, que é a nossa detecção de intervalos crescentes, o que isso significa? Que nos pontos de maior ativação foi onde ele encontrou o intervalo crescente mais significativo que é esse pixel aqui o \"9.4\", esse pixel, ele é referente a esse intervalo aqui.\n",
        "\n",
        "[14:48] Ele é referente a esse pixel aqui, que é o intervalo onde os 3 pixels aqui são mais crescentes, casam melhor com o \"Kernel\" que criamos, então esse é o intervalo de menos \"1.6\" até \"7.1\" que tem a maior crescente existente aqui no nosso sinal, então ele detectou esse pico aqui na ativação aqui é onde está o sinal mais crescente que eu encontrei, o padrão mais bem casado que eu encontrei.\n",
        "\n",
        "[15:22] Então aqui eu deixei um plot pronto, é só um plot eu vou até desfazer aqui para fazer de novo, é só uma figura que vamos fazer, ´plt.figure(figsize=(12,4))´, aqui iremos plotar o sinal, ´plt.plot(sinal, color=’k’, linewidth = 4)´, e coloquei esse plot da ativação como imagem, então essa ativação aqui veremos como imagem em cima do plot do meu sinal para vermos como funciona essa detecção.\n",
        "\n",
        "[16:18] Então aqui eu peguei exatamente esse mapa de ativação que calculamos, um pedaço na mão e plotei como imagem sobre o meu sinal que é o mesmo sinal que recortamos lá em cima é esse mesmo sinal, o que significa? Que onde tem intervalo crescente é onde ele vai ter suas ativações mais fortes.\n",
        "\n",
        "[16:41] E ativação mais fraca que ele vai ter é justamente no ponto onde tem um sinal oposto ao sinal que o \"Kernel\" representa, então se o \"Kernel\" representa o intervalo crescente o seu ponto mínimo de ativação vai ser no intervalo decrescente que o oposto que o \"Kernel\" representa.\n",
        "\n",
        "Imagem do output contendo um gráfico com eixo horizontal graduado de zero a oito a cada dois pontos, os quais dividem o gráfico em dez partes com tons de vermelho variando de acordo com a variação dos valores da linha, em que a ascensão da linha possui tons escuros, e a descensão está sobre tons claros. O eixo vertical está graduado de menos dez a dez a cada dois pontos e meio. A linha inicia próximo de zero dos dois eixos, sobe com algumas variações até o pico próximo do valor três do eixo \"x\" e seis do eixo \"y\", onde desce até o pico negativo no ponto seis do eixo \"x\" e menos seis do eixo \"y\", e por fim sobe vertiginosamente quase em linha reta até o final do gráfico.\n",
        "\n",
        "[17:04] E aqui eu rodei para o restante do sinal, mudei os nomes, maravilha, se eu rodo esse código aqui que é o mesmo código que fizemos aqui em cima só que para o sinal completo, temos aqui uma espécie de classificação dos intervalos crescentes do nosso sinal, simplesmente usando operação de convolução.\n",
        "\n",
        "[17:24] Eu peguei intervalo bem maior do meu sinal, fiz a convolução desse intervalo com meu \"Kernel\", fiz uma espécie de classificação falei todos os pontos onde a ativação for menor que zero, iguale a zero, só porque eu quero os intervalos crescentes, então só quero ativações positivas.\n",
        "\n",
        "Imagem do output contendo um gráfico com eixo horizontal graduado de zero a cinquenta a cada dez pontos, e eixo vertical graduado de menos dez a dez a cada dois pontos e meio. Ao lado, há um gradiente indicando a variação de cores vermelhas do menor valor zero da cor branca, e maior valor dez próximo ao vermelho escuro. O gráfico possui cerca de cinquenta divisões em tons de vermelho que variam de acordo com a variação e inclinação da linha, sendo os valores ascendentes em tons escuros e descendentes em tons mais claros. A linha possui cerca de oito picos positivos variados e oito picos negativos variados ao longo do gráfico.\n",
        "\n",
        "[17:49] E quando eu ploto isso aqui temos esse mapa de ativações que é forte onde tem intervalos crescentes e é negativo onde não tem esses intervalos crescentes que no caso do nosso sinal significa que são intervalos decrescentes, então aqui usamos uma simples convolução 1D para detectar intervalos crescentes em um sinal de acelerômetro.\n",
        "\n",
        "Para ilustrar o uso da Convolução 1D na busca de padrões em sinais, modelamos um kernel especializado em identificar intervalos crescentes em um sinal. Essa definição pode ser encontrada na seguinte célula:\n",
        "\n",
        "kernel = np.asarray([1,0,-1])\n",
        "show(kernel, 'Kernel')COPIAR CÓDIGO\n",
        "Proponha um outro kernel de mesmo tamanho, dessa vez para identificar os picos do sinal e teste-o no script Convolução1D.ipynb.\n",
        "\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "Sabendo que o kernel deve carregar o padrão que se busca no sinal, podemos sugerir o kernel a seguir.\n",
        "\n",
        "kernel = np.asarray([0,1,0])\n",
        "show(kernel, 'Kernel')COPIAR CÓDIGO\n",
        "Note que kernels com picos mais extremos, como [-1,1,-1], vão identificar melhor picos igualmente extremos no sinal, podendo ignorar intervalos onde o padrão é mais suave.\n",
        "\n",
        "Definição da operação Convolução\n",
        "Kernels para Convolução 1D\n",
        "Convolução com a biblioteca scipy\n",
        "Uso da convolução para Reconhecimento de Padrões\n",
        "\n",
        "Agora nós vamos falar de convolução no contexto desse curso que é o contexto de imagens, vamos usar a convolução 2D, ou seja, que consegue se deslocar em duas dimensões do espaço para características e aprender características em imagens, a partir de imagens.\n",
        "\n",
        "[00:21] Então agora que estamos falando de Convolução 2D, nós estamos falando de um “Kernel” que se desloca em duas dimensões porque precisamos processar imagens que tem as dimensões da altura e largura, estamos falando por enquanto, de imagens preto e branco,.\n",
        "\n",
        "[00:36] Então não tem canal, aqui nós temos uma imagem ilustrativa mas na mesma representação de uma imagem real que é uma matriz de valores, onde cada valor representa a intensidade daquele Pixel. Quanto maior o valor mais próximo do branco, quanto menor o valor mais próximo do preto.\n",
        "\n",
        "Imagem com título \"Convolução 2D\" contendo o texto \"O kernel se desloca em duas dimensões\". Abaixo, há uma tabela de seis por seis nomeada \"Imagem\", em que cada coluna possui uma progressão de tom cinza do mais claro ao mais escuro de acordo com os valores que possuem nas linhas. A primeira coluna da esquerda para a direita só possui valores \"90\" e cor branca, a segunda coluna possui só valores \"85\", a terceira só valores \"80\" com tons de cinza claro, a próxima só \"20\", a seguinte \"15\" e a última coluna de cor mais escura próximo ao preto possui só o valor \"10\" em todas as linhas.\n",
        "\n",
        "[00:55] E agora nós vamos propor aqui um “Kernel”. Se você fosse supor, eu acho que você já com base na Convolução 1D, você já deve conseguir supor qual tipo de padrão que esse “Kernel” procura, ele vai procurar bordas verticais, só que não do escuro para o claro como estamos vendo agora, mas do claro para o escuro que é como o “Kernel” invertido, lembrando a definição de convolução que nós demos na aula anterior.\n",
        "\n",
        "[01:25] A convolução ela vai operar duas funções nesse caso, a imagem e o “Kernel” sendo uma delas invertida que é o caso do “Kernel” invertido e deslocada ao longo do sinal, como estamos falando de Convolução 2D, esse deslocamento agora vai acontecer em duas dimensões.\n",
        "\n",
        "[01:42] Então nessa imagem, vamos procurar padrões de bordas verticais do claro para o escuro, ou seja, na região que casar melhor com esse padrão aqui modelado, ativação vai ser maior, já podemos até fazer uma suposição que a região de borda do claro para o escuro mais forte que temos aqui na imagem é alguma coisa por aqui, nessa região que vamos ter as ativações mais fortes, onde acontece a borda do claro para o escuro que essa borda que estamos vendo aqui.\n",
        "\n",
        "Mesma imagem anterior, mas há uma nova tabela ao lado da anterior chamada \"Kernel invertido\", a qual é composta por três colunas e três linhas. A primeira coluna da esquerda para a direita só possui valores \"1\" e cor branca, a segunda só valores zero com tom de cinza médio, e a terceira coluna só possui valores menos um e cor preta.\n",
        "\n",
        "[02:20] Nós vamos fazer agora é deslocar esse nosso “Kernel” ao longo da imagem para produzir um mapa de características, então aqui nós temos uma matriz, uma estrutura de 2 dimensões, que é o “Kernel” é o nosso filtro Convolucional, aqui a nossa imagem e o mapa de características resultante que vamos ver junto como que ele é criado.\n",
        "\n",
        "[02:46] Nós já vimos em uma dimensão mas agora em duas dimensões o que vai mudar é que vamos deslocar o “Kernel” em duas dimensões e produzir um resultado também duas dimensões.\n",
        "\n",
        "Imagem com título \"Convolução 2D\". Abaixo, há as duas tabelas descritas anteriormente, ambas com uma tabela feita com linhas tracejadas em vermelho apenas, contendo três linhas e três colunas que as sobrepõem, selecionando as primeiras três linhas das primeiras três colunas destas. Ao lado, há uma tabela chamada \"Mapa de características\" formada por quatro colunas e quatro linhas: A primeira coluna da esquerda para a direita possui cor preta e somente valores \"30\" em suas linhas, a segunda e terceira colunas possuem a cor branca e os valores \"195\" em todas as linhas, e a última coluna é idêntica à primeira. Nesta última tabela, apenas o primeiro quadrado com valor \"30\" da primeira linha está selecionada por uma linha vermelha.\n",
        "\n",
        "[02:57] Dado o comportamento dessa imagem aqui, nós só conseguimos operar a partir desse Pixel, por quê? Porque o nosso “Kernel” é 3 x 3, precisamos de um trecho de imagem que possua esse mesmo tamanho de pixel para nós começarmos a operar. Então nós só conseguimos operar a partir desse Pixel, porque se eu tentar operar por exemplo, com esse Pixel aqui eu tenho que centralizar o meu “Kernel” nesse Pixel, ou seja, eu vou precisar operar aqui com valores inexistentes.\n",
        "\n",
        "[03:32] Tem jeito de fazer isso? Sim, tem jeito de fazer isso, mas por enquanto nós estamos vendo a convolução mais tradicional, onde precisamos que a imagem exista na região onde vamos sobrepor o “Kernel”. O primeiro Pixel que podemos sobrepor é o Pixel 85 e com a operação da convolução vamos entender o padrão da vizinhança dele.\n",
        "\n",
        "[03:59] A resposta que estamos procurando é o quão provável é que a vizinhança do Pixel 85 contém uma borda vertical, que vai do claro para o escuro. Essa pergunta que nós estamos tentando responder. Então aqui vamos fazer uma operação ponto a ponto do “Kernel” que vai resultar em um único valor que vai ser esse primeiro Pixel aqui.\n",
        "\n",
        "[04:22] Vamos fazer: -1 x 90 + 0 x 85 que é a primeira linha aqui que eu estou operando as duas primeiras linhas, mais - 1 x 80 agora para segunda linha ponto a ponto vai ser: 1 x x90, 0 x 85 - 1 x 80 terceira linha agora: 1 x 90, 0 x 85 - 1 x 80.\n",
        "\n",
        "[04:54] Eu e minha calculadora mental, que já fez esse cálculo antes de começar a aula, sabemos que se você fizer essa soma de multiplicações Pixel a Pixel, o resultado vai ser 30 que vai ser o primeiro pixel do nosso mapa de características resultantes.\n",
        "\n",
        "[05:12] Na hora que você vai montar o mapa de características final, você vai obedecer a distribuição espacial da imagem de entrada, então se eu estou operando o primeiro pixel superior na esquerda, esse mapa de características, essa ativação também vai estar no canto superior esquerdo do mapa de características.\n",
        "\n",
        "[05:32] Se nós seguirmos adiante agora, deslocamos para a direita em uma dimensão o nosso “Kernel”, então agora eu estou operando com o Pixel aqui 80, vou fazer a mesma pergunta, a vizinhança dele aqui possui o padrão borda vertical do claro para o escuro? Que é o que o nosso “Kernel” mapeia. Aí vamos fazer a mesma, nós vamos fazer: 1 X 85, 0 x 80 -1 x 20 até o final vai somar todos esses produtos e vai dar o valor: 195.\n",
        "\n",
        "[06:12] 195 é maior que 30? Sim, que significa que na vizinhança desse Pixel é mais provável que tenha uma borda vertical do claro para o escuro, que é exatamente o que estamos procurando aqui, então a convolução é esse casamento de padrões onde no resultado você consegue saber onde que na imagem original está esse padrão.\n",
        "\n",
        "[06:37 ] Então eu estou aqui olhando para o meu mapa de características, que a borda vertical do claro para o escuro está mais ou menos no meio da imagem que eu olho para esse meu mapa de características e ele me diz que a borda está mais ou menos por aqui.\n",
        "\n",
        "Mesma imagem anterior, porém a seleção da tabela de seis por seis possui uma seleção feita por linhas tracejadas vermelhas que é de três por três, e abrange as primeiras três linhas da segunda, terceira e quarta coluna. Já a tabela de \"Mapa de características\" possui o primeiro valor \"195\" da primeira linha da segunda coluna selecionado por uma linha vermelha. Em cima destas seleções puxando para baixo, a instrutora fez um rabisco azul.\n",
        "\n",
        "[06:53] Se continuarmos aqui deslocando esse “Kernel” vamos colocar mais um pouquinho para direita, vai dar 195, porque aqui também uma borda do claro para o escuro, vai seguir adiante agora estamos numa região sem borda e vai dar um valor baixo porque aqui ele não localizou o padrão do “Kernel”.\n",
        "\n",
        "[07:13] Chegamos na nossa borda da imagem, chegamos a nossa borda de imagem de novo, se eu tentar deslocar mais para direita para operar com esse Pixel vou cair no mesmo problema que não existem valores aqui para eu operar, certo? Se não tem valores aqui para operar existe um artifício que permita que eu opere com uma borda inexistente, uma borda artificial.\n",
        "\n",
        "[07:41 ] Mas não estamos falando isso agora, então esse é o último Pixel da direita que eu consigo operar com “Kernel” 3 x 3, então agora vamos deslocar esse “Kernel” na segunda dimensão ele vai descer na dimensão Y agora e vai continuar deslocando para direita na dimensão X.\n",
        "\n",
        "[08:00] Como o “Kernel” tem duas dimensões ele também se desloca nas duas dimensões, como uma janela deslizante, como uma máquina de escrever da esquerda para direita, terminou próxima linha, daí por diante até nós chegarmos no último Pixel possível de operar na imagem que esse Pixel aqui.\n",
        "\n",
        "Mesma imagem anterior, porém a seleção da tabela de seis por seis possui uma seleção feita por linhas tracejadas vermelhas que é de três por três, e abrange as últimas três linhas das últimas três colunas, e a instrutora selecionou manualmente o valor \"15\" do centro da seleção. Já a tabela \"Mapa de características\" possui o ultimo valor \"30\" da última coluna selecionado por um quadrado de linha vermelha.\n",
        "\n",
        "[08:19] Que vai ter uma posição correspondente no mapa de características, também vai ser o último Pixel do mapa de características. Então essa informação espacial é mantida no mapa de características e você consegue saber exatamente onde está a característica que você procura na imagem original.\n",
        "\n",
        "[08:40] Então esse é o último pixel que nós conseguimos operar e como consequência isso mexe na dimensão espacial dessa transformação, aqui temos uma imagem que é 6 x 6 e já que nós não consegue operar com todos os pixels da imagem porque essa borda não existe Nós vamos partir de 6 x 6, para um resultado que é 4 x 4.\n",
        "\n",
        "[09:09] Nós perdemos dimensionalidade na hora de fazer essa transformação, as dimensões 2D 6 x 6 e viraram 4 x 4, porque na hora de fazer a Convolução eu não consigo operar com as bordas, por causa do tamanho do meu “Kernel”, então eu perdi todos esses pixels aqui e fiquei só com processamento dessa região, que não coincidentemente é uma região 4 x 4.\n",
        "\n",
        "[09:38] Para cada Pixel que eu opero na imagem original, vai ter um valor correspondente no mapa de característica, certo? Então essa perda de dimensionalidade vai ser um fator muito importante daqui para frente, porque na hora que você vai construir uma rede, você precisa estar atento às mudanças na dimensionalidade do seu dado que vão acontecer ao longo da rede.\n",
        "\n",
        "[10:02] Legal, então aqui nós já viu como é que a convolução funciona em duas dimensões já vimos que o “Kernel” que nós modelamos procura bordas verticais e isso é dado pelos pesos que estão nele, sabemos que ele procura por bordas verticais porque os valores da esquerda para direita vão aumentando,’- 1, 0, 1’.\n",
        "\n",
        "[10:22] Esses são os pesos do nosso “Kernel”, no caso do treinamento de redes convolucionais você vai inicializar vários Kernels com pesos aleatórios ou com peso igual a zero e ao longo do treinamento esses Kernels vão ser otimizados para procurar as características mais relevantes do problema, não sei o que é distinguir maçã de banana, a minha rede vai aprender quais são os padrões de imagem que os Kernels tem que carregar e tem que procurar nas imagens de entrada.\n",
        "\n",
        "[10:53] Quais são os padrões de imagem que vão melhor distinguir uma maçã de uma Banana, então por exemplo, eu posso dizer que bordas horizontais são importantes para distinguir maçã e banana que a banana é um meio uma lua, então vai ter bordas diagonais, por exemplo.\n",
        "\n",
        "[11:14] No próximo vídeo veremos na prática como isso funciona.\n",
        "\n",
        "Agora nós vamos ver em código mesmo como funciona essa convulsão, fazendo aqui uma busca por bordas horizontais e verticais em imagens, então de novo, assim como fizemos com Convolução 1D , vamos estar usando as bibliotecas no ply e matplotlib para operar e visualizar, vamos usar a biblioteca do scipy o “Python” científicos para fazer a convolução.\n",
        "\n",
        "[00:27] E aqui eu peguei umas operações, umas funções do skimage que ele vai dar a imagem para nós, então vamos baixar uma imagem onde vai ser muito óbvia a existência de borda, tanto na vertical quanto na horizontal, que é essa imagens, se eu não me engano é um chão de tijolos ou é uma parede de tijolos, vamos ver aqui.\n",
        "\n",
        "[00:52] Então vamos plotar a imagem com mapa de cores em imagem em cinza, que é imagem preto e branco.\n",
        "\n",
        "[01:07] Beleza, então temos aqui um chão de tijolos mesmo, com bordas muito claras tanto na horizontal quanto na vertical, aqui eu deixei pronto uma função de novo, só para ajudar nós com a visualização das nossas imagens dos nossos Kernels, então aqui eu tive que escrever o texto do campo à campo, com peso do “Kernel” eu já deixei isso pronto porque não vale a pena ficar falando muito de matplotlib.\n",
        "\n",
        "[01:36 ] O que interessa aqui é como definir os Kernels e operar os Kernels que é o que as camadas convolucionais vão fazer internamente, mas é importante que nós entenda isso fora da estrutura de uma rede primeiro.\n",
        "\n",
        "[01:48] Vamos definir duas Kernels, um Kernel de borda vertical e outro de borda horizontal, o borda vertical nós já conhecemos na aula teórica que aquele Kernel que vai fazer colunas de intensidade diferentes, então queremos fazer uma coluna preta, uma cinza e uma coluna branca, ele vai procurar as bordas que se assemelham ao padrão que ele carrega.\n",
        "\n",
        "[02:13] Aqui nós faremos [-1, 0 e 1] igual como fizemos na aula teórica e repetir, [-1, 0, 1] onde tem uma coluna preta, uma coluna cinza e uma coluna branca que ao visualizar como imagem que aquela função faz, shou(kernel_v ‘Kernel vertical').\n",
        "\n",
        "[02:35] Ele vai definir uma borda vertical, vai procurar esse padrão nas imagens, para nós construirmos um “Kernel” de bordas horizontais nós vamos ter que fazer em vez de coluna diferentes temos que fazer linhas diferentes, então nós vamos condicionais aqui, vamos fazer uma linha preta, uma linha cinza e uma linha Branca.\n",
        "\n",
        "[02:57] Então vai ser [-1, -1, -1 ] essa vez é nossa linha e que vai do escuro para o claro [0, 0, 0]a nossa linha cinza e [1, 1, 1] que é nossa linha clara, quando visualizarmos isso que é o nosso “Kernel” horizontal.\n",
        "\n",
        "[03:19] Visualizando se temos agora um padrão de bordas horizontais que vai buscar casamentos desse mesmo padrão na imagem. Lembrando que isso aqui vai ser invertido, rotacionado antes de ser operado com a imagem, então na verdade nós vamos estar procurando bordas que vão do claro para o escuro, da esquerda para direita e do claro para o escuro de cima para baixo.\n",
        "\n",
        "[03:45] Então vamos agora Convoluir os “Kernels” com a parede de tijolos, parede de tijolos está numa variável que chama “Imagem”, então agora vamos fazer a convolução usando a função do scipy.\n",
        "\n",
        "[03:56] Para isso o mapa de características, vai ser o resultado da convolução da minha imagem, primeiro com o “Kernel” de bordas verticais. Aqui eu tenho que definir o modo válido para que ele só opere com pixels válidos como vimos na aula teórica.\n",
        "\n",
        "[04:18] Então ele vai perder informação das bordas e agora esse mapa de características e nós vamos visualizar, então eu vou plotar de novo a imagem da parede de tijolos para conseguir comparar (img,cmap=Greys) e botar também o mapa de características plt.figure(), acho que eu vou pegar a definição da figura lá da função só para criar imagens do mesmo tamanho.\n",
        "\n",
        "[05:00] (mapa_de _característica, cmap= 'Grey'). Aqui temos em cima a parede de tijolos, em baixo mapa de ativações. E aí que começamos a enxergar a mágica da convolução, quando você considera só as ativações com a característica que importa. você consegue isolar essa característica do restante da imagem, aqui nós só está vendo as bordas verticais da imagem praticamente não fica visível a existência das bordas horizontais.\n",
        "\n",
        "Sequência de três imagens de um padrão de tijolos com definições de pixels e claridade diferentes. Todos possuem eixo horizontal graduado de zero a quinhentos a cada cem pontos, e eixo vertical graduado de quinhentos a zero a cada cem pontos.\n",
        "\n",
        "[05:46] Isso é muito relevante para você avaliar a existência de características individuais na sua imagem, para você conseguir fazer uma previsão baseada em partes também, se nós fizermos uma pequena troca e colocar agora o meu “Kernel” horizontal para operar nós vamos ver um resultado completamente diferente aqui, vamos lá.\n",
        "\n",
        "[06:10 Pronto, agora por procurei pelas bordas horizontais usando o meu segundo “Kernel” na horizontal e a bordas verticais são pouco visíveis, podemos discutir um pouco isso mais para frente mas quando existe alguma angulação na borda vertical ele pega levemente, como se existisse uma borda horizontal mas essencialmente onde as bordas verticais são mais retinhos aqui, ele praticamente não pega, ele pega mais as bordas horizontais.\n",
        "\n",
        "[06:41] Imagens completamente diferentes entre si que isolam características diferentes, agora eu deixei uma parte aqui já implementada, só para fazer uma observação sobre as ativações, que a questão de ser do claro para o escuro, do escuro para o claro.\n",
        "\n",
        "[06:57] Porque na prática tem uma nuance que nós podemos observar que o “Kernel” de borda eles vão detectar tanto bordas do claro para o escuro, quanto do escuro para o claro, só que ativando de formas diferentes, então eu baixei aqui uma imagem que a “logo’ da Alura.\n",
        "\n",
        "[07:33] Então eu vou procurar bordas verticais na logo da Alura, o que acontece aqui quando a vemos o mapa de ativação: nas bordas vão do claro para o escuro ele dá uma ativação alta branca a ativação é mais positiva. Na borda que faz do escuro para o claro, ele dá uma ativação muito baixa, é uma ativação negativa.\n",
        "\n",
        "[07:56] Significa que eu consigo visualizar as bordas, independente se elas vão do claro para o escuro, do escuro para o claro. Com a diferença de que elas vão dar ativações em extremos diferentes ou muito baixo ou muito alto, por isso é comum nós ver visualizações que dão o valor absoluto da saída para vermos só as bordas independente da direção delas, independente se o gradiente do claro para o escuro ou vice versa.\n",
        "\n",
        "[08:26] Se eu pego só o valor absoluto da ativação eu estou considerando o ponto onde a ativação foi mais forte, então aqui eu só estou pegando os lugares onde independente se foi do claro para escuro, do escuro para o claro, ele vai representar da mesma forma pela força da ativação.\n",
        "\n",
        "[08:42] Nós vamos ver só as bordas efetivamente Então é isso, no próximo vídeo veremos alguns filtros legais e explorar padrões mais complexos.\n",
        "\n",
        "Agora vamos ver alguns filtros convolucionais, os mais tradicionais dos manualmente projetados, eu acho importante vermos esses filtros manuais que foram definidos por uma pessoa, por que o que acabamos descobrindo na prática é que as redes neurais elas as primeiras camadas, pelo menos acabam convergindo para algo muito parecido com os filtros que foram manualmente projetados pelos engenheiros e profissionais de visão computacional do passado.\n",
        "\n",
        "[00:29] Então, só retomando rapidamente “kernels” que é o que vamos modelar aqui são Filtros Convolucionais que são arrays matrizes, no nosso caso operado com dado através da convolução.\n",
        "\n",
        "[00:45] E os pesos desse “Kernel” vão carregar o padrão que procuramos na imagem original. Esses filtros podem ser manualmente projetados e esse tipo de filtro que nós vamos ver aqui, os mais tradicionais.\n",
        "\n",
        "[00:55] Nós iremos ver também a intuição por parte do padrão que eles projetam que como eu falei, já que as redes convergem para esse mesmo tipo de filtro, são padrões que são relevantes para você realizar inferências. Então, estamos usando as mesmas bibliotecas de sempre matplot, skimage, “convolve” do scipy e o numpy.\n",
        "\n",
        "[01:20] E a nossa função show, de cada dia que vai nos ajudar a plotar as coisas de forma organizada, bonita, arrumada. Agora nós vamos usar imagem, ou até apagar isso aqui para mostrar para vocês, vamos usar imagem do Guilherme Silveira para fazer as nossas filtragens. Eu vou carregar aqui a imagem e mostrar uma questão para vocês, eu vou usar o input-output que é do pacote do skimage para ler a imagem: img = io.imread('GUI.jpg').\n",
        "\n",
        "[02:01] Eu vou imprimir aqui para vocês verem o shape dessa imagem, essa imagem colorida vou até plotar aqui para vocês verem, é uma imagem colorida do Guilherme da Alura, e só que nós não vimos ainda como “Kernels” funcionam quando a imagem tem canais de cor, não tem três dimensões, a dimensão da cor tem três canais.\n",
        "\n",
        "Imagem do output com fotografia do CEO da Plataforma Alura, Guilherme Silveira sorrindo e usando uma camiseta com o logotipo da Alura, abrangendo o tronco e a cabeça em visão frontal.\n",
        "\n",
        "[02:27] Então o que vamos fazer agora é trabalhar com essa imagem em escala de cinza, eu vou pegar só um canal, vou pegar por exemplo, o canal vermelho da imagem, peguei a dimensão zero, quer dizer na última dimensão peguei a posição 0.img = img [:, :, 0]e vou usar essa imagem em escala de cinza.plt. imshow(img, cmap 'gray').\n",
        "\n",
        "Mesma fotografia de Guilherme Silveira anterior, porém em preto e branco.\n",
        "\n",
        "[02:48] Agora vamos trabalhar com o Gui em preto e branco daqui para frente. Vamos lá, o que vamos fazer agora é explicar intuição por trás desses filtros que são manualmente projetados, para nós entendermos como que funciona o papel de um filtro numa rede.\n",
        "\n",
        "[03:03] O primeiro filtro é o filtro da média e ele traz um comentário interessante que às vezes você não quer só necessariamente realizar uma inferência em cima das ativações, porque nós vamos falar muito do resultado de uma convolução, como características features, você vai usar para fazer algum tipo de inferência, classificação, regressão.\n",
        "\n",
        "[03:25] Só que às vezes você está interessado no mapa de ativação como uma espécie de estilização da imagem por exemplo, filtro da média vai resultar numa versão suavizada, uma versão mais borrada da imagem.\n",
        "\n",
        "Filtro da Média\n",
        "Ao aplicar este filtro a uma imagem, o mapa de ativação resultante será uma versão suavizada da imagem original (mais \"borrada\", menos nítida). Este efeito é alcançado explorando a operação de convolução para tirar a média dos pixels de subregiões da imagem. Os pesos desse kernel são definidos para replicar a operação da média, ou seja, para um filtro 3×3 com 9 pesos, temos que:\n",
        "\n",
        "Fórmula do filtro da média em que o somatório de \"p\" de \"x\", em que \"x\" varia de \"1\" a nove, dividido por nove, sendo igual ao somatório de um sobre nove vezes \"p\" de \"x\" em que \"x\" varia de um a nove.\n",
        "\n",
        "[03:40] Então alguns filtros, por exemplo, filtros de Photoshop que transforma sua imagem numa imagem que parece ter sido feita a lápis, isso é um filtro que você pode passar nos pixels da imagem que vão estilizar a sua imagem, o filtro da média ele serve para os dois, tanto de estar com as características para você fazer inferência, como também pode servir para processamento desta imagem, para você estilizar imagem de alguma forma.\n",
        "\n",
        "[05:05] Basicamente com esse filtro funciona, ele vai tirar a média da imagem explorando a operação da convolução, como é que nós tiramos a média de pixels da imagem? Vamos somar os pixels e dividir pela quantidade de pixels, se pegarmos uma região da imagem que é 3 x 3, nós teremos que somar todas as posições e dividir por 9. Certo? Então eu vou somar “a1+ a1.2 + a3” e dividir pela quantidade que são 9.\n",
        "\n",
        "[04:46] Para fazer isso como uma convolução, o que é feito você propor um “Kernel” onde cada posição do Kernel vai ser o valor de 1 sobre 9 , no caso nosso “Kernel” é 3 x 3 tem 9 valores, quando você for operar esse “Kernel” aqui com a imagem, você vai operar 1/ 9 * a1.1 + 1/ 9 * a 1.2 que tem o mesmo efeito de você somar todos os elementos aqui vai +1/9 * a 33, tem o mesmo efeito de você somar todos os elementos e dividir por 9.\n",
        "\n",
        "[05:36 ] Então se você cria um filtro onde os valores elementos desse filtro são 1 dividido pela quantidade de pixels do filtro, está criando um filtro da Média. Vamos adiante, nós podemos definir isso de forma muito fácil, se eu crio por exemplo: “Kernel” média eu vou criar primeiro umarray vazio com a dimensão que eu quero, 3 x 3, certo? Kernel_media=np.zeros(((3, 3))\n",
        "\n",
        "[06:05] E agora, eu vou dizer que todas as posições desse array Kernem_media[:] = 1.0/(3**2), e todos os valores vão receber isso, e agora eu posso convoluir com a minha imagem.\n",
        "\n",
        "[06:30] Então se eu imprimo aqui o “Kernel” médio, ele é um array onde todos os elementos é igual 0.11. E se eu convoluir isso, com a minha imagem resultado = convolve (img, kernel_media)e eu vou fazer de novo aquela convolução no modo válido, aí agora usa minha função show para imprimir tanto o “Kernel” ela pede a imagem, o Kernel e o resultado para imprimir os três de uma vez, show(img, kernel_media, resultado, 'Kernel Média').\n",
        "\n",
        "[07:25] E aqui ele vai nos dar o resultado que é um mapa de ativação, uma imagem levemente borrada porque se um “Kernel” pequeno no 3 x 3, ou seja ele vai pegar regiões 3 x 3 na imagem tirar a média daquela região e o resultado vai ser o pixel no mapa de ativação, ele vai deslizar tirar a média de uma região 3 x 3 e daí por diante.\n",
        "\n",
        "Sequência de três imagens geradas no output: a primeira de título \"imagem\" contém uma fotografia de Guilherme Silveira sorrindo e usando uma camiseta com o logotipo da Alura em visão frontal do tronco para cima em preto e branco, a segunda chamada \"Kernel Média\" é um quadrado de fundo preto com diversos valores \"0 11\" alinhados dentro, e a terceira imagem é a mesma de Guilherme Silveira, mas com menor definição de pixels.\n",
        "\n",
        "[07:52] Se fizermos um “Kernel” maior 9 x 9, ele vai pegar a região 9 x 9 dá imagem, tirar média daquela região e aquela média vai ser o próximo Pixel Central. Isso vai resultar numa imagem muito mais borrada porque eu estou atuando agora numa região maior da minha imagem original.\n",
        "\n",
        "[08:17] E veja que eu posso alterar o tamanho do “Kernel” sem precisar me preocupar com tamanho da imagem porque eu altero o tamanho do “Kernel” e ele vai deslizar da forma como for possível, independentemente do tamanho da imagem.\n",
        "\n",
        "[08:30] Eu poderia diminuir, aumentar a imagem para o “Kernel” não faz diferença, por isso que falamos que o benefício da convolução, é que o tamanho do “Kernel” independe do tamanho da imagem, como eu aumentei esse “Kernel” o campo de visão dele em relação a imagem aumentou e ele está tirando a média de uma região um pouco maior, ou seja está borrando mais a imagem.\n",
        "\n",
        "[08:51] Bom, agora nós vamos ver o filtro de Sobel que se parece muito com filtro de bordas que nós já fizemos, só que dessa vez ele tem um toque diferente que vai pegar as bordas de uma forma um pouco mais organizada, nós vamos ver o resultado aqui.\n",
        "\n",
        "[09:08] Eu vou aproveitar essa oportunidade para montar o Kernel de uma forma diferente, para vermos como que o NumPy pode ajudar, então se eu fosse replicar exatamente o “Kernel” vertical que nós fizemos, eu posso criar aqui um array NumPy preenchido com 0 de tamanho 3 x 3 que o tamanho do nosso “Kernel”.\n",
        "\n",
        "[09:28] Na primeira coluna, como queremos um Kernel vertical, eu vou dizer que todas as linhas da primeira coluna eu quero que tenha -1 e todas as linhas da última coluna eu quero que tenha o valor 1.\n",
        "\n",
        "[09:46] Se eu imprimir esse print(sobel_v) eu tenho aqui exatamente aquele mesmo filtro que nós modelamos, só que o “sobel” também ele vai pegar uma leve ocorrência de bordas horizontais, na linha 1, na primeira posição e na última posição, tenha valores mais extremos.\n",
        "\n",
        "[10:10] Então ele vai pegar também pequenas transições de cor nas linhas, nós vamos ver isso como funciona, eu posso até copiar isso aqui que nós fizemos aqui em cima, só para poupar um tempo que aqui já é a convolução e o plot do resultado. Legal, só que em vez de ser o Kernel da média é o “Sobel v”.\n",
        "\n",
        "[10:38] Beleza, então ele pega não só as bordas verticais como ele também pega um pouquinho de borda horizontal com essas transições que ele faz aqui, eu posso imprimir aqui como eu falei para vocês, o valor absoluto do resultado para nós vermos as bordas Independentes da direção de onde vai o gradiente de cor, já que ele está pegando todas as bordas com destaque para as bordas verticais.\n",
        "\n",
        "[11:14] Se nós fizermos agora a versão horizontal disso, aí vai ser em vez de mudar a linha, a coluna, vamos mudar a linha e em vez de mudar a linha 1 mudamos agora a coluna 1, ficamos antes de plotar o resultado, ficamos com um “sobel” horizontal que é exatamente a mesma coisa rotacionada, em vez de ser nas colunas, agora é nas linhas.\n",
        "\n",
        "[11:50] Legal, então o resultado disso aqui desse “Sobel” horizontal ele vai destacar agora muito mais as bordas horizontais, mas também pegando um pouquinho de borda vertical por causa dessa transição que ele coloca aqui também, então se nós plotarmos o resultado dos dois, vamos ver que os dois “Kernels” eles pegam um pouquinho das duas bordas, só que o “sobel” vertical destaca bordas verticais e “sobel” a horizontal destaca bordas horizontais.\n",
        "\n",
        "Sequência de seis imagens distribuídas em duas linhas geradas no output: na primeira linha, a primeira de título \"imagem\" contém uma fotografia de Guilherme Silveira sorrindo e usando uma camiseta com o logotipo da Alura em visão frontal do tronco para cima em preto e branco, a segunda chamada \"Sobel Vertical\" é um quadrado composto de nove quadrados internos em tons de cinza com valores dentro, em que os maiores valores se aproximam da cor branca e os menores da cor preta, considerando que a coluna central na posição vertical possui os três valores \"0.0\" em tom de cinza médio, a terceira imagem é chamada \"Mapa de Ativação\" e possui a mesma fotografia apenas com fundo preto e definição de algumas linhas principais no sentido vertical da figura em branco. Na linha baixo, a mesma fotografia \"Imagem\" porém um pouco mais clara, a imagem seguinte chamada \"Sobel Horizontal\" é um quadrado composto de nove quadrados internos em tons de cinza com valores dentro, em que os maiores valores se aproximam da cor branca e os menores da cor preta, considerando que a linha central na posição horizontal possui os três valores \"0.0\" em tom de cinza médio, e por fim a imagem seguinte chamada \"Mapa de Ativação\" também é uma fotografia de Guilherme Silveira em fundo preto com definição de algumas poucas linhas principais na posição horizontal em branco.\n",
        "\n",
        "[12:31] Passando adiante, vamos conhecer agora um último “Kernel” de “Laplace” super simples e a super vantajoso também, porque o Laplace eu posso colocar aqui umarray preenchido com números, uns nós explicamos, uma vez que nós vejamos como ele era.\n",
        "\n",
        "[12:53] Então vou criar aqui: laplace = np.ones((3,3)) * -1 preenchido com valores um dessa vez não mas com zero, só que eu vou multiplicar por -1 por que o “sobel” ele é preenchido com valores -1 em todas as posições exceto na posição central. Então na linha 1 coluna 1 ele tem um valor mais positivo, muito mais positivo.\n",
        "\n",
        "[13:17] Então se nós imprimirmos o “Laplace” ele tem essa carinha aqui, ele é cercado por valores negativos muito baixos exceto pelo Pixel Central que é muito alto, então ele ao mesmo tempo vai pegar bordas verticais e bordas horizontais, também ele pega e consegue pegar os dois muito bem com a mesma força e intensidade.\n",
        "\n",
        "[13:40] Se nós fizermos a convolução, imprimir o resultado do “Laplace”, troca aqui pelo laplace, ele vai pegar as duas bordas com a mesma intensidade, você vê que é pouco menos destacado que o “sobel”, porque o “sobel ele é especializado numa direção de borda, no caso do “Laplace” ele pega todas as bordas horizontais e verticais igualmente.\n",
        "\n",
        "Sequência horizontal de três imagens do output, sendo a primeira uma fotografia em visão frontal de Guilherme Silveira sorrindo e em preto e branco, a imagem seguinte é um quadrado com nove quadrados internos, em que apenas o quadrado central possui o valor \"8.00\" e fundo branco, e todos os outros possuem valor \"-1.00\" e fundo preto, e a terceira imagem é a mesma fotografia de Guilherme Silveira, mas com fundo preto e algumas linhas realçadas em branco.\n",
        "\n",
        "[14:14] Se eu tirar esse absoluto aqui talvez dê para nós enxergarmos melhor, ele pega as duas bordas igualmente, tanto vertical como horizontal e apresenta elas consegue quase que ver uma versão suave, uma versão só de bordas da imagem, como o nosso Kernel é pequeno conseguimos ver até bordas mais finas, como olho do Guilherme, sorriso a covinha, se nós fizéssemos um “Kernel” um pouco maior não seria tão evidente assim.\n",
        "\n",
        "Sequência horizontal de três imagens do output, sendo a primeira uma fotografia em visão frontal de Guilherme Silveira sorrindo e em preto e branco, a imagem seguinte é um quadrado com nove quadrados internos, em que apenas o quadrado central possui o valor \"8.00\" e fundo branco, e todos os outros possuem valor \"-1.00\" e fundo preto, e a terceira imagem é a mesma fotografia de Guilherme Silveira, mas com fundo cinza e algumas áreas principais da figura realçadas um pouco mais clara e escura, como o cabelo e o logotipo da \"Alura\" na camiseta.\n",
        "\n",
        "[14:52] Esses são os “Kernels” manualmente projetados que vamos conhecer e no último vídeo dessa aula nós vamos ver convoluções com padrões complexos para estabelecer o que é a operação de convolução está buscando.\n",
        "\n",
        "\n",
        "\n",
        "Filtros Low Level em Redes Neurais Convolucionais\n",
        "Antes da popularização de Redes Neurais Convolucionais (CNN), o Reconhecimento de Padrões e o Processamento de Imagens eram realizados a partir de filtros manualmente projetados. Profissionais da área de Visão Computacional em geral concentravam seus esforços em compreender os padrões visuais nas imagens de interesse, ou até estudar o funcionamento do sistema visual biológico e replicá-lo de forma simplificada ao projetar kernels convolucionais.\n",
        "\n",
        "Um dos conjuntos de filtros que ganhou maior popularidade por ser aplicável a diferentes domínios são os filtros Gabor. Projetados para análise de textura, esses filtros buscam padrões baixo nível como as bordas que vimos em aula, variando em orientação e na frequência do sinal. A imagem a seguir mostra alguns filtros retirados de um banco Gabor para ilustrar a sua variação.\n",
        "\n",
        "\n",
        "\n",
        "A razão de estarmos falando desses filtros em um curso de Redes Neurais Convolucionais é a sua estreita relação entre ambos (Gabor e CNN). Enquanto o Gabor foi fruto da expertise de profissionais (humanos) de Reconhecimento de Padrões em imagens, filtros aprendidos por CNN são resultado de um processo automático baseado em dados. Mesmo tendo origens diferentes, veremos a seguir a sua forte intersecção.\n",
        "\n",
        "No curso de Redes Neurais Convolucionais publicamente disponível pela Universidade de Stanford, existe uma aula voltada para intepretar o que uma CNN aprende. Ao treinar uma AlexNet (arquitetura de CNN) para reconhecer objetos, os filtros da sua primeira camada foram visualizados nessa aula, e estão apresentados na imagem a seguir.\n",
        "\n",
        "\n",
        "\n",
        "Parece familiar? Podemos ver nesse conjunto de filtros inúmeras instâncias que parecem ter sido retiradas de um banco Gabor! Esta semelhança nos permite contemplar o trabalho de profissionais que definiram manualmente kernels convolucionais, sem saber que futuramente métodos tão poderosos como redes convolucionais seguiriam o mesmo caminho.\n",
        "\n",
        "Vale lembrar que o aprendizado de uma CNN é hierárquico, portanto o baixo nível de uma CNN é apenas o primeiro passo do aprendizado de características. As características extraídas por esses filtros irão alimentar uma hierarquia de filtros cada vez mais semânticos.\n",
        "\n",
        "Olá pessoal, pra que nós não fique falando só de bordas horizontais, verticais bordas para todos os lados, vamos ver também como que a convolução funciona para fazer casamento de padrões complexo, também porque uma coisa que vimos na introdução e vamos ver bastante mais para frente, é como que as redes Profundas elas criam filtros com hierarquias, os iniciais são mais simples e os filtros finais mais complexos vão procurar padrões mais complexos.\n",
        "\n",
        "[00:31] É aquele exemplo que nós demos que se os primeiras camadas pegariam bordas e círculos e essas coisas assim, camadas finais estariam procurando, por exemplo cabeças de cachorro, colmeia de abelha, padrões muito mais complexos. E como é que funciona, ele realmente faz um casamento de padrões complexos ao longo da imagem, eu vou baixar aqui a imagem de um avião, para nós fazermos um casamento de padrões com um patche dessa imagem.\n",
        "\n",
        "[01:05] Primeiro vou imprimir a imagem e escolher uma região específica da turbina que eu já selecionei aqui, então eu vou definir aqui que a minha imagem vai ser essa imagem que chama plane.jpg, eu vou carregar aqui na imagem com o “input-output” do “sikmage” eu vou ler a imagem que chama plane.jpg já é uma imagem, aqui eu deixei a função de impressão já pronta, é só para não dar trabalho ela é colorida, então vou fazer a mesma coisa para pegar só um canal dela. im = io.imread(plane.jpg) img = img[:, :, 0].\n",
        "\n",
        "[01:51 ] Imagem todas as linhas todas as colunas canal zero e eu vou considerar que ela só tem um canal de cor, daqui para frente o retângulo que eu vou selecionar eu já deixa eu imprimir a imagem primeiro, eu defini lá em cima as coordenadas do retângulo que eu quero selecionar. Primeiro eu vou imprimir a imagem que eu chamei de “img”, esse aqui é o nosso avião então eu selecionei uma região desse avião que foi a turbina dele para fazer um casamento de padrões complexos ao longo da imagem.\n",
        "\n",
        "Imagem do output em preto e branco de um avião estacionado em um aeroporto em visão frontal na mesma altura do topo do avião. Há dois funcionários próximos e algumas outras aeronaves estacionadas ao fundo.\n",
        "\n",
        "[02:27] Eu tenho que fazer algumas alterações que nós vamos ver aqui porque, então vamos lá, o retângulo ele é definido pela região onde está contida a turbina e esse vai ser o meu “Kernel”. Então está mais ou menos aqui na posição 109 no y e mais ou menos na posição 250 aqui no x, então meu retângulo vai ser definido por essas coordenadas e agora eu consigo imprimir o patch correspondente ao retângulo que é aqui a turbina.\n",
        "\n",
        "[03:00] O que eu vou ter agora uma imagem que a imagem do avião no aeroporto e um patch que é a imagem da turbina separado, eu vou convoluir esse patch esse “Kernel” com a minha imagem nós vamos ver o que acontece, só que eu não posso usar o patch na sua forma original como “Kernel” eu tenho que transformar ele numa distribuição pouco mais controlada.\n",
        "\n",
        "[03:25] Não é à toa que nós temos feito “Kernel” com valores 1, - 1 valores que são mais controlados, por que não fiz com 27 e -27 ou 27 e 3, sei lá? Porque o interessante é que você tenha distribuições mais controladas na hora de modelar o “Kernel” então o que eu vou fazer? Tanto é que se eu não fizesse esses passos a convolução não daria um resultado tão claramente bem casado.\n",
        "\n",
        "[03:50] Eu vou pegar esse meu retângulo, esse meu patch correspondente ao retângulo, vou subtrair pela média dos pixels, ou seja estou centralizando a distribuição e eu vou também inverter esse patch, por quê? Porque na hora que a operação da convolução acontecer ele vai ser invertido tudo novamente e ele vai voltar para o seu padrão original.\n",
        "\n",
        "[04:16] Então que eu estou fazendo aqui ao inverter esse patch eu estou fazendo que na hora que a convolução aconteça procure exatamente o padrão do *patch que é a turbina.\n",
        "\n",
        "[04:28] Se eu imprimi aqui o patch definir como img o nome da imagem, se imprimir aqui o patch em escala de cinza, esse é o padrão que estamos procurando é a turbina com a distribuição centralizada, esse passo aqui é essencial, eu preciso de uma distribuição controlada e rotacionado.\n",
        "\n",
        "Imagem do output em preto e branco e pixelizada da turbina do avião, com o eixo horizontal graduado de zero a vinte a cada cinco pontos, e eixo vertical graduado de dezessete ponto cinto a zero ponto zero a cada dois pontos e meio.\n",
        "\n",
        "[04:50] Então vamos convoluir aqui agora a nossa imagem: resultado = convolve(img, patch, mode ‘valid’) imagem com o “Kernel” que nesse caso agora é o patch do próprio avião é a turbina do próprio avião e eu vou usar o modo que sempre temos feito é o modo válido, vamos ver como é que fica, eu vou usar o modo same para ele manter a dimensionalidade da Imagem e nas próximas aulas entramos nesse nuance.\n",
        "\n",
        "[05:24] O que vai acontecer aqui é que no ponto deixa eu aumentar a imagem, no ponto exato onde a turbina do avião está o casamento é perfeito, é o ponto de maior intensidade no mapa de ativação, então eu posso sim ter um padrão mais complexo, não precisa ser uma borda, não precisa ser um padrão simples que esse padrão complexo vai ser buscado na nossa imagens.\n",
        "\n",
        "Imagem do output em tons de cinza do avião estacionado descrito anteriormente. Há um ponto branco em realce na turbina do avião com um quadrado de linhas vermelhas realçando-o.\n",
        "\n",
        "[05:48] O problema é que eu sei que o casamento perfeito, eu peguei um pedaço da própria imagem para fazer convolução, no caso das redes convolucionais, para que eu consiga identificar padrões complexos, antes de procurar aquele padrão eu preciso simplificar a imagem negra no processo, eu não posso pegar a imagem original e já buscar padrões complexos nela porque seria muito sensível a variações na imagem.\n",
        "\n",
        "[06:17] Então eu extraio características mais simples, vou com todas as características mais simples ver que para construir um modelo daquela imagem, em vez de eu pegar uma imagem real cheia de nuances, ruídos e etc., aos poucos eu vou construindo uma versão simplificada daquela imagem modelo para em cima desse modelo eu consegui procurar padrões mais complexos.\n",
        "\n",
        "Na seção “Convolução e Padrões Complexos” no script Filtros.ipynb selecionamos uma região da imagem para convoluir com a própria imagem. Dessa forma garantimos que em pelo menos um ponto da imagem haveria um casamento perfeito entre o dado (imagem) e o kernel (região), definindo um ponto de ativação máxima na saída.\n",
        "\n",
        "Selecione uma diferente região da imagem para ser utilizada como kernel de convolução. A alteração deve ser feita na variável boundaries, que atualmente possui os seguintes valores referentes à turbina na imagem:\n",
        "\n",
        "boundaries = [109, 129, 255, 275]COPIAR CÓDIGO\n",
        "Como sugestão, você pode selecionar a pessoa à frente do avião, sinalizando para o seu pouso.\n",
        "\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "A janela que contém a região proposta no enunciado pode ser definida por:\n",
        "\n",
        "boundaries = [180, 230, 195, 215]COPIAR CÓDIGO\n",
        "Ao executar o restante do código, você pode perceber que o pixel localizado exatamente no centro da região selecionada será o ponto de ativação máxima na imagem resultante. Isso indica que a convolução não irá ativar toda a região onde está contido o padrão (a turbina ou a pessoa), apenas a posição central dessa região, onde o casamento entre dado e kernel é maximizado.\n",
        "\n",
        "Operação da Convolução 2D\n",
        "Kernels 2D\n",
        "Exemplos de filtros manualmente projetados\n",
        "Casamento de padrões complexos\n",
        "\n",
        "Agora falaremos, um pouco das camadas de uma rede convolucional, então o primeiro passo para começarmos a construir efetivamente uma rede, e a camada principal que dá nome a rede neural convolucional, é a camada convolucional que é a primeira que veremos aqui.\n",
        "\n",
        "[00:21] As camadas mais comuns de uma \"CNN\" como já falamos um pouco, são camadas que alternam entre transformação e sub-amostragem, então quando comentamos sobre transformação, falamos da camada convolucional como a camada de transformação e a camada de \"Pooling\" como a camada de sub-amostragem.\n",
        "\n",
        "[00:47] Então além dessas duas camadas aqui, nesse curso a também vamos conhecer a camada de \"Batch Normalization\" que é uma operação super simples, é uma operação de normalização das características ao longo da rede que melhora e muito o treinamento.\n",
        "\n",
        "Imagem com título \"Camadas\". Abaixo e em sequência horizontal, há um quadrado chamado \"Entrada\" com uma pequena área quadrada projetada para outro quadrado chamado \"conv1\" sobreposto a uma sequência de outros cinco quadrados, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos iguais chamados \"Pool 1\". Em seguida, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\".\n",
        "\n",
        "Camadas comuns em CNN\n",
        "Convolucionais\n",
        "Pooling\n",
        "Batch Normalization\n",
        "Totalmente Conectadas\n",
        "[01:05] Então são as três camadas principais, quando você vai construir uma rede convolucional, além das camadas totalmente conectadas que são as camadas de \"MLP\" Multilayer Perceptron, essas camadas geralmente ficam no final aqui da rede e são as camadas que vão fazer a inferência, classificação ou regressão a depender do seu problema.\n",
        "\n",
        "[01:27] Podemos pensar na construção da arquitetura de uma rede neural como um aprendizado de características, então nessa etapa eu vou representar a minha imagem, vou transformar minha entrada num conjunto de características de alto nível e a partir desse ponto o alimentar as características para camadas totalmente conectadas que vão ser o meu modelo de inferência, então aqui aprendizado de características de um lado e classificação ou regressão do outro lado aqui da rede.\n",
        "\n",
        "[02:02] Então não falaremos de \"MLP\" porque tem um curso inteiro na Alura, só sobre \"MLP\", mas vamos falar dessas três camadas que são específicas das redes convolucionais.\n",
        "\n",
        "[02:15] Rapidamente entendermos sobre dimensionalidade porque quando eu falo de redes convolucionais, eu bato muito na tecla das dimensões das características ao longo da rede, porque se não ficar muito claro, a construção de uma rede ou até pequenas alterações numa rede convolucional fica inviável.\n",
        "\n",
        "[02:34] Então mesmo que você vá usar uma rede pronta, se você precisar fazer uma pequena alteração, você precisa conhecer a dimensionalidade das características que passam por toda a rede.\n",
        "\n",
        "[02:46] Então quando falamos aqui de imagens com um canal comentamos que dado uma imagem de um canal que tem duas dimensões, altura e largura, vamos ter um \"kernel\" também com canal, altura e largura, que vai gerar um resultado, mapa de características, que vai perder dimensão em relação a entrada.\n",
        "\n",
        "[03:09] Então se eu opero esse \"kernel\" 3 por 3 ao longo da minha imagem como eu não consigo operar com os pixels da borda, eu vou perder dois pixels em cada dimensão no meu resultado.\n",
        "\n",
        "Imagem com título \"Camada convolucional\" seguido do texto \"convoluções com imagens de 1 canal (preta e branca)”. Abaixo e em sequência horizontal, há uma matriz de seis por seis, multiplicada por outra matriz de três por três, resultando em uma matriz de quatro por quatro.\n",
        "\n",
        "[03:23] Quando falamos de convoluções com imagens coloridas RGB, estamos adicionando uma terceira dimensão, mas isso aqui é extremamente importante de entender, mesmo agora tendo um dado em três dimensões, que são altura, largura e profundidade, ainda assim estaremos usando a convolução \"2D\" que só se desloca em duas dimensões.\n",
        "\n",
        "[03:48] Só que como agora temos uma terceira dimensão que são os canais da imagem, o nosso \"kernel\" também vai ter três dimensões.\n",
        "\n",
        "[03:57] Então dessa forma o que a faremos aqui em vez de deslizarmos um \"kernel\" \"2D\" na imagem, iremos deslizar esse cubo aqui que é o \"kernel\" com três dimensões, então ainda é uma convolução \"2D\", por quê a convolução \"2D\", significa que o \"kernel\" só tem dois graus de liberdade que é altura, então esse grau de liberdade aqui, e a largura que é esse grau de liberdade aqui.\n",
        "\n",
        "Imagem com título \"Camada convolucional\" seguido do texto \"convoluções com imagens de 3 canais (coloridas)\". Abaixo e em sequência horizontal, há uma sobreposição de três matrizes de seis por seis em três cores distintas, multiplicada por outras três matrizes sobrepostas de três por três, resultando em uma matriz de quatro por quatro.\n",
        "\n",
        "[04:27] Então o \"kernel\" só vai se deslocar nessas duas dimensões, mas vai fazer operações com cada um dos canais, então já aprendemos que faremos uma multiplicação ponto a ponto, com os dados que estão aqui na nossa imagem e com essa multiplicação ponto a ponto ao final iremos somar os valores do resultado.\n",
        "\n",
        "[04:50] Agora a única coisa que muda é também faremos multiplicações ponto a ponto nos três canais, cada canal do \"kernel\" vai atuar de forma independente em cada canal da imagem, e no final somaremos também nessa dimensão.\n",
        "\n",
        "[05:07] Então basicamente, iremos operar com a dimensão da altura, multiplicar ponto a ponto e somar, vai operar na largura multiplicar ponto a ponto e somar e também vai somar os valores na profundidade, então é só multiplicar ponto a ponto cada canal somar tudo no final e a multiplicação desse cubo com essa região na profundidade vai gerar o primeiro pixel resultado.\n",
        "\n",
        "[05:45] Independente da imagem se é preto e branco ou colorida, o que mudará é a dimensionalidade do \"kernel\", que vai ou não ter uma quantidade de canais, uma profundidade, porém isso aqui é muito importante, o resultado vai gerar um mapa de ativação com um único canal, então para cada filtro convolucional para cada \"kernel\" que eu tiver na minha camada eu vou gerar um resultado, um mapa de ativação com um único canal.\n",
        "\n",
        "[06:17] Então se eu tenho uma imagem de entrada com 3 canais e eu opero um \"kernel\" convolucional, a minha saída é um mapa de ativação com um único canal, isso é extremamente importante.\n",
        "\n",
        "[06:32] Só para fazermos um paralelo com \"MLP\", o multilayer perceptron, quando defino uma arquitetura de \"MLP\", a pergunta que eu quero responder é quantos neurônios eu quero em cada camada, então aqui na primeira camada eu tenho meu dado, então meu dado tem três dimensões e mais um viés, quantos neurônios eu quero na camada seguinte, aqui tem três neurônios mais um viés, e na última camada tem um neurônio.\n",
        "\n",
        "Cada kernel na camada convolucional gera um único mapa de ativação.\n",
        "Ao definir a arquitetura de um MLP, respondemos a pergunta:\n",
        "Quantos neurônios queremos em cada camada?\n",
        "\n",
        "Imagem com uma sequência horizontal de quatro círculos representando o \"Layer L1\", os quais contém \"x\" no índice \"1\", \"x\" no índice \"2\", \"x\" no índice \"3\" e \"+1\" de cima para baixo. Estes estão interligados a três círculos em sequência vertical ao lado representando o \"Layer L2\", e há um quarto que não está interligado aos quatro primeiros citados e possui o valor \"+1\". Em seguida, cada um destes últimos quatro círculos da segunda sequência vertical se ligam a um único círculo representando o \"Layer L3\", e estas ligações possuem as respectivas legendas \"a\" elevado a dois entre parênteses no índice \"1\", \"a\" elevado a dois entre parênteses no índice dois, \"a\" elevado a dois no índice \"3\" e a última linha não possui legenda. Este último círculo que se liga aos quatro anteriores também finaliza com uma seta apontando para a direita com a legenda \"h\" da localização \"w,b\" em função de \"x\".\n",
        "\n",
        "[06:58] Quando definimos uma rede convolucional, a pergunta que queremos responder, é quantos filtros convolucionais eu quero em cada camada, então em vez de pensar em quantos neurônios, eu quero eu vou pensar em quantos \"kernels\" eu quero.\n",
        "\n",
        "[07:15] É a mesma pergunta, só que em vez de fazer uma operação que vai contar com um peso de um neurônio, eu vou fazer uma outra operação que vai contar com \"kernel\" convolucional, basicamente a mesma pergunta, só que são operações diferentes então muda um pouco.\n",
        "\n",
        "[07:36] Então, quantos filtros convolucionais eu quero em cada camada? Se eu defino uma camada e agora já estamos começando a chegar em termos de como que a definiríamos no \"PyTorch\" e tudo mais, mas primeiro vamos ver de forma abstrata, se eu defino uma camada, ainda não aprendemos a definir camadas, mas vamos supor que eu defini uma camada com 4 filtros convolucionais, significa que eu quero aprender quatro características sobre a minha entrada.\n",
        "\n",
        "[08:04] Então eu posso dizer que por exemplo esse primeiro conjunto, esse primeiro filtro aprende bordas verticais, o segundo aprende bordas horizontais, o terceiro aprende bordas diagonais e daí por diante.\n",
        "\n",
        "[08:16] Então, cada filtro desses vai ser especializado em uma característica, vai procurar uma característica aqui na minha imagem e vai gerar um mapa de ativação de um único canal, então para cada filtro convolucional que eu definir aqui na minha camada, essa aqui no meio é a minha camada convolucional efetivamente, eu vou gerar um canal de saída aqui.\n",
        "\n",
        "Ao definir a arquitetura convolucional, respondemos a pergunta:\n",
        "Quantos filtros convolucionais queremos em cada camada?\n",
        "\n",
        "Imagem contendo uma operação com três matrizes seis por seis sobrepostas, as quais multiplicam uma sequência de quatro conjuntos de cores diferentes formados de três matrizes de ordem três por três sobrepostas. Esta multiplicação resulta em quatro matrizes de ordem quatro por quatro e de cores diferentes sobrepostas.\n",
        "\n",
        "[08:42] Então se eu tenho uma entrada L x L x 3, definir quatro neurônios aqui na minha camada intermediária, a minha saída vai ter como profundidade o número de canais, o número de filtros que tinha na minha camada.\n",
        "\n",
        "[09:09] Então se eu tinha quatro filtros, a minha saída vai ter quatro canais então lembrando se eu tenho uma imagem 6 por 6 aqui, e eu opero com \"kernels\" que são 3 por 3 a minha saída vai ser 4 por 4, se eu estou operando dessa vez 4 filtros 3 por 3, a minha saída vai ser 4 por 4 por 4 aqui na profundidade também.\n",
        "\n",
        "A saída tem dimensão A x L x N, sendo N o número de kernels da camada. Ou seja, N mapas de ativação.\n",
        "Imagem contendo uma operação com três matrizes seis por seis sobrepostas, as quais multiplicam uma sequência de quatro conjuntos de cores diferentes formados de três matrizes de ordem três por três sobrepostas. Nestes conjuntos, há demarcação de cada cor com uma numeração de um a três. Esta multiplicação resulta em quatro matrizes de ordem quatro por quatro e de cores diferentes sobrepostas, as quais possuem a legenda \"4 x 4 x 4\" e \"A x L x N\".\n",
        "\n",
        "[09:40] Então vou ter três dimensões de saída, a altura, a largura e a quantidade de mapas de ativação, ou seja o número de canais da saída.\n",
        "\n",
        "[09:56] A representação de arquiteturas convolucionais que vemos apresenta os mapas de ativação resultante de cada operação, então o que estamos vendo aqui é a saída de cada camada, então isso aqui é a saída de uma transformação convolucional, isso aqui é a saída de uma operação de \"Pooling\", aqui tenho outra transformação convolucional, dessa vez uma convolução com 12 filtros.\n",
        "\n",
        "[10:23] Aqui era uma convolução com 6 filtros, aqui é uma convolução com 12 filtros, então vou pegar essa entrada aqui, vou transformá-la através de uma convolução com 12 filtros e cada um dos meus filtros vai gerar um dos mapas de ativação aqui da saída.\n",
        "\n",
        "[10:41] Vou fazer mais um \"Pooling\", vou fazer mais uma convolução com 18 filtros, então aqui eu já estou definindo a minha rede praticamente, é isso que faremos no \"PyTorch\", definiremos uma sequência de convoluções e \"Poolings\", aqui eu defini uma convolução com 18 filtros, então estou aprendendo 18 características diferentes sobre a minha entrada e assim por diante.\n",
        "\n",
        "As representações de arquiteturas convolucionais apresentam os mapas de ativação resultantes de cada operação.\n",
        "Imagem com um quadrado chamado \"Entrada\" com uma pequena área quadrada projetada para outro quadrado chamado \"conv1\" sobreposto a uma sequência de outros e com a legenda \"A x L x 6\" abaixo, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos chamados \"Pool 1\". Em seguida, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" que possui a legenda \"A' x L' x 12\" abaixo, e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" que possui a legenda \"A'' x L'' x 18\", e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\".\n",
        "\n",
        "[11:08] No final vou ter aqui mais um \"Pooling\", então vou ter um resultado que é a altura transformada, largura transformada, por 18, por que o \"Pooling\" não vai alterar a profundidade, veremos isso em mais detalhes.\n",
        "\n",
        "[11:24] Então aqui, basicamente, se eu colocar os nomes corretos, eu já estou definindo a minha arquitetura, é isso, e essa representação vemos são as saídas de cada camada então no próximo vídeo, veremos um pouco mais de detalhes sobre a definição da sua camada no \"PyTorch\" e depois iremos codificar de fato.\n",
        "\n",
        "Agora estamos quase chegando na hora de implementar uma camada convolucional, mas antes vamos conhecer rapidamente os parâmetros dessa camada, então para definir no \"PyTorch\", usaremos o pacote \"nn\", que é um pacote de redes neurais e simplesmente atribuir o resultado do objeto nn.Conv2d a uma variável e essa vai ser a variável que usaremos para realizar as operações de convolução.\n",
        "\n",
        "Conv2d no Pytorch\n",
        "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)COPIAR CÓDIGO\n",
        "Pacote nn (neural networks)\n",
        "[00:26] Então dos cinco parâmetros que essa camada recebe, tem dois conjuntos diferentes de parâmetros que são muito importantes de entendermos, o primeiro conjunto de parâmetros é o in_channels, out_channels, que é quantidade de canais da entrada, e quantidade de canais da saída, que foi o que ficamos falando bastante no vídeo anterior.\n",
        "\n",
        "[00:46] Então para definir o tamanho da entrada e o tamanho da saída, como a convolução não se importa com a altura e a largura da entrada, porque eu tenho um \"kernel\" que vai deslizar na imagem independentemente do tamanho dessa imagem.\n",
        "\n",
        "[01:04] Então eu não preciso perguntar qual que é a altura e a largura da imagem, eu só preciso saber qual o número de canais dessa minha entrada para que eu possa definir um filtro com o mesmo número de canais, que é aquele cubo que nós definimos para operar com imagens coloridas.\n",
        "\n",
        "[01:21] Então para primeira camada eu vou dizer que o in_channels é o número de canais da imagem, se for preto e branco é 1, se for colorido é 3 e o out_channels é o número de mapas de ativação que eu posso também dizer que é o número de filtros, porque se eu quero definir uma camada e eu quero aprender 16 características sobre a minha imagem, eu vou definir 16 filtros, que por sua vez vão gerar 16 mapas de ativação na saída.\n",
        "\n",
        "[01:56] Então se eu vou definir a camada conv2 aqui, que ela vai receber a entrada do \"Pooling 1\", então se eu for definir essa camada, eu tenho aqui 6 canais, 6 mapas de ativação e vai ter como saída 12 mapas de ativação, ou seja eu quero 12 filtros nessa minha camada convolucional, e esses 12 filtros vão operar com entrada que possui 6 canais, aqui eu estou definindo o número de filtros, e aqui eu estou definindo quantos canais vai ter a minha entrada.\n",
        "\n",
        "Conv2d no Pytorch\n",
        "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)COPIAR CÓDIGO\n",
        "Para as camadas seguintes:\n",
        "\n",
        "Profundidade da entrada (ativações da camada i)\n",
        "Profundidade da saída (ativações da camada i+1)\n",
        "Imagem com sequência horizontal, há um quadrado chamado \"Entrada\" com uma pequena área quadrada projetada para outro quadrado chamado \"conv1\" sobreposto a uma sequência de outros cinco quadrados, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos iguais chamados \"Pool 1\". Em seguida, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\".\n",
        "\n",
        "[02:40] Mas basicamente você pode pensar nisso como tamanho da entrada, tamanho da saída. Então basicamente é isso, aqui a entrada é 12, a saída é 18, aqui a entrada é 6, a saída é 12 e assim por diante, aqui a entrada é 1 e a saída é 6.\n",
        "\n",
        "[02:58] O outro conjunto de parâmetros extremamente importante, são os parâmetros referentes ao comportamento do filtro do \"kernel\" convolucional. Então eu tenho o \"Field of View\", que é o campo de visão do meu filtro, que é definido no parâmetro kernel_size, então vimos bastante, filtro 3 por 3, que é o tamanho do \"kernel\", o tamanho do nosso filtro.\n",
        "\n",
        "[03:20] Temos o \"Stride\", que é o tamanho do passo ao deslizar o filtro, então se eu vou de 1 em 1 pixel, se eu vou de 2 em 2 pixels com a minha janela deslizante e o \"Padding\", que é o preenchimento com zeros, isso aqui é importante porque comentamos que não dá para operar com as bordas de uma imagem, porque eu preciso centralizar o \"kernel\" no pixel que eu vou operar.\n",
        "\n",
        "[03:44] E para isso eu preciso que a vizinhança daquele pixel possa ser operada com meu \"kernel\", se o pixel não tem uma vizinhança completa, eu não consigo operar, por isso colocamos o \"Padding\", colocamos zero nas bordas, para que consigamos operar com as bordas reais da imagem levando em consideração as bordas iguais a zero para representar a vizinhança.\n",
        "\n",
        "[04:07] Então aqui o campo de visão mais comum que vemos é o \"3x3\", que vai pegar as regiões de três pixels por três *pixels, mas eu posso definir também um \"kernel\" de \"5x5\", que a cada deslocamento dele, vai pegar uma região de \"5x5\" da minha imagem.\n",
        "\n",
        "[04:27] Isso permite que eu pegue padrões mais complexos, por exemplo bordas de alta frequência, padrões que exijam um tamanho de \"kernel\" maior, isso também permite que eu pegue regiões maiores da minha imagem, eu consiga reconhecer padrões mais complexos que geralmente ocupam mais espaço na imagem.\n",
        "\n",
        "[04:49] O \"Stride\", ele é o pulo que eu vou dar na hora de deslizar janela, isso tudo interfere com a resolução, a dimensionalidade do mapa de ativação, porque se eu tenho um \"Stride\" de um de um pixel por um pixel, o que eu vou ter aqui é: a medida que eu deslizar a janela eu vou operar com um pixel diretamente vizinho, então se eu opero com esse pixel aqui primeiro, quando eu deslizar esse \"Stride\", eu vou estar operando com o pixel diretamente na vizinhança está vendo.\n",
        "\n",
        "Imagem de título \"Stride\" para \"Stride: 1\". Abaixo, há uma matriz de três por três e tons de cinza, em que a coluna com as três linhas de valores \"1\" com fundo branco, a coluna central em cinza com valores zero nas três linhas, e a terceira coluna com fundo preto e valores menos um. Esta projeta através de linhas pontilhadas para as três primeiras linhas para as três colunas do meio de uma matriz de ordem cinco por cinco, a qual possui todos os valores da primeira coluna igual a dez e fundo preto, a segunda coluna possui valores \"15\" e fundo cinza escuro, a central possui valores \"20\" e fundo cinza médio, a quarta coluna possui valores de \"80\" e fundo cinza claro, e a quinta coluna possui valores \"85\" em todas as linhas e fundo quase branco. Ao lado desta, as extremidades da última projeção de ordem três por três citada se projeta para o primeiro valor da primeira linha da coluna central de nova terceira matriz de ordem três por três através de linhas pontilhadas de uma segunda cor. Esta última matriz possui o valor \"-30\" em todas as linhas da primeira coluna e fundo cinza escuro, enquanto todos os outros valores são \"-195\" em fundo branco. \n",
        "\n",
        "[05:21] Se eu fizer mais um \"Stride\", eu vou operar com o pixel diretamente na vizinhança, então estou fazendo aqui um pulo de \"1x1\", e aqui o mesmo vale para a segunda dimensão, eu vou pular para linha seguinte, então nas duas dimensões eu vou fazer um \"Stride\" de \"1x1\".\n",
        "\n",
        "[05:41] Já com o \"Stride 2\", eu não vou de pixel a pixel, eu vou pulando de 2 em 2, é bem intuitivo, então se eu operei com esse pixel aqui, qual que é o próximo pixel que eu vou operar? São esses daqui que eu estou pulando de 2 em 2.\n",
        "\n",
        "[05:56] Então quando eu fizer o \"Stride\", eu vou estar operando com o segundo pixel, o mesmo vale para segunda dimensão, então se eu estou operando com essa linha agora, a próxima linha que eu vou operar é essa daqui, então vou operar com esse pixel aqui.\n",
        "\n",
        "[06:13] O que significa que quando terminar a minha janela deslizante eu vou ter operado só com 4 pixels da imagem, então a minha saída vai ser \"2x2\", certo? Então se eu continuar aqui a operação, eu vou deslizar a janela só quatro vezes, em compensação, com o \"Stride\" de um, eu tinha uma saída de tamanho \"3x3\", então tudo isso vai interferir na dimensionalidade da minha saída.\n",
        "\n",
        "Imagem de título \"Stride\" para \"Stride: 2\". Abaixo, há uma matriz de três por três e tons de cinza, em que a coluna com as três linhas de valores \"1\" com fundo branco, a coluna central em cinza com valores zero nas três linhas, e a terceira coluna com fundo preto e valores menos um. Esta projeta através de linhas pontilhadas para as três primeiras linhas das três primeiras colunas de uma matriz de ordem cinco por cinco, a qual possui todos os valores da primeira coluna igual a dez e fundo preto, a segunda coluna possui valores \"15\" e fundo cinza escuro, a central possui valores \"20\" e fundo cinza médio, a quarta coluna possui valores de \"80\" e fundo cinza claro, e a quinta coluna possui valores \"85\" em todas as linhas e fundo quase branco. Ao lado desta, as extremidades da última projeção de ordem três por três citada se projeta para o valor da primeira linha da primeira coluna de uma nova  matriz de ordem dois por dois através de linhas pontilhadas de uma segunda cor. Esta última matriz possui o valor \"-30\" na primeira coluna e fundo cinza escuro, enquanto os outros valores são \"-195\" em fundo branco.\n",
        "\n",
        "[06:41] Porque eu tenho que me preocupar com isso, porque eu tenho que saber se eu consigo realizar essas operações, por que eu vou receber uma imagem \"25x25\", se na primeira convolução eu já transformo ela em um tamanho de \"5x5\", eu já não consigo mais fazer operações que diminuam tanto assim essa imagem, senão vou ter um erro na minha rede.\n",
        "\n",
        "[07:05] Passando do \"Stride\" agora, teremos o parâmetro do \"Padding\", então se eu tenho um \"Padding 0\", eu tenho aquele problema que eu não consigo operar com as bordas da minha imagem porque o meu \"kernel\" é \"3x3\", então eu preciso de uma vizinhança de pelo menos um pixel aqui ao redor do meu pixel central, então não consigo operar com essa minha vizinhança de tamanho 1.\n",
        "\n",
        "![Imagem de título \"Padding\" para \"Padding: 0\". Abaixo, há uma matriz de ordem três por três e tons de cinza, em que a coluna com as três linhas de valores \"1\" com fundo branco, a coluna central em cinza com valores zero nas três linhas, e a terceira coluna com fundo preto e valores menos um. Esta projeta através de linhas pontilhadas para as três primeiras linhas das três primeiras colunas de uma matriz de ordem cinco por cinco, a qual possui todos os valores da primeira coluna igual a dez e fundo preto, a segunda coluna possui valores \"15\" e fundo cinza escuro, a central possui valores \"20\" e fundo cinza médio, a quarta coluna possui valores de \"80\" e fundo cinza claro, e a quinta coluna possui valores \"85\" em todas as linhas e fundo quase branco. Ao lado desta, as extremidades da última projeção de ordem três por três citada se projeta para o valor da primeira linha da primeira coluna de uma nova matriz de ordem três por três através de linhas pontilhadas de uma segunda cor. Esta última matriz possui o valor \"-30\" na primeira coluna e fundo branco, enquanto os outros valores são \"-195\" em fundo preto.](https://caelum-online-public.s3.amazonaws.com/1734-cnnpytorch/Transcri%C3%A7%C3%A3o/aula4_video2_imagem6.png\n",
        "\n",
        "[07:41] Então com o \"Padding 0\", eu vou perder resolução de \"5x5\", para \"3x3\", porque vou perder um pixel na direita, esquerda, em cima e em baixo. Som o \"Padding\" igual a um, eu tenho que a minha imagem original é essa aqui, mas eu acrescentei elementos artificiais que não existiam na imagem original.\n",
        "\n",
        "[08:10] Mas a imagem mesmo era \"5x5\", mesmo eu tendo feito a convolução que é uma operação que costuma perder resolução, quando eu acrescento as bordas artificiais eu consigo preservar a dimensionalidade da informação original.\n",
        "\n",
        "![Imagem de título \"Padding\" para \"Padding: 1\". Abaixo, há uma matriz de ordem três por três e tons de cinza, em que a coluna com as três linhas de valores \"1\" com fundo branco, a coluna central em cinza com valores zero nas três linhas, e a terceira coluna com fundo preto e valores menos um. Esta projeta através de linhas pontilhadas para as três primeiras linhas das três primeiras colunas de uma matriz de ordem sete por sete, a qual possui todos os valores das unidades externas iguais a zero e fundo preto, enquanto as seis unidades centrais da segunda coluna são valores \"10\" e fundo preto, as cinco centrais da quarta coluna são \"20\" e fundo cinza escuro, as cinco linhas da quinta coluna são valores \"80\" e fundo cinza claro, e as cinco linhas da sexta coluna são valores \"85\" em fundo cinza claro. A matriz projetada de ordem três por três nesta última se projetam por linhas pontilhadas até o valor \"-30\"0 da primeira linha da primeira coluna de uma nova matriz de cinco por cinco ao lado com diversos valores e tons de cinza, em que os mais escuros tendendo ao preto se encontram com os menores valores ao centro.](https://caelum-online-public.s3.amazonaws.com/1734-cnnpytorch/Transcri%C3%A7%C3%A3o/aula4_video2_imagem6.png\n",
        "\n",
        "[08:26] Com \"Padding 2\" eu já consigo manter a resolução da minha imagem, mesmo que o meu \"kernel\" seja de \"5x5\", por que veja, se meu \"kernel\" é \"5x5\", eu preciso centralizar esse pixel aqui com um pixel que tem uma vizinhança de tamanho 2, então tem que ter uma vizinha de tamanho 2 para cima, para os lados, para baixo, enfim, se eu coloco um \"Padding\" também de tamanho 2, eu consigo gerar bordas artificiais que vão permitir com que eu opere com toda a região da imagem original.\n",
        "\n",
        "[09:07] Então tenho aqui minhas bordas artificiais que vão permitir que eu opere com todos os pixels da imagem e gere uma saída de igual dimensão, então de novo, aqui está escrito \"9x9\", mas a minha imagem original é \"5x5\", então esse casamento de \"kernel\" \"3x3\" e \"Padding = 1\", \"kernel\" \"5x5\" e \"Padding = 2\", são os casamentos de quando você quer preservar a dimensionalidade, a resolução espacial da sua imagem.\n",
        "\n",
        "[09:35] Porque com \"kernel\" \"3x3\" você precisa da vizinhança de um, com \"kernel\" \"5x5\" você precisa da vizinhança de dois, com \"kernel\" \"7x7\" você precisa de uma vizinhança de três, e assim por diante.\n",
        "\n",
        "[09:45] Então temos uma fórmula que define a perda de resolução espacial ao longo da rede, então no momento que eu defino a minha camada convolucional, eu já consigo prever quanto de resolução espacial eu vou perder na minha imagem, e ela é definida de acordo com esses três parâmetros, o campo de visão, que é o kernel_size o \"Stride\" e o \"Padding\", o campo de visão está representado pela letra \"F\", o \"Padding\" pela letra \"P\", e o \"Stride\" pela letra \"S\".\n",
        "\n",
        "Conv2d no PyTorch\n",
        "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)COPIAR CÓDIGO\n",
        "Resolução espacial do mapa de ativação\n",
        "Xₒᵤₜ = ( (Xᵢₙ - F + 2P) / S) + 1COPIAR CÓDIGO\n",
        "[10:18] Então eu consigo calcular aqui olharmos por exemplo, aquele casamento que eu falei de \"3x3\" com \"Padding 1\" e de \"5x5\" com \"Padding 2\", eu consigo substituir aqui, (25-3+2x1)/1+1 = 25, raramente vamos colocar um Stride maior que “1” em uma rede convolucional.\n",
        "\n",
        "[11:17] Ou seja, eu preservei o tamanho da entrada da minha imagem, se eu não tivesse o \"Padding\", se eu fizesse aqui um \"Padding = 0\", eu não teria essa compensação do 2 vezes o \"Padding\", porque isso aqui seria dois vezes zero eu não teria esse mais, eu teria \"22 / 1 + 1\", a minha saída seria de tamanho 23 ou seja eu perderia 2 pixels em relação entrada.\n",
        "\n",
        "[11:50] Isso é super intuitivo, porque se eu tenho que operar um \"kernel\" \"3x3\", é aquilo que eu falei eu preciso de uma vizinhança \"= 1\" para todas as direções então eu perco 2 pixels na dimensão horizontal e 2 pixels na dimensão vertical, então de \"25x25\" eu passo a \"23x23\".\n",
        "\n",
        "[12:13] Aqui tem uma pergunta para vocês responderem, eu acho que é um negócio a se pensar, qual a resolução da saída de uma camada convolucional se a imagem de entrada for \"224x224\", o \"Field of View\" for \"7x7\" e o \"Stride\" for \"= 7\" e o \"Padding\" \"= 0\".\n",
        "\n",
        "Conv2d no PyTorch\n",
        "Resolução espacial do mapa de ativação\n",
        "Xₒᵤₜ = ( (Xᵢₙ - F + 2P) / S) + 1COPIAR CÓDIGO\n",
        "Qual a resolução da saída de uma camada convolucional se a imagem de entrada for 224×224, o Field of View (F) for 7x7, o Stride (S) for igual a 7 e o Padding (P) for igual a 0?\n",
        "[12:33] Isso aqui, o número em si não importa tanto, importa quando você for implementar efetivamente, mas é importante que você como uma pessoa que vai implementar redes convolucionais, olhe para isso aqui e pense, minha entrada é \"224x224\", meu \"kernel\" é \"7x7\", isso aqui tem perda de resolução a menos que eu compense com \"Padding\", se o \"Padding\" é 0, com certeza eu já vou perder aqui 3 pixels da minha imagem original em cada dimensão.\n",
        "\n",
        "[13:04] Se o \"Stride\" é igual a 7 significa que eu vou pulando de 7 em 7 pixels, ou seja, eu vou operar em cada uma das dimensões altura e largura com um em cada 7 pixels, então eu vou perder muita resolução, vai ser dividido por sete que é o pulo que eu dou no \"Stride\", e eu ainda vou perder mais três pixels aqui por causa desse \"kernel\" \"7x7\".\n",
        "\n",
        "[13:33] Dá para fazermos um cálculo rápido mas isso não importa tanto quando você olhar esses números e dizer: eu vou perder muita resolução com essa convolução e perda de resolução significa um pouco de perda de informação também, é importante termos isso na cabeça.\n",
        "\n",
        "[13:49] Agora no próximo vídeo vamos implementar uma camada convolucional e entender o impacto dela.\n",
        "\n",
        "Agora vamos pôr a mão na massa e ver como é que definimos no \"PyTorch\", uma camada convolucional e como é que usamos. Então nós vamos pouco a pouco construir o conhecimento para chegar na definição de uma rede, primeiro vamos camada por camada.\n",
        "\n",
        "[00:15] O que vamos precisar aqui é o pacote import torch, que ele é necessário para fazer qualquer operação básica de tensores então, sempre importamos, porque eventualmente vai precisar, só que dessa vez importaremos aqui um pacote específico que é o pacote from torch import nn #neural networks, este sempre vamos precisar.\n",
        "\n",
        "[00:35] Esse é onde estão as camadas, onde estão as funções de perda, onde está a maioria das coisas, para esse exemplo que vamos fazer aqui, eu vou importar do from skimage import data, os dados, ele tem algumas imagens padrão, para você realizar operações simples de processamento de imagem e vamos precisar importar o import matplotlib.pyplot as plt.\n",
        "\n",
        "Convolucional\n",
        "Documentação: https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
        "Começando pela operação principal, a convolução está contida na camada nn.Conv2d. Dentre os parâmetros que ela recebe, vamos focar nos que já conhecemos, que já são suficientes para uma diversa gama de aplicações.\n",
        "\n",
        "[01:03] Só precisamos disso para fazer uma camada convolucional, eu deixei aqui embaixo uma descrição de tudo que vimos no vídeo anterior, então se você precisar de algum lembrete do que é o \"Padding”, “Stride\", \"Channels\", está tudo escrito aqui e agora vamos brincar com duas imagens uma imagem em preto e branco, e uma imagem em RGB, para vermos a diferença, primeiro vamos carregar essas imagens.\n",
        "\n",
        "[01:29] Então a nossa imagem preto e branco que eu vou chamar de pb = data.brick() que são as imagens de tijolos que já vimos antes aqui, e a nossa imagem RGB vai ser o rgb = data.astronaut().\n",
        "\n",
        "[01:44] Vamos ver essas imagens, aqui é só plotar plt.imshow (pb) e plt.imshow (rgb), vou definir uma figura nova plt.imshor(pb) e plt.imshor(rgb). Vou colocar um mapa de cores, porque o matplotlib, costuma imprimir com esse mapa de cores que varia do azul ao amarelo, verde ao amarelo, se não me engano e aí nós colocamos um mapa aqui * plt.imshow (pb, cmap =Greys), é uma imagem preto e branco e aqui a imagem de astronauta com 3 canais.\n",
        "\n",
        "Imagem do output contendo um quadrado com uma imagem de padrão de tijolos somente com as linhas em preto e cinza, cujo eixo horizontal está graduado de zero a quinhentos a cada cem pontos, e eixo vertical de quinhentos a zero a cada cem pontos. Abaixo, há um quadrado com uma fotografia colorida de uma astronauta com roupas espaciais e segurando seu capacete. A astronauta está sorrindo e possui cabelos curtos e penteados para o lado com uma franja, e ao fundo está uma bandeira dos Estados Unidos e uma réplica em miniatura de um foguete espacial como cenário.\n",
        "\n",
        "[02:26] Então se eu imprimir o tamanho dessas duas imagens print(pb.shape(), rgb.shape() temos aqui então uma 512, 512, a terceira dimensões de canais está omitida, mas é um único canal e a outra é 512, 512, 3, porque são três canais de cor que o que torna ela uma imagem colorida.\n",
        "\n",
        "[03:02] Na próxima célula agora iremos definir a camada convolucional e fazer o forward da figura preto e branca e aqui já iremos entender que o forward, ele precisa ser feito sempre com tensores, precisamos converter essa imagem para um tensor e preciso obedecer às dimensões que o \"PyTorch\" espera.\n",
        "\n",
        "[03:23] Que é o tamanho do bet, ou seja se você vai passar uma imagem de uma vez, 10 imagens de uma vez, 50 imagens de uma vez, na sua rede, a quantidade de canais, então mesmo que seja preto e branco você tem que colocar a dimensão igual a um, dizendo que só tem um canal de cor, a altura e a largura da imagem, a primeira coisa que vamos fazer aqui é definir a nossa camada convolucional, eu acho que é o mais importante.\n",
        "\n",
        "[03:49] Então a camada convolucional nesse caso vai ser um conv = nn.conv2d(1, 16), o que precisamos agora, colab até te dá uma cola, aqui, in_channels, out_channels, kernel_size, e aqui ele já mostra que stride e padding já tem valor padrão, então se quisermos stride = 1 e padding = 0, nem precisamos fazer nada mas vamos lá.\n",
        "\n",
        "[04:18] O in_channels é a quantidade de canais de entrada, como é uma imagem preto e branco o valor é um, o out_channels, eu vou definir quantas características eu quero da minha imagem, então vamos por aqui que eu quero 16 características, então eu vou definir aqui que eu quero um out_channels 16.\n",
        "\n",
        "[04:33] Veja que eu estou omitindo os nomes porque eu estou definido na ordem, mas se te ajudar a memorizar, você pode colocar os nomes aqui e é obrigatório definir também, o kernel_size, que eu também posso omitir o nome do parâmetro.\n",
        "\n",
        "[04:55] Eu posso colocar aqui um kernel_size =, vamos lá, eu quero ou não perder resolução, isso é relevante ou não para o meu problema, eu quero um filtro maior ou um filtro menor, aprender características simples, menores, mais locais, então isso tudo é um raciocínio que ele acaba vindo um pouco mais de você experimentar com seu problema, porque cada problema vai ter um comportamento completamente diferente.\n",
        "\n",
        "[05:22] Mas as principais redes costumam usar \"kernels\" de tamanho \"3x3\", \"5x5\", por que a medida que você vai fazendo o aprendizado hierárquico, você consegue aprender desde características bem locais até características mais globais da sua imagem.\n",
        "\n",
        "[05:37] Então vou colocar aqui um kernel_size=3 e vou deixar sem \"padding\" para vocês verem a perda de resolução, então aqui eu defini a minha camada convolucional, para eu conseguir usar – eu posso até imprimir o tipo do dado para vocês verem que é uma camada convolucional exatamente do jeito que defini, e ele coloca o stride explícito, apesar de eu não ter definido – então para conseguir passar a imagem eu preciso transformar ela para tensor.\n",
        "\n",
        "[06:07] Então eu preciso pegar minha imagem que chama pb, que por enquanto ela não é um tensor e transformar ela para um pb_tns = torch.Tensor (pb). No primeiro curso que fizemos sobre Rede Neural com PyTorch tem um vídeo inteiro só sobre operações de conversão, então você pode ir lá olhar.\n",
        "\n",
        "[06:36] Então tem que converter isso aqui para tensor e eu também preciso transformar na dimensão correta, porque lembra que tem que ser bet_size, channel_size que é tamanho do canal, altura e largura, por enquanto qual é o tamanho que ele está? Com tamanho que o \"Array\" tinha.\n",
        "\n",
        "[06:58] Então ele está com 512, 512 que é altura e largura, eu preciso alterar a dimensão, então vou fazer uma operação que também está explicado neste primeiro curso de redes neurais que é a operação .view().\n",
        "\n",
        "[07:17] Que vai alterar as dimensões do meu tensor, então só preciso dizer olha, eu quero que as dimensões do meu tensor sejam \"1\", porque é uma única imagem então bet_size = 1, \"1\" de novo, porque é uma imagem preto e branco que só tem um canal, e, agora eu posso recuperar as dimensões originais do meu tensor para ficar com altura e largura.\n",
        "\n",
        "[07:49] Então aqui eu estou definindo tamanho do bet, número de canais, a altura da imagem, a largura da imagem, então se eu imprimir agora print(pb_tns.size()), eu vou ter aqui que ele está nas dimensões certas que o \"PyTorch\" precisa, tudo que eu faço agora é mapa_de_ativação =, ou do jeito que preferir, vai ser conv(pb_tns) que é a nossa variável de convolução e passando o meu tensor como entrada.\n",
        "\n",
        "[08:32] pb_tns, que é o nosso tensor de imagem, qual é o tamanho desse mapa de ativação? Nós já sabemos,e o tamanho? print(mapa_de_ativacao.size()). Se eu coloquei que eu quero 16 filtros, então minha convolução vai produzir 16 mapas de ativação.\n",
        "\n",
        "[08:56] Então a minha saída vai ser tamanho do bet, eu passei uma imagem só, quantidade de canais da saída, 16, porque tenho 16 mapas de ativação, e eu perdi resolução aqui quando eu usei o \"kernel\" \"3x3\", sem \"padding\", quanto de resolução que eu perdi? 510, 510, eu perdi \"2x2\" de resolução então aqui fica 510, 510 de uma imagem que era 512, 512.\n",
        "\n",
        "[09:32] Então se eu coloco aqui agora o padding = 1 que é aquele casamento de \"kernel = 3\" e \"padding = 1\", eu vou manter a minha resolução 512, 512, então assim operamos uma imagem preto e branco, quanto a imagem colorida vou até copiar isso aqui porque são poucas mudanças, a única coisa que tem que fazer é corrigir o número de canais de entrada.\n",
        "\n",
        "[09:56] Irei até deixar desse jeito para vocês verem que vai dar um erro de dimensionalidade inconsistente, o qual é muito comum. Então eu vou trocar agora o pb_tns pelo rgb_tns. Não vamos precisar da linha 6, porque iremos fazer outra operação.\n",
        "\n",
        "[10:23] Então eu vou criar aqui o meu tensor a partir da imagem RGB, só que agora ele tem uma outra dimensionalidade, ele tem a dimensionalidade de 512, 512, 3, então a primeira coisa que eu tenho que fazer é uma operação de permuta, rgb_tns = rgb_tns.permute e eu preciso jogar os canais para o início, então eu coloco a dimensão 2 no início e a dimensão 0 e 1 ficam em seguida, então aqui fica canal x altura x largura, e eu vou usar uma operação que chama .unsqueeze que vai “desespremer” as dimensões.\n",
        "\n",
        "[11:11] Então estou criando uma nova dimensão na posição \"0\", então se eu imprimo agora esse tensor, ele tem 1, que é o tamanho do bet, é uma imagem só, três que é o número de canais, 512, 512.\n",
        "\n",
        "[11:26] Se eu tento gerar um mapa de ativação a partir desse tensor, lembrando que eu não mudei o número de canais aqui eu vou ter um erro muito comum: dado um grupo de pesos que é 16, 1, 3, 3, o input esperado era 1, 3, 512, 512, esse input tinha que ter um canal só, mas eu recebi três canais. Então ele está dizendo claramente, seu input que tem esse tamanho devia ter um canal, mas ele tem três, então o que você tem que fazer é corrigir a sua rede.\n",
        "\n",
        "[12:02] É importante aprender a ler os erros, você corrige aqui, minha imagem agora tem três canais e agora eu posso imprimir o tamanho do mapa de ativação, de novo vai ser 16, que é o número de filtros que eu defini aqui, então, vai ser uma imagem, 16 mapas de ativação gerados e o tamanho de novo vai ser 510, 510, porque em resolução espacial as duas imagens tem o mesmo tamanho, todas são 512, 512.\n",
        "\n",
        "[12:33] Se eu coloco um kernel_size = 5, eu vou perder \"4x4\" de dimensão, então vai ficar 508, 508, e daí por diante, basta colocar o padding= 2 para eu conseguir manter a resolução espacial.\n",
        "\n",
        "[12:50] Esses são os principais parâmetros, as principais operações aqui da convolução, mais para frente nós aprendemos como é que coloca isso numa rede.\n",
        "\n",
        "No script Conv2d.ipynb altere as camadas convolucionais definidas, para que o campo de visão do kernel seja 7x7, mas garantindo que não haja perda de resolução espacial ao processar a imagem de entrada. A definição da convolução é atribuída na variável conv, como apresentado a seguir:\n",
        "\n",
        "conv = nn.Conv2d(1, 6, kernel_size=3, padding=1)COPIAR CÓDIGO\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "Mantendo o stride em seu valor padrão (1), basta que o padding seja igual à metade do campo de visão menos 1 (F - 1)/2 para que a resolução seja mantida. Sendo o campo de visão igual a 7, temos que:\n",
        "\n",
        "conv = nn.Conv2d(1, 6, kernel_size=7, padding=3)\n",
        "\n",
        "Convolução com imagens RGB\n",
        "Parâmetros da Convolução\n",
        "Controle de resolução da informação\n",
        "Definindo uma convolução no PyTorch\n",
        "\n",
        "Agora vamos falar um pouco da “Operação de Pooling” que é a segunda operação mais importante de uma rede convolucional, então falamos bastante que se CNNs se intercalam transformação e subamostragem e basicamente é da subamostragem que nós vamos falar hoje que é realizada através da operação do “Pooling”\n",
        "\n",
        "[00:19] Se for pensar em subamostragem a partir do conceito mais abstrato, por exemplo a perda de resolução de uma imagem, temos aqui a foto do Paulo Silveira dos irmãos Silveira da Alura.\n",
        "\n",
        "Imagem de título \"Subamostragem em CNNs\", e abaixo há dois quadrados lado a lado: o primeiro possui uma imagem colorida e bem definida do CEO da Alura Paulo Silveira, sorrindo e segurando a câmera fazendo uma selfie, e ao fundo há um gramado com uma casa de cor clara. Este ainda possui o eixo horizontal graduado de zero a quatrocentos a cada cem pontos, e o eixo vertical graduado de quatrocentos a zero a cada cinquenta pontos. Ao lado, Há a mesma imagem, porém embaçada com baixa resolução, e seu eixo horizontal está graduado de zero a oitenta a cada vinte pontos, e o eixo vertical está graduado de oitenta a zero a cada dez pontos.\n",
        "\n",
        "[0032] O que eu fiz aqui foi reduzir a resolução espacial da imagem de 400 x 400 para 80 x 80, eu reduzi bastante o número de pixels da imagem mas eu preservei que tem de mais importante, então eu fiz uma subamostragem que seleciona uma quantidade menor de pixels de forma inteligente, de forma que eu consigo compreender a informação da imagem inteira.\n",
        "\n",
        "[01:00] Eu estou vendo aqui um Paulo mais borrado, um Paulo Silveira mais borrado, uma árvore mais borrada, uma casa mais borrada mas todas essas informações ainda estão aí, se eu fizer uma subamostragem mais agressiva, eu estou vendo aqui eu vejo menos coisas mas eu vejo coisas que me dá uma ideia mais geral da imagem. Então estou vendo que é uma pessoa branca com cabelo e barba, camisa azul, talvez tenha plantas atrás.\n",
        "\n",
        "[01:28] Eu já não sei mais que tem um arbusto como eu sabia no início, eu sei que tem coisas verdes que normalmente são plantas atrás, eu já não sei mais aqui que o Paulo Silveira usa óculos mas eu sei que ele é uma pessoa.\n",
        "\n",
        "[01:44] Então enfim a subamostragem é um subconjunto representativo do dado original. Isso é muito importante em CNNs por algumas razões, as duas principais têm a ver com você reduzir o custo computacional que quando você reduz, faz a subamostragem você diminui a quantidade de informação que vai trafegar na rede.\n",
        "\n",
        "[02:08] Você reduz a computação necessária para processar o dado e você também precisa de menos parâmetros para produzir características representativas daquela imagens, então os dois primeiro tem a ver com redução de custo computacional e também você tem menos chance de *overfitting.\n",
        "\n",
        "[02:27] Conserto de overfitting eu recomendo muito que vocês estudem se vocês vão trabalhar com redes neurais vale a pena procurar um pouco mais sobre isso mas basicamente a ideia de que você não vai se ater tanto se não vai confiar muito em pequenos detalhes da imagem, você não vai se viciar em certos atributos na hora de tomar uma decisão.\n",
        "\n",
        "[02:50] Então eu não preciso saber que o Paulo Silveira está de óculos se eu quero dizer se ele é uma pessoa ou não, então essa subamostragem ela me ajuda a concentrar no que é mais importante e relevante das características da imagem.\n",
        "\n",
        "[03:07] Então nós falamos muito do aprendizado hierárquico e aqui também tem a ver com como nós conseguimos pegar características tão complexas com Kernels pequenos isso é porque quando nós criamos uma estrutura em camadas da estrutura hierárquica eu vou conseguir com a sub amostragem a cada camada, a cada bloco evolucional eu estou conseguindo reduzir a quantidade de informação daquela imagem.\n",
        "\n",
        "[03:36] E concentrar aquela informação de forma mais densa. então se eu tinha várias bordas espaçados ao longo da imagem, se eu tinha aqui uma imagem, aqui eu tinha no início uma imagem uma característica com bordas mais espaçadas.\n",
        "\n",
        "[03:53] Eu tenho uma borda aqui, uma borda aqui, outra aqui, quando eu chego no médio nível, que eu faço o pooling aqui eu já vou, talvez tenha uma imagem que vai se parecer com losango, porque eu juntei essas bordas, eu peguei o que tinha de mais importante aqui e eu juntei e a partir de um conjunto eu formei que losango.\n",
        "\n",
        "[04:15] Legal, então essa subamostragem ela vai destacar o que há de mais importante na imagem e vai permitir que um conjunto de várias bordas eu pego aqui, por exemplo um círculo e a partir de vários olhos e vários bordas de alta frequência eu consegui montar, por exemplo uma colmeia, Cisne, Ganso, assim por diante, então o pooling contribuir com o aprendizado hierárquico.\n",
        "\n",
        "[04:44] Vamos ver por exemplo se nós passarmos a imagem do Paulo Silveira numa rede que só possui filtro 3 x 3 ela só pega informação de um campo de visão de 3 pixels por 3 pixels, mas com poolings consecutivos eu consigo pegar uma região cada vez maior da imagem com filtros 3 x 3.\n",
        "\n",
        "[05:12] Então nas camadas iniciais eu vou ter características, ativações que são ruidosos, eu tenho várias bordas eu consigo ver a alta frequência das plantas lá atrás, eu consigo ver a silhueta do Paulo, da boca eu tenho por exemplo, o filtro da Média que vou pegar uma região pequeno a região 3 x 3 na imagem e vão tirar uma média dos pixels e eu não consigo ver a imagem como todos só que mais borrada.\n",
        "\n",
        "Imagem com título \"Aprendizado Hierárquico\". Abaixo, há um quadrado chamado \"Entrada\" contendo uma selfie colorida de Paulo Silveira sorrindo em uma pequena área quadrada, a qual está projetada através de linhas para outro quadrado ao lado chamado \"conv1\" sobreposto a uma sequência de outros, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos chamados \"Pool 1\". Abaixo destas duas sequências, há uma chave que aponta para a mesma selfie de Paulo Silveira, porém em preto e branco e bastante pixelizada. Em seguida aos quadrados, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\".\n",
        "\n",
        "[05:36] Eu tenho que filtro por exemplo vamos fazer essa média dos pixels mas negativando a imagem invertendo então enfim eu tenho operações de que ainda vão atuar em regiões locais da Imagem e a informação Global aqui tá bem preservada no nível bastante detalhado.\n",
        "\n",
        "Imagem com título \"Aprendizado Hierárquico\". Abaixo, há um quadrado chamado \"Entrada\" contendo uma selfie colorida de Paulo Silveira sorrindo em uma pequena área quadrada, a qual está projetada através de linhas para outro quadrado ao lado chamado \"conv1\" sobreposto a uma sequência de outros, o qual por sua vez projeta uma área quadrada menor para outra sequência de quadrados sobrepostos chamados \"Pool 1\". Abaixo destas duas sequências, há uma chave que aponta para a mesma selfie de Paulo Silveira, porém em alto contraste preto e branco e bastante pixelizada. Em seguida aos quadrados, há duas sequências de doze quadrados sobrepostos cada chamados \"Conv2\" e \"Pool2\". Ao lado, há duas sequências de dezoito quadrados sobrepostos chamados \"Conv3\" e \"Pool3\". Por fim, há duas barras inclinadas chamadas \"FC1\" e \"FC2\".\n",
        "\n",
        "[05:57] Quando eu passo mais adiante na rede eu vou pegando regiões cada vez maiores Por que eu diminui a imagem, então quero 3 x 3 pega uma região muito maior em relação a imagem original.\n",
        "\n",
        "[06:08] Então já vou pegando características, por exemplo bordas mais grossas porque agora eu consigo pegar uma região da imagem que eu represento por exemplo o olho do Paulo Silveira e menos pixels, então passando que era 3 x 3 em três janelas já peguei o olho inteiro, e daí por diante eu vou pegando informações aqui eu perdi a alta frequência das árvores lá atrás, e assim por diante.\n",
        "\n",
        "[06:37] Já no final dessa minha rede eu perdi completamente, eu peguei uma ativação aqui específica do neurônio que detecta pessoas é treinada para detectar objetos e o objetivo dela aqui é descobrir se tem uma pessoa ou não na imagem, então esse neurônio ele ativa quando ele ver pixels que pertencem a uma pessoa.\n",
        "\n",
        "[06:58] Então ativa muito mais na região onde tem o Paulo Silveira do que por exemplo, atrás nas árvores, então aqui esses “kernels” do final eles são muito mais concentrados no que realmente importa, são regiões da imagem que se referem a uma pessoa, ele não vai distinguir cidade, óculos, não vai ver se tem planta atrás, qual que é a cor da camisa.\n",
        "\n",
        "[07:35] Ele está mais interessado na localização da pessoa, na presença de características de uma pessoa.\n",
        "\n",
        "Imagem com título \"MaxPool2d\" para \"Field of View: 2\" e \"Stride: 2\". Abaixo, há uma matriz de ordem quatro por quatro, e cada matriz interna de dois por dois possui uma cor de fundo diferente. Há uma seta vertical de legenda \"x\" e uma horizontal de legenda \"y\" indicando os eixos da matriz. De cima para baixo, da esquerda para a direita, possui os valores: 1, 1, 2, 4, 5, 6, 7, 8, 3, 2, 1, 0, 1, 2, 3 e 4. Ao lado, há uma seta azul indicando para a direita.\n",
        "\n",
        "[07:31] Então o “Max Pooling”, para nós já conhecermos os parâmetros da camada, ele tem dois parâmetros que são os mesmos que nós já conhecemos para convolução, que é o field of view ou campo de visão da imagem e o Stride, então aqui nessa matriz temos uma representação de uma imagem que nós vamos passar uma operação de “MaxPooling” ou seja, vamos pegar o Pixel de maior ativação em uma janela 2 por 2 que ao campo de visão, dando pulos de dois em dois pixels.\n",
        "\n",
        "Imagem com título \"MaxPool2d\" para \"Field of View: 2\" e \"Stride: 2\". Abaixo, há uma matriz de ordem quatro por quatro, e cada matriz interna de dois por dois possui uma cor de fundo diferente, sendo a do canto superior direito em destaque por linhas tracejadas em vermelho. Há uma seta vertical de legenda \"x\" e uma horizontal de legenda \"y\" indicando os eixos da matriz. De cima para baixo, da esquerda para a direita, possui os valores: 1, 1, 2, 4, 5, 6, 7, 8, 3, 2, 1, 0, 1, 2, 3 e 4. Ao lado, há uma seta azul indicando para a direita e para um quadrado em destaque por linhas vermelhas pontilhadas com o valor \"6\".\n",
        "\n",
        "[08:07] Então vou passar aqui na primeira janela eu fiz um MaxPooling, então qual que é o valor máximo dessa janela? É o 6, então dessa janela 2 por 2 eu vou pegar o Pixel de maior ativação 6, passando adiante fazendo um stride de 2 nesta nova janela.\n",
        "\n",
        "[08:25] Qual é o Pixel de maior ativação? É o 8 passando no stride de dois em dois na dimensão agora da altura, pixel de maior ativação é o 3 e o terceiro maior motivação é o 4.\n",
        "\n",
        "[08:37] Então transformei uma imagem 4 por 4 numa imagem 2 por 2, pegando de cada janela a informação de máxima ativação. Esse é o Maxpooling.\n",
        "\n",
        "Imagem com título \"Kernel size: 2\". Abaixo, há uma matriz de ordem quatro por quatro, em que todos os valores das linhas da primeira coluna são \"35\", da segunda coluna são \"60\", da terceira são \"275\" e \"-280\" nas linhas da quarta coluna. A matriz interna de dois por dois no canto superior direito da matriz 4x4 está em destaque e se projeta através de linhas tracejadas para o primeiro valor \"60\" em destaque de uma segunda matriz de ordem três por três ao lado, a qual possui todos os valores da primeira coluna igual a \"60\", da segunda é \"275\" e da terceiraé \"275\" também.\n",
        "\n",
        "[08:50] Aqui vamos ter exemplo Kernel size = 2 é que nós vamos definir igual fizemos com convulsão com Kernel size igual a 2 eu vou pegar a região 2 por 2 na imagem, com Kernel Size igual a 3, eu vou pegar a região 3 por 3 na imagem e veja que eu já diminui muito mais a resolução aqui eu passei de 4 por 4 para 3 por 3, aqui eu passei de 4 por 4 para 2 por 2 só mudando o tamanho do Kernel numa região maior da imagem.\n",
        "\n",
        "Imagem com título \"Stride: 1\". Abaixo, há uma matriz de ordem quatro por quatro, em que todos os valores das linhas da primeira coluna são \"35\", da segunda coluna são \"60\", da terceira são \"275\" e \"-280\" nas linhas da quarta coluna. A matriz interna de dois por dois no canto superior direito da matriz 4x4 está em destaque e se projeta através de linhas tracejadas para o primeiro valor \"60\" em destaque de uma segunda matriz de ordem três por três ao lado, a qual possui todos os valores da primeira coluna igual a \"60\", da segunda é \"275\" e da terceira é \"275\" também. Ao lado destas, há uma terceira matriz idêntica à primeira descrita, porém a matriz interna dois por dois em destaque abrange as duas primeiras linhas da segunda e terceira coluna, a qual se projeta para o valor \"275\" da primeira linha da coluna central de uma quarta matriz de ordem três por três idêntica à segunda descrita.\n",
        "\n",
        "[09:21] O Stride aqui eu tenho um exemplo de stride = 1 então a janela passou dessa região para só um pixel ao lado nessa região, então aqui eu vou transformar uma imagem 4 por 4 numa imagem 3 por 3 e com stride de dois que é o mais comum , o valor padrão do policial stride do tamanho da janela.\n",
        "\n",
        "[09:42] Ou seja você desliza janela sem sobrepor com as regiões onde você já passou, e aqui ele vai transformar uma imagem em 4 por 4 em 2 por 2 dando um pulo igual a 2 e agora nós vamos ver na prática como implementar o poolinge qual o efeito dele.\n",
        "\n",
        "Vamos agora colocar a mão na massa e implementar a camada de Pooling e ver como é que ela funciona na prática. Eu estou fazendo um pouco mais rapidamente nos parâmetros da camada de pooling porque o comportamento da janela que vai fazer o pooling é muito parecido com o da convolução Stride a mesma ideia, o padding é a mesma ideia, o que muda é só o tipo de operação que vai ser realizada.\n",
        "\n",
        "[00:24] Então vou definir um tamanho do meu “Kernel” que nesse caso não se trata de um filtro convolucional mas só a janela, que vai deslizar e define também o stride e o padding. A alteração que eu vou realizar vai ser maxpooling entregando o valor máximo que vai ser minipooling, vai ser “Average Pooling” vai depender da camada que eu estou definindo.\n",
        "\n",
        "[00:46 ] Então aqui eu estou definindo um “MaxPool 2D” significa que eu vou pegar o máximo o valor dentro da janela, em geral as duas operações mais usadas são máximo e a média que é o “Max” e o “Average”, mas vocês podem consultar todos os tipos de pooling na documentação que está linkada aqui.\n",
        "\n",
        "[01:04] Aqui tem os parâmetros do “Kernel” stride padding e o fato de que a camada de pooling espera uma entrada de pelo menos três dimensões, que são os canais da imagem altura e largura, mas em geral dentro da rede nós vamos ter essa dimensionalidade do batch canal por altura por largura que atende a camada de pooling.\n",
        "\n",
        "[01:26] Então, vamos criar aqui, no Google Collab, um tensor para nós poder ver como é que o pooling funciona tns = torch.FloatTensor(), vamos criar aqui com pelo menos três dimensões, então com 3 dimensões uma lista simples Torch.FloatTensor([[[1, 2, 3 ] , [4, 5, 6], [7, 8, 9]).\n",
        "\n",
        "[01:53] Aqui nós temos as três dimensões 1, 2, 3, 4, 5, 6, 7, 8, 9, com esses dois colchetes aqui definido que são novas dimensões na nossa lista, e definir a camada de pooling, vamos definir a nossa camada com pool = nn.MaxPool2d(), ele até disse o que eu preciso definir. Tem várias operações de arquiteturas mais complexas que vocês vão ver aqui na assinatura, mas nós só vamos nos preocupar mesmo com o Stride e o tamanho do “Kernel” porque nem padding não precisa se preocupar.\n",
        "\n",
        "[02:30] Em geral, não se usa o padding em redes neurais convolucionais, porque o objetivo do pooling é perder resolução, é você pegar a máxima ativação de cada janela ou a média então o padding meio que perde um pouco do sentido dele aqui, mas tem casos raríssimos onde se usa, então vamos definir aqui um maxpooling2d com Kernel de tamanho 2 por 2 e um stride de tamanho 1.\n",
        "\n",
        "[02:57] Certo? E agora nós vamos aplicar essa camada de pooling para produzir uma saída, tensor, vamos imprimir agora o tamanho do nosso tensor e imprimir também o tamanho da saída. Beleza? Então eu tinha aqui eu vou até imprimir o próprio tensor também para nós vermos que o que que aconteceu.\n",
        "\n",
        "[03:30] Eu perdi só um pixel em cada dimensão porque eu tinha uma imagem, vamos dizer que se uma imagem de um canal por 3 de altura e 3 de largura e eu produzir uma saída - que eu não imprimi aqui - de um canal também, que ele não vai mexer na dimensão dos canais, mas 2 por 2 porque eu transformei cada janela 2 por 2 aqui em um único Pixel na saída.\n",
        "\n",
        "[03:56] Eu peguei o máximo dessa janela, dessa daqui de baixo máximo daqui, e produzir essa saída 2 por 2, se eu colocasse se eu não colocasse nada no straid o valor padrão dessa camada é colocar o straid igual ao tamanho da janela, para que não haja sobreposição.\n",
        "\n",
        "[04:15] Então se eu der “ok” aqui eu fiquei com o número só, porque essa janela não consegue deslizar. Eu vou colocar essa janela aqui em cima, preciso mudar a cor, essa janela nessa região aqui da minha matriz, da minha imagem e vou ver que ativação máximo é o 5.\n",
        "\n",
        "[04:49] Agora eu preciso deslizar a janela mas o straid é igual a 2 eu não consigo pular essa janela 2 pixels, por que não existe imagem aqui, então não consigo fazer esses straid então se eu não tomar cuidado eu posso perder muita informação, olha quanta informação da minha imagem eu perdi e isso acaba não sendo útil na prática.\n",
        "\n",
        "[05:16] Então nesse caso de um dado tão pequeno, eu coloco o Kernel 2 e 1 Stride de um por exemplo, e que eu consigo ter uma resposta que opera com todo a imagem e agora eu botei aqui para nós vermos como que funciona na imagem da astronauta, no resultado da convolução que fizemos, vamos ver que opooling não vai mexer na dimensão dos canais nem na dimensão do bach, ele só vai mexer com as duas últimas dimensões que são as altura e largura que a resolução espacial da imagem.\n",
        "\n",
        "[05:55] O mapa de ativação, mapa de características né que é o nosso feature map de 1 por 16 por 512 por 512, e agora eu vou criar a mesma camada de pooling aqui vamos ver.\n",
        "\n",
        "[06:04] Criamos a mesma camada de pooling aqui e operar ela com a minha imagem, com meu mapa de ativação, e vou imprimir print (saída.size()) ele vai me dar aqui 1, 16, 511, 511, 511, eu perdi um pixel só da minha imagem por causa do Stride = 1, porque eu fui de 1 em 1 sobrepondo ao longo da Imagem e pegando o máximo valor daquela região.\n",
        "\n",
        "[06:36] Em geral não é isso que se faz, em geral nós colocamos o “Kernel” igual a 2, kernel_size = 1 e deixa o stride igual ao tamanho do “Kernel” e o que nós vamos ver aqui agora é que a imagem vai ser reduzida pela metade, no caso mapa de características vai ser reduzido pela metade.\n",
        "\n",
        "[06:57] E aí é que vem o grande ganho poder de processamento que vem o grande ganho de você reduzir o número de operações da sua rede, porque eu vou operar minha janela deslizante do pooling ao longo da imagem e eu vou reduzir a minha imagem pela metade, mantendo as principais características de cada janela dois por dois.\n",
        "\n",
        "[07:18]Então não é uma grande perda de informação mas é uma grande redução de resolução espacial que vai resultar no ganho de processamento computacional.\n",
        "\n",
        "Enumeramos em aula algumas vantagens da subamostragem em redes convolucionais. Em termos de custo computacional, reduzir a dimensionalidade espacial dos mapas de ativação permite uma grande economia de processamento. Por si só essa já é uma vantagem bastante convincente para usar o pooling, pois imagens são dados de altíssima dimensionalidade.\n",
        "\n",
        "Um dos principais livros de Deep Learning [1] define o Pooling como um \"resumo estatístico\" das saídas de uma convolução. Isso porque a camada não realiza uma subamostragem aleatória dos mapas de ativação, mas destaca as características mais relevantes de cada vizinhança local (definida pelo campo de visão do pooling).\n",
        "\n",
        "Um importante efeito disso é que o Pooling torna a representação aproximadamente invariante a pequenas translações. Em termos claros, se a imagem de entrada for levemente deslocada no espaço, uma boa parte do mapa de características resultante do pooling não será alterado. A imagem a seguir mostra exemplos do mesmo objeto após translações no espaço.\n",
        "\n",
        "\n",
        "\n",
        "Os mapas de ativação produzidos pela convolução mantêm a informação espacial da entrada, ou seja, a localização das características ativadas está fortemente relacionada à localização das regiões correspondentes. Uma imagem espacialmente deslocada terá como saída um mapa de ativação espacialmente deslocado. Com o Pooling e o seu resumo estatístico, a rede concentra seus esforços em identificar a presença de características relevantes, independentes da sua localização específica na imagem.\n",
        "\n",
        "Vale ressaltar que a informação espacial da imagem ainda é mantida até certo nível. O exemplo dado pelo livro menciona o problema de detecção de faces. Não importa a localização precisa dos olhos, mas importa bastante saber que existe um olho do lado esquerdo da face e outro do lado direito. O Pooling preserva esse nível de informação, que para a maioria dos problemas não só é suficiente, como também é vantajoso pela adição da invariância.\n",
        "\n",
        "[1] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016, pg 342.\n",
        "\n",
        "A última camada que nós vamos falar é a de “Batch Normalization”, que como o próprio nome dá uma ideia, é uma normalização ao longo dos Batchs que passam na rede, vale a pena revisar os conceitos de Batch de normalização, vamos dar uma passada neles mas bem rapidamente aqui.\n",
        "\n",
        "[00:23] Quando nós falamos de normalização geralmente, estamos falando do pré-processamento que vai entrar na rede, então é um pré-processamento é essencial que nós até fizemos com os “Kernels”, na hora de produzir os “Kernels” complexos, é você controlar essa distribuição, seja subtraindo pela média, dividido pelo desvio padrão. Mas você dá uma espécie de “normalizada” na distribuição, para permitir que a convergência do treinamento seja mais estável.\n",
        "\n",
        "[00:58] Então aqui temos um exemplo de dado, um exemplo de distribuição que essa dispersão em vermelho, e nós podemos entender melhor o que acontece à medida que você vai normalizando esse dado, você começa a ver uma distribuição mais controlada. E qual é o ganho que nós temos nisso?\n",
        "\n",
        "[01:17] Quando você normaliza a distribuição, você controla a distribuição é mais fácil você encontrar correlações e até definir modelos em cima desses dados, então em relação ao dado original eu consigo encontrar aqui, por exemplo uma reta ou uma elipse que melhor descreve a minha distribuição.\n",
        "\n",
        "Imagem com título \"Pré processamento\" seguido do texto \"Dados centralizados e normalizados aceleram a convergência do treinamento: convergência estável\". Abaixo, há uma sequência de três gráficos de pontos alinhados horizontalmente, cujos eixos horizontais e verticais são graduados de menos seis a seis a cada dois pontos. O primeiro chamado \"Dado original\" possui maior concentração de valores ao centro, tendendo para o primeiro, segundo e quarto quadrantes. O segundo gráfico possui maior concentração ao centro tendendo ao segundo, terceiro e quarto quadrantes. Por fim, o último gráfico possui uma concentração de pontos mais densa próximos à origem em zero e zero.\n",
        "\n",
        "[01:38] Tem aqui um espaço de busca mais controlado. Então é isso tudo que eu estou falando é só para dizer que normalize e seus dados antes de alimentar uma rede neural, por que a convergência do treinamento vai ser estável e possivelmente você vai conseguir chegar num bom resultado mais rapidamente.\n",
        "\n",
        "[01:58] Existem casos, onde se você não normalizar, você nem consegue chegar no resultado viável, resultado útil então normalize seus dados. Só que quando nós fazemos esse pré-processamento quando normalizamos os nossos dados, estamos trabalhando só com a entrada da rede, então uma vez que você alimenta o dado na rede, teoricamente você só vai pegar o resultado produzido no final, todas as operações vão ser realizadas internamente pela rede.\n",
        "\n",
        "[02:28] Então esse controle de normalização nós fazemos com entrada e uma vez que o dado entra na rede nenhuma normalização é realizada se não tiver um batch Normalization que vamos aprender agora. Então os autores da camada do Batch normalization eles propuseram que a diferença nas distribuições das características intermediárias da rede, dificultam a convergência e propuseram uma camada a ser adicionada após a convolução e antes da ativação linear.\n",
        "\n",
        "[03:03] Antes de nós seguirmos adiante eu vou voltar rapidamente só para dizer que porque eles propuseram uma nova camada? Porque se você quer realizar operações à medida que a imagem passa na rede ao longo dessa rede, a melhor forma de você fazer isso é criando uma camada que faça isso por você.\n",
        "\n",
        "[03:24] Por exemplo a subamostragem era uma boa ideia para você fazer o aprendizado hierárquico, adicionar invariância translação, então em vez de pegar a saída intermediária na rede fazer uma sub amostragem, devolver para rede você, cria isso como uma camada e o aprendizado fica de ponta a ponta.\n",
        "\n",
        "[03:45] Você consegue fazer um fluxo contínuo do seu dado sem precisar ficar tirando e colocando ele de volta na rede. A ideia da rede é ela ser autocontida, então se você quer acrescentar alterações nela você acrescenta na forma de camadas\n",
        "\n",
        "O pré-processamento é realizado com o dado de entrada\n",
        "Por que não fazer isso também com as ativações das camadas intermediárias?\n",
        "Imagem com sequência de quadrados. O primeiro chamado \"Input 32x32\" possui um \"A\" escrito com linhas tortas. Uma parte desta letra se projeta através de \"convolutions\" para uma sequência de seis quadrados sobrepostos chamada \"C1: feature maps 6@28x28\". Destes, uma pequena área quadrada se projeta através de \"subsampling\" para outra sequência de seis quadrados sobrepostos chamada \"S2: f. maps 6@14x14\". Desta última se projeta através de \"convolutions\" uma outra pequena área para uma sequência de dezesseis quadrados sobrepostos chamada \"C3: f.maps 16@10x10\", que por sua vez se projeta através de \"Subsampling\" para outros dezesseis quadrados sobrepostos chamados \"S4: f.maps 16@5x5\". Por fim, estes dão inicio a uma sequência de barras através de \"Full connection\", a primeira chamada \"c5: layer 120\", a segunda \"F6: layer 84\" e a última é \"Output 10\" em \"gaussian conection\".\n",
        "\n",
        "[04:00] Então aqui, agora os autores do Batch Normalization propuseram o seguinte bloco convolucional: que é começando pela transformação que a convolução, passando pela normalização do que sai da convolução e só depois pela ativação não linear.\n",
        "\n",
        "[04:16] Então nós conversamos extensivamente sobre isso no curso sobre introdução a rede neurais da importância da ativação não linear, para você aprender características mais complexas. A diferença aqui, é que em vez de ativar a saída da convolução nós vamos ativar a saída normalizada da convolução.\n",
        "\n",
        "[04:41] Então, só para entendermos essa camada bem brevemente, aqui no meio nós temos uma operação muito familiar que é a normalização, você pega o dado subtrai da média e divide pelo desvio padrão, isso é feito ao longo dos Batches e além dessa normalização normal padrão que conhecemos a camada também vai ter dois pesos esse gama e beta que vão ser aprendidos para poder flexibilizar essa normalização à medida que for necessário.\n",
        "\n",
        "Imagem com título \"Batch Normalization\". Abaixo, há a fórmula \"BN\" em função de \"x\" é igual à letra grega \"gama\" vezes uma divisão em que \"x\" menos a letra grega \"mu\" com acento circunflexo é dividido pela letra grega \"sigma\" com acento circunflexo, depois somado a letra grega \"beta\". Ao lado, há as legendas em que a média é representada por \"mu\", o desvio padrão é representado por sigma, e os pesos aprendidos são representados por gama e beta. Abaixo, há uma sequência de três retângulos, em que o primeiro \"conv\" aponta para o segundo \"batch norm\" que por sua vez aponta para \"ReLU\".\n",
        "\n",
        "[05:18] Algumas características vão precisar ser mais normalizadas, outras menos, é interessante ter esses pesos na camada para você ter um controle e não fazer uma normalização direta simples, pelo menos foi o que os autores da camada encontraram.\n",
        "\n",
        "[05:33] Então o fato dela ter pesos aprendidos, significa que você tem que redefinir essa camada cada vez que você quer realizar uma normalização, então diferente da ativação não linear, que é uma função sem parâmetros e você pode até repetir a distância ao longo da rede.\n",
        "\n",
        "[05:59] O Batch normalization você precisa reinstanciar cada vez que você quer usar, porque ela tem pesos, ela aprende pesos, então você vai ter um tipo de normalização na primeira convolução outro tipo de normalização na segunda convolução e daí por diante, já ativação não linear é a mesma ativação, é a mesma função Independente de onde que ela está ao longo da rede.\n",
        "\n",
        "[06:24] Os parâmetros das camadas só tem um porque a normalização é realizada de forma independente para cada canal, é importante lembrar que cada canal é um conjunto Independente de características do seu dado.\n",
        "\n",
        "[06:36] Então nós precisamos dizer para o Batch normalization quantos canais tem entrada, então se antes desse dessa camada tem uma convolução com 12 filtros, eu tenho que dizer para Batch normalization que ele vai receber um dado com 12 canais e ele vai normalizar cada canal de forma independente, e agora nós vamos no próximo vídeo implementar um bloco completo convolucional incluindo a normalização.\n",
        "\n",
        "Agora vamos construir um bloco convolucional completo e aplicar ele num dado para nós vermos como é que funciona. E esse momento é interessante para lembrarmos de um objeto do “pyTortch” que é útil para agregar essas camadas em um só lugar para não ter que ficar chamando camada por camada, na hora de usar o modelos que é o objeto do tipo de sequencial o que vai criar um módulo único com uma sequência de camadas.\n",
        "\n",
        "[00:32] Esse sequencial ele está dentro do pacote neuronetwork que é pacote NN do “PyTortch”, então importamos ele aqui em cima com from torch import nn, lembrando que nós estamos no mesmo script que fizemos o pooling em cima do dado da astronauta, na imagem da astronauta.\n",
        "\n",
        "[00:54] Já temos aqui o nosso tensor pronto para usar. Vamos definir o bloco convolucional blococonv = nn.Sequencial( vai ser um sequencial que o Google colab vai fazer o favor de colocar documentação aqui na nossa frente, que sequencial é um objeto tipo container onde você pode adicionar módulos que são as camadas convolucionais.\n",
        "\n",
        "[01:17] Então aqui tem até um exemplo de como que você constrói um sequencial é como se fosse uma lista de camadas, só que essa estrutura permite que você usa essa variável como o seu modelo que você conseguiu passar os dados no modelo, e ele vai passar na ordem camada por camada para produzir a saída do seu modelo.\n",
        "\n",
        "[01:38] Vamos definir um bloco convolucional, que vai começar com a convolução 2D, e essa convolução lembrando se nós estamos trabalhando com dado da astronauta é uma imagem RGB que possui três canais, colocamos 3 canais de entrada, nn.Conv2d(3, 32), a quantidade de canais de saída vai ser quantos filtros eu quero na minha camada que eu vou colocar 32.\n",
        "\n",
        "[02:08] Então 32 Canais, falta só definir agora o tamanho do “Kernel” eu quero um “Kernel” três nn.Conv2d(3, 32, kernel_size=3), que é o tamanho padrão na maioria das redes,três por três e vou colocar padding=1.\n",
        "\n",
        "[02:23] Que aí eu não perco resolução na característica de saída, depois o que eu faço agora criar minha camada de Batchnorm 2D porque eu estou trabalhando com dado, com duas dimensões e esse Batch Normalization o único parâmetro que eu preciso alimentar que é obrigatório é quantos canais vão entrar, então se a minha camada anterior produziu 32 mapas de ativação eu vou dizer para o Batch normalization que ele tem 32 Canais para normalizar.\n",
        "\n",
        "[02:57] Próxima camada vai ser ativação não linear, que é o nosso nn.Relu() por fim eu vou fazer que o nn.Maxpool2d para fechar o meu bloco convolucional , que recebe como entrada o tamanho do “Kernel” , a partir do tamanho ele já vai inferir um stride de mesmo tamanho. Podemos até imprimir esse bloco para vermos os paramentos que ficaram implícitos, por exemplo no MaxPool vemos que Kernel size é 2, o stride também é 2.\n",
        "\n",
        "[03:39] Se eu fizesse um Kernel size de 10 o stride também seria 10 porque ele vai acompanhar o tamanho do Kernel. O que ele vai fazer a essa altura nós já devemos conseguir inferir é reduzir a resolução espacial da nossa imagem 10 vezes, certo?\n",
        "\n",
        "[04:00] Então vamos pegar o nosso tensor e imprimir aqui o tamanho do tensor para nós vermos como ele vai ser modificado ao longo da rede, vou passar o tensor no nosso bloco convolucional print(rgb_tns.size()), saida = blococonv(rgb_tns) e vou imprimir o tamanho da saída print(saída.size()).\n",
        "\n",
        "[04:32] Nós demos como entrada uma imagem com três canais de tamanho 512 x 512 e como saída a nós tivemos uma característica 32 Canais, 32 mapas de ativação em resolução espacial 5151 porque o meu “maxpooling” reduziu em 10 vezes é resolução espacial. É importante falar que o Batch Normalization não vai mudar em nada a dimensionalidade do dado, a única coisa que ele faz é normalizar internamente esses dados mas não tem nenhuma alteração.\n",
        "\n",
        "[05:05] Então se eu tirar essas camadas seguintes, porque nós vamos ter uma saída de mesmo tamanho que a própria convolução, então a convolução ela vai produzir uma saída de 1 x 32 por 512 x 512 é o Batch normalization vai só normalizar internamente esses valores. O batch normalization, lembrando ele normaliza ao longo do batch, então aqui meu batch size é igual a 1 então meio que não faz muito sentido usar o batch normalization com uma única imagem.\n",
        "\n",
        "[05:41 ] Então o ideal é que por exemplo, nós tivéssemos um batch maior, eu produzisse aqui um mini batch, eu vou concatenar várias vezes o mesmo tensor, então eu vou produzir um mini Batch com 12 e imagens, digamos assim e ele vai produzir uma saída também com 12 características, uma característica para cada imagem que eu dei de entrada.\n",
        "\n",
        "[06:15] Então é nessa situação onde nós temos o tamanho de Batch maior que um, e o Batch normalization vai acrescentar efetivamente o que ele faz a normalização ao longo do batch. Então esse aqui é o nosso bloco convolucional, vamos agora construir redes convolucionais mais completas e soluções mais completas.\n",
        "\n",
        "\n",
        "A biblioteca do PyTorch implementa as principais funções de Pooling utilizadas em redes neurais. Explore a documentação e substitua o pooling utilizado no script Outras_Camadas.ipynb por outras funções 2D, como o AvgPool2d ou o AdaptiveMaxPool2d. Note que esse último não leva como parâmetro o tamanho do kernel, mas sim o tamanho da saída desejada.\n",
        "\n",
        "A alteração deve ser feita na célula que define o Pooling na variável pool\n",
        "\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=1)COPIAR CÓDIGO\n",
        "Dica: ao usar Poolings adaptativos (AdaptiveMaxPool2d), varie o tamanho da entrada e analise o efeito disso na saída.\n",
        "\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "Para realizar a substituição da função de Pooling, basta trocar o nome MaxPool2d pela função do seu interesse, dentre as listadas na documentação. Por exemplo:\n",
        "\n",
        "pool = nn.AdaptiveMaxPool2d(output_size=(7,7))COPIAR CÓDIGO\n",
        "Note que o pooling adaptativo permite que, independentemente do tamanho da entrada, a saída esteja de acordo com o tamanho definido na função. É preciso tomar cuidado ao usar esse Pooling, pois se o tamanho da entrada for muito maior que a saída desejada haverá grande perda de informação.\n",
        "\n",
        "Camada de Pooling\n",
        "Visualizando ativações hierárquicas\n",
        "Camada de Batch Normalization\n",
        "Implementação de um bloco convolucional em PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "MqlRg8Pc-k9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora falaremos um pouco de estratégias de treino de uma rede convolucional, e assim, a principal estratégia de todas é o treinamento do zero, onde você constrói a sua própria rede ou usa algum modelo pré-carregado do \"PyTorch\", e faz todo processo de treinamento que vamos ver até o final dessa última aula.\n",
        "\n",
        "[00:25] Só que além do treinamento do zero, se estabeleceu para redes convolucionais que à medida que foram sendo treinados cada vez modelos melhores e mais robustos, que elas também são muito úteis de outras formas, que através da extração de características e do finetunning.\n",
        "\n",
        "[00:43]Isso é válido para qualquer modelo recorrente, MLP, convolucional, mas acabou se tornando especialmente popular para redes convolucionais pela robustez dos modelos que existem já pré treinados, disponíveis online.\n",
        "\n",
        "[01:01] Então, a princípio vou me preocupar em implementar uma CNN do zero completa para nós treinarmos ela mais um pouco, então vamos importar aqui algumas bibliotecas são referentes a implementação da rede, treinamento deixaremos para o próximo vídeo, então, aqui são só as coisas da implementação e ao final desse vídeo vamos também já mostrar os dados carregados que vão ser usados no treinamento.\n",
        "\n",
        "[01:31] Então vamos lá, a primeira coisa que temos que fazer é implementar uma CNN, e para implementarmos um modelo que seja mais tranquilo de entender, sem muitas operações complexas, eu estou usando aqui a primeira CNN da história, a primeira bem-sucedida da história, que foi definida em 1998 e até hoje ela faz um bom trabalho em tarefas simples.\n",
        "\n",
        "[01:56] É uma rede de pouca capacidade, mas ela é muito melhor que outras estratégias né, por ser uma CNN, ela acaba sendo melhor do que estratégias que não usam CNN, que usam MLP, por exemplo, e aqui a implementaremos com base nessa tabela, que nos explica todas as operações que são realizadas, basicamente vamos colocar aqui a convolução e a ativação, que tratamos como camadas diferentes, na implementação com \"PyTorch\", e coloca também a operação de pooling.\n",
        "\n",
        "[02:32] Mas aqui introduziremos operações de normalização de batch ao longo dessas camadas, para criarmos blocos convolucionais mais robustos, então nosso objetivo aqui com essa rede é fazer classificação de objetos, mais para frente quando carregarmos os dados eu vou mostrar para vocês, que estamos construindo exatamente essa rede que vai receber como entrada uma imagem \"32x32\", com três canais de cor e vai dar como saída 10 probabilidades, então vai fazer uma classificação de categoria de objetos.\n",
        "\n",
        "[03:11] Então, vamos lá implementar a nossa CNN, a primeira camada, a única diferença em relação essa tabela é que trabalharemos com imagens com 3 canais, imagens coloridas, então ao invés da imagem ter um feature map, um canal, ela vai ter três.\n",
        "\n",
        "[03:27] Então, vamos começar a definir nossa primeira camada convolucional, vamos definir um bloco convolucional, que vai ter seis filtros, vai produzir seis mapas de características, a convolução vai ter o tamanho do kernel de \"5x5\", stride = 1, e, ativação, tangente hiperbólica, você pode perguntar: \"está faltando a informação do padding aqui\", mas essa informação acaba estando implícita, por que ele dá a informação do tamanho das saídas na nossa rede.\n",
        "\n",
        "[04:00] Então a imagem vai ter \"32x32\", a primeira convolução vai produzir uma saída \"28x28\", então tem uma redução de resolução aqui de 4 pixels, então se o kernel_size é \"5x5\", significa que não tem padding, ele vai reduzir a dimensionalidade da imagem com esse kernel, se você preenche aquela fórmula vocês vão ver que com padding = 0, temos essa perda de resolução aqui.\n",
        "\n",
        "[04:30] Então vamos lá, aqui construiremos o nosso modelo a partir de um objeto sequencial, então sabemos que a primeira convolução vai receber uma imagem colorida, vai produzir 6 filtros, porque é o que está escrito aqui em cima, o kernel é \"5x5\", kernel_size = 5, o stride = 1, já é o valor padrão então não precisa preencher, e o padding = 0, também é o valor padrão, não preciso preencher.\n",
        "\n",
        "[05:05] Então aqui ele vai receber como entrada uma imagem batch por três canais de cor por \"32x32\", # entrada (b, 3, 32, 32), e essa convolução vai produzir uma saída que vai ser batch por 6, que é o número de mapas que vai produzir, por \"28x28\", porque meu kernel_size = 5, #saida (b, 6, 28, 28).\n",
        "\n",
        "[05:35] Irá fazer a normalização de batch, que recebe 6 canais, vai ativar com a tangente hiperbólica que é específica da \"LeNet\", e vai fazer um average de pooling com kernel_size \"2x2\", que vai reduzir a imagem de \"28x28\" para \"14x14\", quando eu falo imagem na verdade é o mapa de características, não estamos trabalhando mais com a imagem, kernel_size = 2, implicitamente sabemos que o stride também é dois, mas já está escrito aqui.\n",
        "\n",
        "[06:12] Beleza, então temos aqui nosso primeiro bloco convolucional, agora iremos construir o próximo bloco convolucional, vai ter como entrada (b, 6, 14, 14), e vai dar como saída (10, 10, 16) \"Feature Maps\", então aqui temos uma convolução que vai receber 6 canais de entrada, vai produzir 16 mapas de ativação, e tem um kernel = 5.\n",
        "\n",
        "[07:13] De novo, em relação à entrada essa característica perdeu 4 pixels de cada dimensão, então significa que o kernel_size é \"5x5\", sem nenhum padding, normalização do batch tamanho 16, tangente hiperbólica e average de pooling que vai receber uma # entrada (b, 16, 10, 10), e vai produzir uma saída reduzindo ele pela metade, então já posso prever que é \"5x5\", # saida (b, 16, 5, 5), a saída que vai ser produzida por esse é average pooling.\n",
        "\n",
        "[07:48] Temos mais uma convolução sem pooling, então vamos fazer mais um bloco convolucional aqui, que vai receber como # entrada (b, 16, 5, 5), e vai dar como saída se o kernel_size = 5, ainda vai reduzir 4 pixels de cada dimensão \"1x1\", e vai produzir 120 \"Feature Maps\", então ele vai ter como entrada 16 canais que veio da característica anterior, vai produzir 120 canais e a saída aqui desse kernel = 5 vai ser # saída (b, 120, 1, 1).\n",
        "\n",
        "[08:33] Vamos fazer o batch normalization para 120 canais, ativação tangente que está escrito aqui, ativação tangente hiperbólica, e agora precisamos alimentar camadas totalmente conectadas, aí temos um problema, porque essas camadas recebem vetores de uma única dimensão, além da dimensão, do batch.\n",
        "\n",
        "[08:54] Para conseguirmos alimentar a camada não-linear, a entrada precisa ser # entrada (b, N), então, antes de alimentarmos a nossa camada linear, precisamos de uma camada de flatten, e essa camada vai linearizar o resultado.\n",
        "\n",
        "[09:19] Então se o nosso resultado é (b, 120, 1, 1), esse \"N\" aqui vai ser igual a (b, N=120*1*1), então é uma entrada na camada totalmente conectada igual a 120, que vai produzir uma saída igual a 84.\n",
        "\n",
        "[09:43] Então significa que tem 84 neurônios nessa camada, então aqui eu vou criar uma camada totalmente conectada que vai ter como entrada a saída dessa convolução linearizada, que passou pelo flatten, ou seja, vai receber como entrada 120 valores, e vai dar como saída 84 valores, significa que essa camada tem 84 neurônios.\n",
        "\n",
        "[10:13] Tem uma ativação tangente hiperbólica e a última camada linear que vai produzir os 10 valores referentes a distribuição de probabilidades para 10 categorias, aqui colocaremos uma ativação tangente hiperbólica, como a arquitetura pede, a saída produzida vai ser (b, 84) e a nossa última camada linear vai ser nn.Linear(84, 10), que são os 10 valores de probabilidade.\n",
        "\n",
        "[11:04] Acabamos a nossa rede, ela já está instanciada aqui em cima, então eu posso só imprimir essa rede e se fizemos tudo certo, está aqui o nosso modelo a \"LeNet 5\", a primeira arquitetura convolucional que foi efetiva na história.\n",
        "\n",
        "[11:34] Então ela vai produzir no final 10 valores de probabilidade de cada classe, do dataset que usaremos, o que é o \"DataSet SeeFar\", eu vou passar rapidamente, porque, já vimos o carregamento de dados no início desse curso, mas, já está aqui baixado o \"SeeFar\", estão aqui impressas algumas imagem para vocês lembrarem dele, conversamos sobre ele no início, e o carregamento de batches através do data loader.\n",
        "\n",
        "[12:02] Lembrando que no curso de introdução deve ter faltado o carregamento de algo, que no curso de \"Treinando Redes Neurais com PyTorch\", falamos bastante de carregamento de dados, do papel do data loader, do que que é um batch, então, em resumo o data loader, ele vai fazer esse carregamento de todo seu conjunto de dados em pequenos batches, que você vai alimentar para rede, mas, se você não está bem familiarizado com o conceito dataloader, vale voltar no curso.\n",
        "\n",
        "[12:38] Então é isso que queremos classificar, esse conjunto de dados que possui 10 classes de objetos e nós já construímos a nossa rede que vai transformar uma imagem de três canais em 10 valores de uma distribuição de probabilidades, e no próximo vídeo treinaremos esse modelo.\n",
        "\n",
        "Intercalar camadas de convolução e pooling é a composição mais utilizada nas arquiteturas convolucionais, visto que explora da melhor forma a eficiência trazida pelo pooling, bem como a invariância à translação.\n",
        "\n",
        "\n",
        " Agora que implementamos a nossa \"CNN\", já fez passo a passo aqui na implementação, faz sentido falarmos de hiperparâmetros que vão ser necessários para fazer o treinamento do zero, lembrando que tem um curso inteiro aqui na Alura, sobre \"Treinando Redes Neurais com PyTorch\" que vai falar com muito mais detalhes cada um dos elementos que eu vou mostrar aqui.\n",
        "\n",
        "[00:27] Mas vou dar uma passada rapidamente no treinamento de redes convolucionais para mostrar que não muda, do jeito que treinamos \"MLP\", é o mesmo jeito que vamos treinar \"CNN\", e mostrar elementos em que, caso, você já tenha feito o curso daremos uma revisada.\n",
        "\n",
        "[00:47] Então vou fazendo à medida que for necessário, preenchendo os hiperparâmetros, a primeira coisa que precisamos definir é qual dispositivo de hardware que usaremos, ou seja, se vai usar \"CPU\" ou \"GPU\".\n",
        "\n",
        "[01:00] Aqui no ambiente do collab, você tem como definir as configurações do seu notebook, que no caso eu já defini para aceleração com \"GPU\", mas se você colocar none, que é a configuração padrão do collab, ele vai está usando só \"CPU\" no processamento.\n",
        "\n",
        "[01:18] Para o que o código estar generalizável, possa ser usado em qualquer máquina, fazemos um pequeno if torch.cuda.is_available ():, para perguntar se o dispositivo cuda, que é o dispositivo de \"GPU\", está disponível, perguntamos se tem cuda disponível nessa máquina, se tem eu vou definir no meu dicionário de hiperparâmetros que o dispositivo que eu vou usar é args ['device']=torch.device ('cuda').\n",
        "\n",
        "[01:57] Se não tem cuda disponível, o dispositivo que eu vou usar é \"CPU\", e aqui eu imprimo qual é o device que eu estou usando print(args['device']), ele vai me dizer que eu estou usando o dispositivo cuda, que é \"GPU\".\n",
        "\n",
        "[02:13] Quando definimos a nossa rede, algo que não fizemos foi subir ela no dispositivo de hardware, então é necessário que se você vai usar \"GPU\", ou tanto faz, para você deixar o seu código fácil de usar em qualquer máquina, você pode subir seu dispositivo, sua rede, sua função de perda, do dispositivo de hardware definido, então fazemos isso com a função to, vamos subir agora nossa rede no device quer foi definido lá em cima.\n",
        "\n",
        "[02:49] O que significa que se não tivesse \"GPU\" disponível, o nosso if aqui ia tomar conta disso e ia dizer que se não tem cuda disponível, device é \"CPU\", na hora que você faz isso net = net.to(args[‘device’]), que ele vai subir na \"CPU\" a sua rede que na verdade é onde ela já está, mas dessa forma você deixa o código fácil de usar em qualquer computador.\n",
        "\n",
        "[03:16] Agora subimos aqui a rede na \"GPU\" e imprime ela de novo, daqui por diante agora a segunda coisa que precisamos é definir o tamanho do batch, para o carregamento de dados, então eu mostrei para vocês aqui o carregamento de dados com \"SeeFar\", mostrei alguns plots das imagens, mas, na hora de carregar esses dados em conjuntos de batchs.\n",
        "\n",
        "[03:47] Então dou um OK aqui, defini o meu batch_size, e agora quando eu rodar o código do dataloader, já tenho um batch_size definido no carregamento, se você quiser saber mais detalhes de dataloader, recomendo o curso.\n",
        "\n",
        "[04:09] Aqui eu coloquei o único import que precisamos para fazer o treinamento é o import optim, que é a biblioteca de otimização, então é onde estão os otimizadores que estão implementados no \"PyTorch\", e o resto aqui é só para fazer plots e análises do resultado, como por exemplo, a acurácia do que vamos estar avaliando.\n",
        "\n",
        "[04:31] Então primeiro eu vou fazer aqui a definição da função de perda que é a nossa métrica de qualidade da rede, então cada vez que eu passar uma imagem na minha rede, ela vai me dar uma distribuição de probabilidades com 10 classes, que foi o que definimos aqui, a saída final da nossa rede, são 10 características, que é a distribuição de probabilidades para cada uma das classes.\n",
        "\n",
        "[04:58] Então eu vou dizer que uma vez que eu tenho essa distribuição de probabilidades, eu tenho o rótulo da minha classe, o que eu vou fazer com essas duas coisas, a função de perda que usamos para problemas de classificação, que é o nosso caso, queremos classificar objetos é a cross-entropy, que a entropia cruzada.\n",
        "\n",
        "[05:21] Então definimos que o nosso criterion, nossa função de perda, vai ser nn, que é a biblioteca de redes neurais .CrossEntropyLoss, criterion = nn.CrossEntropyLoss().to(args['device']).\n",
        "\n",
        "[05:33] Eu não preciso definir nada, só preciso subir no dispositivo de hardware, que no nosso caso é a \"GPU\", quando e também vale a pena perceber que essa função de perda vai mudar, se por exemplo, você estiver lidando com problema de regressão, no caso da repressão a usamos o meansquare, que é os mínimos quadrados, usamos a \"L1\", enfim, é um estudo muito amplo de qual função de perda você vai usar.\n",
        "\n",
        "[06:03] E o otimizador do algoritmo que vai atualizando os pesos a cada interação para conseguir chegar no conjunto de pesos ideal para uso da sua rede, então nós definimos que o nosso optimizer = optim.Adam, ele nós fornece uma lista e o mais usado, atualmente, é o Adam, devido ao seu desempenho.\n",
        "\n",
        "[06:38] Então eu preciso alimentar para o otimizador o parâmetro da minha rede e preciso alimentar também a taxa de aprendizado, no caso do Adam, se a nós alimentarmos a taxa de aprendizado e o weight_decay, para regularização já é suficiente porque os outros parâmetros padrão dele já são muito bons.\n",
        "\n",
        "[07:01] Então preciso definir aqui qual vai ser a taxa de aprendizado e qual vai ser weight_decay, para definir elas eu vou voltar lá no dicionário de hiperparâmetros e vou definir aqui em cima, eu quero que a minha taxa de aprendizado seja por exemplo \"10e -3\".\n",
        "\n",
        "[07:23] E eu quero que a minha regularização seja “5e - 4\", pode ser assim, como eu cheguei nesses valores, por que eu defini esses valores, por que eu já testei antes esse problema e eu vi que esse conjunto de parâmetros funciona muito bem.\n",
        "\n",
        "[07:50] Uma dica para você escolher a taxa de aprendizado e o decaimento de pesos, que é o regularizador, é você ir multiplicando de 10 em 10, então por exemplo, para encontrar o hiperparâmetro certo, é um trabalho bem custoso, mas é importantíssimo para você conseguir entregar um bom modelo, faz muita diferença se eu coloco \"1e - 4\" ou \"1e - 3\", do quão rápido vai convergir e do quão bom vai ser o resultado final.\n",
        "\n",
        "[08:23] E o último hiperparâmetro é quantas épocas eu quero, ou seja, quantas vezes a minha rede vai ver o conjunto de dados, por que eu preciso ver o conjunto de dados mais de uma vez para conseguir fazer o treinamento.\n",
        "\n",
        "[08:35] Então vou colocar aqui epoch_num, eu quero 100 épocas, isso também tem que ser definido olhando o resultado, olhando quantas épocas demora para sua rede convergir, então é um trabalho muito de observação no comportamento da sua rede, e esses hiperparâmetros podem ser otimizados através de um grid_search.\n",
        "\n",
        "[09:00] Também tem curso explicando essas funções aqui na Alura, mas a depender da aplicação, se você fizer um estudo observando os resultados, já consegue definir uns valores bacanas.\n",
        "\n",
        "[09:16] Então, agora aqui podemos dizer que o meu otimizador vai ter aquela taxa de aprendizado que eu defini lá em cima e aquela taxa de regularização, que eu defini lá em cima também.\n",
        "\n",
        "[09:35] Então já temos a rede implementada, já temos os dados carregados, a loss e o otimizador definidos, agora vamos para a parte que interessa de implementação do fluxo de treinamento.\n",
        "\n",
        " E agora eu vou implementar o passo a passo do curso de treinamento, vou complementar do zero uma versão simples e vou mostrar a versão completa para vocês, calculando accuracy e tudo mais, então basicamente o treinamento consiste nesses passos aqui, então eu vou iterar nas épocas, que é quantas vezes eu vou ver todo o conjunto de dados.\n",
        "\n",
        "[00:20] Iterar nas épocas que é quantas vezes eu vou ver no conjunto de dados, iterar nos batches, que é, a cada época eu vou ver de quantos em quantos dados, então vou ver 50 em 50 dados, esse datset do \"SeeFar\" tem 60 mil dados de treinamento.\n",
        "\n",
        "[00:34] Então a cada época eu vou ver todos os 60 mil dados e a cada iteração eu vou ver 50 dados, então quantas iterações eu vou fazer, eu vou fazer 60 mil dividido por 50 que é o tamanho do batch, eu vou fazer 1200 iterações a cada época, quantas épocas eu vou fazer, eu defini 100 lá em cima.\n",
        "\n",
        "[00:58] Então vamos lá, eu vou iterar for epoch in range(epoch_num): for batch in train_loader:, aqui tem uma versão mais completa, para cada época e número de épocas, para cada batch no meu dataloader, que vai me dar a batch por batch.\n",
        "\n",
        "[01:32] O que eu vou fazer, eu vou jogar os dados no dispositivo de hardware, no mesmo dispositivo que está a rede, então o batch, se lembramos bem, é uma dupla: dado, rotulo = batch, vou jogar o dado na \"GPU\": dado = dado.to(args['device']), vou jogar o rótulo na \"GPU\": rotulo= rotulo.to(args['device']), depois eu vou fazer o forward na rede e o cálculo da Loss.\n",
        "\n",
        "[02:03] Então como fazemos o forward na rede, chamamos nossa rede, então ypred = net(dado), vamos imprimir print(dado.size()), ver por enquanto como está, vou dar um break aqui para ele fazer uma iteração só , e a nós vemos o resultado.\n",
        "\n",
        "[02:47] Então, a entrada é \"50\", que é o tamanho do meu batch, por \"3\", que são canais da imagem, por \"32x32\", que é altura e largura da imagem, a saída, são 50 inferências, que é o tamanho do meu batch, por \"10\", já que são 10 classes e tem uma probabilidade para cada classe, então aqui é o forward do dado e ele vai me dar a saída.\n",
        "\n",
        "[03:13] Fiz aqui o forward na rede, vou calcular a Loss a partir da minha variável loss = criterion(ypred, rotulo), essa função eu vou eu vou calcular a minha loss, vai ser a função critérion, que no caso é o cálculo da cross-entropy entre a predição e o rótulo, uma vez que eu calculei a minha loss, isso aqui eu vou fazer tanto no treino quanto a validação, vou fazer o forward e vou calcular a qualidade da minha rede.\n",
        "\n",
        "[03:54] Agora, uma coisa que só fazemos no treinamento é a parte de otimização, que é zerar o gradiente do otimizador, calcular o gradiente e atualizar os pesos, isso é um conjunto simples de funções, a primeira coisa que eu preciso fazer é zerar o gradiente, para ele gerar um novo fluxo a partir dessa nova iteração, mais detalhes estão lá no curso, vou fazer loss.backward, ou seja, derive as operações que eu fiz até agora e calcule o gradiente e atualize os pesos com base nessas derivadas.\n",
        "\n",
        "[04:35] Então, muitos perguntam, são duas variáveis completamente diferentes o critério e o otimizador e elas não estão conectadas entre si, verdade, acaba sendo um processo muito transparente esse processo de treinamento, porque o que acontece é que tem gráfico computacional interno, uma vez que você define os parâmetros.\n",
        "\n",
        "[04:56] Então, ele está definido dentro desse código um gráfico computacional internamente, quando você dá o loss.backward, ele deriva esse gráfico computacional e adiciona a informação nessa estrutura que está transparente para nós.\n",
        "\n",
        "[05:16] Quando você fala optim.step, ele vai nesse mesmo gráfico computacional que está transparente para nós, e pega os valores da derivada para calcular a direção que ele vai atualizar os pesos, apesar dessas duas coisas não estarem conectadas, elas vão consultar a mesma estrutura, que é o gráfico computacional.\n",
        "\n",
        "[05:39] Então eu posso imprimir aqui o valor da loss de forma bem porca, só para vermos que isso aqui está funcionando, eu vou imprimir aqui ao final de uma iteração, de uma época, eu vou acumular as perdas da época, epoch_loss.append(loss.detach.item()), então vou desacoplar aquele dado do gráfico computacional, e no final eu vou imprimir a média desse valor, o erro que ele deu aqui é porque eu chamei o optim que é o nome da biblioteca, e não a variável que é o optimizer.\n",
        "\n",
        "[06:54] Então a variável aqui em cima chama optimizer, eu vou pegar essa variável, zerar o gradiente dela que é o nosso otimizador e dar um step, então aqui a cada final de época ele está imprimindo a média das losses do treinamento, e o que vamos ver que é a cada época esse valor dessa loss vai diminuir, então isso significa que o treinamento está convergindo, porque a cada nova época que essa interação aqui, o valor da loss está sendo reduzido.\n",
        "\n",
        "[07:38] Então, isso já é uma medida de que o treinamento está convergindo, está funcionando, que a função de perda que calcula a entropia entre a predição e o rótulo está diminuindo, está tendendo a 0, então agora vamos ver uma versão mais arrumada com cálculo de acurácia, para conseguirmos ver um treinamento mais completo, e ver a diferença entre treino e validação.\n",
        "\n",
        "[08:03] Então tem aqui um gabarito que já está rodando há algum tempo, que faz exatamente a mesma coisa, só que eu dividi em uma função de treino e em uma função de validação, então tem alguns detalhes que são melhor explorados no outro curso, que você definiu o modo da sua rede, no treinamento você define net.train, na validação você define net.eval, tem essas diferenças.\n",
        "\n",
        "[08:29] No treinamento você faz a otimização que é o zero_grad, o step, na validação você não faz, você só faz o cálculo da loss e as predições, e aqui tem um código para fazer o cálculo da acurácia também, então estou salvando as predições e os rótulos que a minha rede previram.\n",
        "\n",
        "[08:52] Então, basicamente essa predição eu acho que vale a pena a nós dar uma olhada, eu vou pegar o máximo valor dentre todas as predições, acho que vale até a nós dar uma explorada lá no nosso código que que foi feito do zero, o que eu vou fazer aqui eu vou pegar esse ypred, para calcular qual é a classe que ele escolheu e ele me dá o máximo valor.\n",
        "\n",
        "[09:21] E ele me dá a localização do valor máximo, se eu fizer o torch.max(ypred, axis=1), porque vou imprimir de novo, e aqui eu vou mostrar tanto o ypred da posição zero, um dos elementos que eu passei na rede, quanto o max_v[0],idx_max[0], mostrar aqui para a nós ver.\n",
        "\n",
        "[09:58] Aí eu tenho que dar um break break de novo porque senão vamos ter que esperar alguns segundos para ele terminar de rodar, o que aconteceu, minha predição é \"50x10\", que é uma distribuição de probabilidades e quando eu tiro o valor máximo da minha predição, eu estou pegando dentre as 10 probabilidades, qual for a probabilidade maior, então ele vai me dar aqui os valores e os índices desse máximo valor.\n",
        "\n",
        "[10:47] Então, tem um exemplo aqui, eu tenho essa distribuição de ativações para um determinado dado, qual é a maior ativação aqui, é esse \"2.6\", então eu vou pegar o maior valor é \"2.6\", e ele está na posição \"8\", isso significa que o rótulo oito foi o que recebeu uma maior probabilidade.\n",
        "\n",
        "[11:09] Então na hora de calcular a acurácia e eu posso imprimir aqui também o rótulo para vermos, na hora de calcular a acurácia, o rótulo da posição 0 era dois, mas a minha predição foi dois também, eu acertei aqui até porque eu já estou treinando isso aos poucos.\n",
        "\n",
        "[11:35] Eu previ dois, o rótulo é dois, então na hora de calcular a acurácia, eu vou comparar esses dois valores, assim que está sendo feito o cálculo aqui de acurácia, e, aqui a mesma coisa na validação eu calculo a acurácia e aqui já tem um treinamento sendo feito então eu vou ver que a minha loss, ela vai reduzir o treinamento \"1.6\", \"1.4\", \"1.3\", \"1.2\", \"1.25\", ela vai desacelerando, mas ela vai diminuindo isso acontece também com a acurácia, 40%, 48%, 52%, 54%, até convergir.\n",
        "\n",
        "[12:17] E você vê que a loss do treinamento e da validação, elas vão reduzir a um ritmo diferente, a acurácia de cada um vai aumentar a um ritmo diferente, até que elas venham a convergir e só para finalizar eu vou interromper aqui a execução, conseguimos aqui as losses de treinamento e de teste, e eu consigo imprimir com matplotlib.\n",
        "\n",
        "[12:45] Então se eu imprimir quem é esse train_losses, é uma lista com as perdas de cada iteração, se eu ploto isso no matplotlib, ele vai mostrar aqui para nós uma um comportamento de convergência, então no meu eixo \"y\", eu tenho a loss, a perda, e no meu eixo \"x\", eu tenho as épocas.\n",
        "\n",
        "[13:18] Então ele vai mostrar aqui para nós, que a loss ao longo das épocas foi tendendo a zero, foi reduzindo, então chegamos em um ponto aqui que a loss está quase chegando em zero, se eu imprimo a loss do teste também no mesmo plot, vamos ver um negócio muito interessante que é isso aqui olha só.\n",
        "\n",
        "[13:42] Isso aqui significa que a loss do treino está convergindo a \"0\", mas a loss do teste já saturou aqui em mais ou menos \"1\", significa que se essa loss começar a subir, eu estou especializando demais o meu modelo.\n",
        "\n",
        "[14:00] Mas isso é muito comum, o comportamento ideal é provavelmente o que vai acontecendo do início, as primeiras 30 interações, comportamento ideal significa que treinamos demais esse modelo, é o que acontece no início as losses vão decaindo, o treino decai mais suavemente e o teste decai mais desorganizadamente, mas as duas estão caindo.\n",
        "\n",
        "[14:29] Uma situação como a que vimos agora significa talvez que a taxa de aprendizado está muito alta ou que a nós treinou por épocas demais, mas o treinamento já não está sendo mais benéfico para o teste, que são os dados do mundo real, apesar do treino continuar convergindo, então é uma análise que vale a pena fazer;\n",
        "\n",
        "[14:50] No próximo vídeo vamos ver outras estratégias de treinamentos.\n",
        "\n",
        "gora que já sabemos como treinamos uma CNN do zero, outras estratégias de treino ficaram muito populares especialmente para modelos convolucionais, são estratégias que contam com o uso de aprendizado em outros domínios que significa que é com grandes e diversos datasets de imagem que existem atualmente, foi possível treinar modelos tão bons que eles até generalizam domínios que eles nunca viram antes.\n",
        "\n",
        "[00:32] Então eu posso treinar pegar o “Imaginet” que é um dataset de imagens mais populares atualmente.\n",
        "\n",
        "[00:39] Posso treinar um modelo bem treinado no “Imaginet” e por exemplo, só usar essa rede para extrair características e usar essas características para classificar um tumor no raio-x, classificar reconhecer faces, identificar pessoas e daí por diante.\n",
        "\n",
        "[01:02] Então tem alguns conhecimentos bem pontuais que ajudam muito na hora que nós vamos fazer essas outras estratégias de treinamento, então eu já deixei implementado a maior parte do código que a partir do que já conhecemos para poder focar nas partes que mais interessam. Então aqui tem só o que nós já sabemos que são os hiperparâmetros, definição de hardware, GPU, CPU e os imports.\n",
        "\n",
        "[01:26] Aqui em baixo tem uma breve explicação sobre qual que é o processo de extração de características que essencialmente consiste em pegar a rede pré treinada no conjunto diverso, é um modelo muito bom, muito bem treinado, arranca a camada que faz classificação que a camada final da rede, usa essa rede para extrair características e alimenta as características para um outro modelo de classificação.\n",
        "\n",
        "[02:00] Isso porque a qualidade da classificação é tão boa quanto as características que você alimenta e sabemos que essas redes pré treinadas refletem uma qualidade absurda de características.\n",
        "\n",
        "[ 02:14] Eu posso pegar essas características, treinar modelo de classificação como svm, como uma árvore de decisão com essas características, então são basicamente vamos aprender adaptar rede, extrair características e treinar um classificador comum.\n",
        "\n",
        "[02:30] Uma outra coisa muito importante é que você vai carregar esses dados, porque se vamos usar um modelo pré treinando em outro dataset é importante que tanto o carregamento quando o pré-processamento dos seus dados, do seu domínio sigam o treinamento do modelo original.\n",
        "\n",
        "[02:50 ]Então eu treinei o meu modelo no dataset A mas eu quero usar ele no dataset B o fato é que eu preciso fazer o mesmo pré-processamento em B que eu fiz em A, então eu preciso pré processar os meus dados que são CIFAR10 da mesma forma que o “ImageNet” foi processado quando o modelo foi treinado. O “PyTortch” nos mostra essa composição de transformações que foram utilizadas na hora de treinar os modelos que ele provê.\n",
        "\n",
        "[ 03:26] Basicamente vamos precisar de uma imagem de tamanho 224, transformar tensor e normalizar pela média desvio padrão dos dados do ImagiNet. Esse “horizontalflip” é para aumentar, chama “dataAumentation” para gerar dados sintéticos então não precisamos.\n",
        "\n",
        "[03:47] Vamos criar uma composição de transformações e vamos usar o Resize, a primeira transformação que vamos fazer é o Transforms.Resize(224) que é o tamanho dos dados do ImagiNet, depois vamos transformar para tensor Transforms.ToTensor() por fim essa transformação com base nessas médias e desvios-padrão que parecem ser números mágicos, e de fato são, o Pixel médio e o desvio padrão dos valores de pixel do ImagiNet.\n",
        "\n",
        "[04:32] Então esse é o pré-processamento que vamos fazer nos nossos dados. Aí eu coloco na hora de carregar os dados da transformação e carregar meus dados com essa nova transformação aqui.\n",
        "\n",
        "[04:50] Se você plotar esses dados que vamos plotar agora, você vai ver eles de uma forma que parece estourada, imagem parece ruim, inclusive dá até erro na hora de plotar esses dados, mas é só porque ele está normalizado em relação a outra distribuição que numericamente falando vai ser essencial para conseguirmos usar bem esse modelo, faz toda diferença normalizar de acordo com os dados do “ImagiNet”.\n",
        "\n",
        "[05:17] Aqui eu carrego meus dados, o próximo passo importantíssimo é adaptar a minha rede fazendo o quê? Removendo a última camada para que nós possa extrair características e não realizar classificação, então se escolhermos por exemplo, AVGG 16 que é um modelo que nós conseguimos entender com conhecimento que já adquirimos, ele só tem as camadas que já conhecemos.\n",
        "\n",
        "[05:43] Eu posso criar aqui minha rede pegando do pacote Models que é o pacote do TortchVision o modelo VGG16 com Batch normalization , só preciso dizer que eu quero ele pré treinado então eu quero net= models. vgg16_bn(pretreined = True).\n",
        "\n",
        "[06:06] Vamos ver que é um modelo que conseguimos entender muito bem. Ele vai baixar aqui e o modelo é composto de convolução, ativação não linear, convolução batch normalization, MaxPooling então é uma versão daquilo que fizemos aqui só que um pouco maior. Então como adaptamos isso? Nós precisamos remover pelo menos essa última camada aqui, que é a camada que produz as mil probabilidades, considerando que o ImagiNet tem mil classes.\n",
        "\n",
        "[06:41] E aí isso é muito importante para sempre que você precisar mexer nos seus modelos, alterar ele de alguma forma, você consegue acessar camadas pela função children().\n",
        "\n",
        "[06:50] Se eu imprimir aqui uma lista do net.childern(), ele vai me dizer quais são “os filhos” desse meu modelo, eu vou até imprimir na verdade o name do children que ele dá uma tupla com o nome e o módulo para vermos quantos childrens tem aqui na verdade só tem “três crianças”. Ele só tem três filhos que são feature o avgpool e o classifier.\n",
        "\n",
        "[07:26] Então só tem três elementos se eu imprimir o tamanho dele aqui ele só tem três elementos, então para fazermos essas alterações, precisamos alterar o net.classiFier, então chegamos nele, eu sei que oclassiFier eu quero remover está no “último filho”.\n",
        "\n",
        "[07:52] Então posso perguntar aqui (net.named_children())[-1]) e ele vai dar para mim só os módulos do classifier, se eu acessar o último elemento dele, todos menos o último da verdade, me dá todas as camadas exceto aquela última que era a camada que realizava classificação.\n",
        "\n",
        "[08:16] Eu posso também tirar o dropout tirar o ReLU porque ele não vai ser relevante para extração de características e deixar que a última camada seja sói essa linear, então eu quero todos do último filho até a posição -3, então ele vai me dar aqui até o linear aqui é um extrator de característica.\n",
        "\n",
        "[08:41] Sabendo acessar dessa forma, eu posso redefinir o meu net.classifier lembrando o último dos módulos da rede vgg16 de eu vou redefinir o meu net.classifier como um sequencial com base nessa lista que eu vou definir aqui. novo_classifier, ele vai ser aqui a minha nova rede, agora eu vou imprimir a rede de novo, só para ver o resultado.\n",
        "\n",
        "[09:18] Não mexi nas features elas estão idênticas, mas agora o classifier tem todas as camadas, menos aquelas últimas que eram responsáveis pela classificação. Para extrair características eu já deixei implementado aqui, que é basicamente um fluxo de validação precisamos organizar essas características como um array NumPy para poder alimentar para o classificador depois.\n",
        "\n",
        "[09:46] Então é um fluxo normal onde jogamos os dados na GPU, faz o ford na rede e só dá append dessa saída numa lista. A diferença é que o que vai sair daqui quando fizer o ford. Não são inferências mais, então a diferença está no significado do que vai sair dessa rede, não é mais a camada de inferência, ainda é camada de extração de características.\n",
        "\n",
        "[10:13] Como demora um pouco de rodar, se eu for extrair todas as características de todos os Batchs do dataloadereu já deixei aqui uma execução pronta, que extraiu 500 batchs do nosso dataloader e só para ficar claro essas características trend x e test x elas vão estar prontas para serem enviadas para o modelo de classificação do Scikit-Learn.\n",
        "\n",
        "[10:38] Que pede uma dimensão de número de amostras por quantidade de atributos, então isso aqui vai ter tamanho pro treino 50.000 é o número total de amostras de treino, por 4.096 que é o número de atributos da saída da nossa rede, para o test vai ser 10.000 que é o número de amostras de teste também por 4096 e aqui eu tenho os rótulos dos valores inteiros mesmo 123456 referentes a classe.\n",
        "\n",
        "[11:11] E para fazer o treinamento no caso dos modelos do scikit-learn vale muito a pena aprender como que usa o pacote do scikit-learn para fazer pequenas inferências como esse é o caso, que ele tem um padrão super simples que é baseada em duas funções, a função ifgt e a função predict.\n",
        "\n",
        "[11:35] Para qualquer modelo seja random forest, seja svm independentes se é Kernel linear você pode simplesmente experimentar diversos modelos de classificação instanciando modelo na primeira linha, na segunda linha dando features nos seus dados considerando os rótulos e depois fazendo as predições aqui com o predict.\n",
        "\n",
        "[11:57] Eu fiz exatamente isso com os nossos dados que eu extraí como um fluxo normal aqui de uso de uma rede neural, eu fiz o feat aqui com os meus dados de treino, rótulo de treino predict nos dados de treino, imprime acurácia que também uma função do Scikit-learn accuracy_score for eu consegui 85% sem nunca ter visto os dados do sifa para fazer extração de características.\n",
        "\n",
        "[12:30] Então não importa o svm não vai fazer milagre se as características fossem ruins da qualidade dos atributos fossem ruins, o svm não ia conseguir uma qualidade como essa de classificação.\n",
        "\n",
        "[12:43] Então significa que o trabalho pesado eu fiz na hora de treinar o modelo ImagiNet, no caso foi o PyTortch mas a feature que veio da rede neural, tinha boa qualidade então eu consigo treinar uma svm linear mesmo que o classificador e consegui uma boa aaccuracy então vale super a pena usar rede pré-treinadas como extrator de características que em muitos casos vai resolver seu problema sem você precisar ficar se debruçando em treinamento.\n",
        "\n",
        " Lembrando o que estamos fazendo aqui, estamos vendo outras estratégias de treino que contam com modelos pré treinados e nessa aula a vamos ver um pouco sobre o fine-tunning, os conhecimentos essenciais do \"PyTorch\", que você para fazer o fine-tunning.\n",
        "\n",
        "[00:15] Lembrando rapidamente aqui que nós extraímos os dados de acordo com o pré processamento necessário para treinar o modelo com ImagiNet, então dando resize para 24, e normalizando em relação à distribuição do ImagiNet, isso é extremamente importante.\n",
        "\n",
        "[00:36] Mesmo que não estamos trabalhando com ImagiNet, precisamos fazer o mesmo pré processamento que o modelo pré treinado. Então o que vamos fazer aqui ao invés das extração de características é uma estratégia que se chama fine-tunning, que basicamente é o ajuste fino da sua rede, em geral consiste em três passos.\n",
        "\n",
        "[00:57] Primeiro você vai substituir a camada de classificação, por que a ideia aqui é você retreinar a própria rede, para usar essa rede pré treinada no seus dados, você vai aproveitar muito bem, você vai substituir a camada de classificação original da rede e substituir pela sua camada.\n",
        "\n",
        "[01:26] Então, se aqui eu estou fazendo detecção de objetos é que eu teria \"x1\", \"y1\", \"x2\", \"y2\", do Bounding Box do objeto, eu vou jogar fora essa camada de regressão no caso, e vou colocar a minha camada de classificação, que vai classificar tumores em benigno e maligno, então joguei fora uma camada, substitui por outra da minha preferência.\n",
        "\n",
        "[01:53] Uma outra coisa que eu posso fazer também é colocar múltiplas faixas de aprendizado ao longo dessa rede, então eu posso dizer por exemplo que as camadas iniciais vão receber uma taxa de aprendizado menor, vão ser menos treinadas e as camadas finais vão receber uma taxa de aprendizado maior, vão ser mais treinadas, então aqui vamos aprender como adaptamos a rede para o fine-tunning e como colocamos múltiplas faixas de aprendizado para fazer um treinamento.\n",
        "\n",
        "[02:23] Então, primeiro adaptando a rede, aqui de novo vamos carregar como modelo vgg16 com batch normalization, pré treinado, no caso aqui no Imaginet, imprimir a rede, a mesma vimos no vídeo passado, ele carrega a rede e está aqui, lembrando que ela tem três filhos, o \"Features\", o \"AvgPool\" e o \"ClassiFier\".\n",
        "\n",
        "[02:54] E agora o que queremos é jogar fora a camada linear dela que gera uma distribuição de 1000 probabilidades, e o nosso problema só tem 10 classes, então queremos uma distribuição de 10 probabilidades, então para isso vamos explorar de novo os filhos da nossa camada, então se eu converto para a lista aqui os filhos da minha camada, e eu pego os filhos list(net.children())[ -1], já conhecemos isso no vídeo anterior.\n",
        "\n",
        "[03:20] Eu vou pegar o \"ClassiFier\", o módulo com todas as camadas do \"ClassiFier\", uma coisa que eu primeiro preciso saber, é qual o tamanho da entrada dessa camada linear, porque eu vou ter que refazer uma nova camada aqui, e eu preciso saber o tamanho da entrada e o tamanho da saída, a saída eu sei que é \"10\", porque eu quero gerar 10 probabilidades, a entrada eu consigo acessar pelo último elemento.\n",
        "\n",
        "[03:59] Lembrando que aqui eu estou com o \"ClassiFier\", vou colocar na linha de baixo, o que eu vou fazer agora é a entrada receber isso daqui, -1.in_features, imprimir a entrada, ele vai me dizer que é \"4096\", eu vou imprimir tanto esse aqui, quanto a entrada para vermos o que é, eu peguei o último elemento do último filho da minha rede, que a camada de classificação, e peguei o tamanho da entrada aqui através do in_features.\n",
        "\n",
        "[04:49] Então aqui eu consigo saber o tamanho da entrada da nova camada que eu vou ter que criar, então vou definir aqui embaixo minha nova camada, para definir a minha nova camada eu vou ter que substituir todo o net.classifier, porque ele é um módulo só, é muito mais complicado você ficar fazendo pequenas alterações nos módulos, então a minha recomendação é você refazer o módulo sequencial do classificador e atribuir a sua rede depois.\n",
        "\n",
        "[05:24] Então, o meu \"ClassiFier\" aqui vai ser, net.classifier.children, transformo isso aqui numa lista, e o que eu tenho aqui no meu \"ClassiFier\", eu tenho uma lista com todas as camadas terminando aqui na camada de classificação, então agora eu quero a lista dos filhos do \"ClassiFier\", exceto o último elemento, ele vai me dar todos menos o último.\n",
        "\n",
        "[05:59] Como eu estou numa lista, eu posso usar a função append, então vou usar o classifer.append, e coloco aqui a minha camada nova que o tamanho da entrada, eu defini aqui em cima, e o tamanho da saída é \"10\", então seu eu imprimir agora meu \"ClassiFier\", ele tem todas as camadas que tinha antes Com a adição da minha camada que agora gera 10 probabilidades.\n",
        "\n",
        "[06:29] Tudo que eu preciso fazer agora é atribuir esse meu classificador à rede, então net.classifier vai receber uma estrutura tipo sequencial com base na minha lista que não preciso converter de novo, se eu imprimir a rede depois, ela está com um novo \"ClassiFier\", que produz agora, 10 probabilidades.\n",
        "\n",
        "[06:55] Então adaptei minha rede aqui pegando uma lista do que era antes, dando append na minha nova camada e transformando de volta para um módulo de rede neural, o último conhecimento que precisamos agora é como definir múltiplas taxas de aprendizado.\n",
        "\n",
        "[07:12] E eu coloquei aqui tanto a referência da documentação do \"PyTorch\", quanto a sintaxe padrão, que essencialmente definimos o otimizador da mesma forma que fazíamos antes , só que dessa vez criaremos uma lista de dicionários com os hiperparâmetros que queremos.\n",
        "\n",
        "[07:33] Então quais são os filhos da rede que queremos treinar, o nome que eles têm é model.features.parameters vai ter uma taxa de aprendizado e model.classifier, vai ter outra taxa de aprendizado e aqui colocamos a taxa de aprendizado padrão para as camadas que não estiverem incluídas nos dicionários, aqui tem que colocar.\n",
        "\n",
        "[08:01] Então vamos lá, eu vou definir uma taxa de aprendizado para a parte de aprendizado de características e outra para o classificador, como eu tenho lá no meu dicionário de argumentos a taxa de aprendizado, eu posso colocar aqui a args, taxa de aprendizado e aqui em cima por exemplo, eu posso colocar 20% da taxa de aprendizado que eu defini lá em cima.\n",
        "\n",
        "[08:27] O mesmo vale para o WeightDecay, eu vou colocar um decaimento de pesos que é argsweightdecay*0.2, que é 20% do WeightDecay que definimos lá em cima, se trocar o nome do model para net e o WeightDecay aqui embaixo vai ser todo o valor completo 100% do WeightDecay que eu defini no dicionário lá em cima.\n",
        "\n",
        "[09:10] Então estou definindo uma taxa de aprendizado menor e uma taxa de regularização menor para a minha base, que é a parte de aprendizado de características e para o \"ClassiFier\", eu defino a taxa de aprendizado maior, completa, então eu rodo aqui esse otimizador, rodo aqui a minha função de perda, e consigo fazer o fluxo de treinamento e validação igual a nós viu nas outras aulas.\n",
        "\n",
        "[09:38] Aqui eu já rodei o finetunnig que é exatamente igual, é o treinamento e a validação, só que você coloca a taxa de aprendizado diferentes para a base e para a ponta, assim sua base não vai perder o aprendizado que ela já tinha e você adapta a rede colocando as camadas de acordo com as suas necessidades, então quando você faz o finetunning no \"SeeFar\", logo na primeira iteração ele já alcança a acurácia de \"85\", \"89\", \"90\", e daí por diante.\n",
        "\n",
        "[10:11] E com cinco épocas, ele alcança uma acurácia de \"92%\" do teste, que está pau a pau com o que normalmente se consegue com a vgg no \"SeeFar\", então se você treinasse o vgg em todos os dados do \"SeeFar\" por várias épocas com a taxa de aprendizado mais alta, demorando muito mais tempo, você alcançaria o mesmo resultado que fazendo um finetunning breve de cinco épocas aqui.\n",
        "\n",
        "[10:39] Então, isso finaliza as estratégias de treino e espero que vocês tenham visto a grande variedade no uso de redes convolucionais, principalmente considerando que a área é muito ativa e está sempre saindo novos modelos tão bons ou melhores, quanto você treinar a sua rede do zero nos seus dados.\n",
        "\n",
        "\n",
        "\n",
        "No script Treino-do-zero.ipynb implementamos a arquitetura da LeNet 5, com base na tabela que descreve as camadas que compõem a rede e seus respectivos atributos (número de canais ou feature maps, tamanho da saída, tamanho do kernel e ativações). A imagem a seguir apresenta uma tabela similar, desta vez descrevendo a arquitetura da VGG11. Apesar da tabela constar 17 elementos de camada, apenas 11 deles são operações que produzem novas características, enquanto o pooling realiza apenas a subamostragem.\n",
        "\n",
        "\n",
        "\n",
        "Implemente a arquitetura da VGG11 descrita, considerando que todas as camadas convolucionais possuem stride=1 e padding=1, mantendo assim as dimensões espaciais da entrada, enquanto as camadas de pooling mantêm o valor padrão de stride (o mesmo tamanho do kernel).\n",
        "\n",
        "VER OPINIÃO DO INSTRUTOR\n",
        "Opinião do instrutor\n",
        "\n",
        "# Definindo a rede\n",
        "net = nn.Sequential(\n",
        "        ## ConvBlock 1\n",
        "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),      # entrada: (b, 3, 224, 224) e saida: (b, 64, 224, 224)\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 64, 224, 224) e saida: (b, 64, 112, 112)\n",
        "\n",
        "        ## ConvBlock 2\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),    # entrada: (b, 64, 112, 112) e saida: (b, 128, 112, 112)\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 128, 112, 112) e saida: (b, 128, 56, 56)\n",
        "\n",
        "        ## ConvBlock 3\n",
        "        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),   # entrada: (b, 128, 56, 56) e saida: (b, 256, 56, 56)\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),   # entrada: (b, 256, 56, 56) e saida: (b, 256, 56, 56)\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 256, 56, 56) e saida: (b, 256, 28, 28)\n",
        "\n",
        "        ## ConvBlock 4\n",
        "        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 256, 28, 28) e saida: (b, 512, 28, 28)\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 28, 28) e saida: (b, 512, 28, 28)\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 512, 28, 28) e saida: (b, 512, 14, 14)\n",
        "\n",
        "        ## ConvBlock 4\n",
        "        nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 14, 14) e saida: (b, 512, 14, 14)\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),   # entrada: (b, 512, 14, 14) e saida: (b, 512, 14, 14)\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),          # entrada: (b, 512, 14, 14) e saida: (b, 512, 7, 7)\n",
        "        nn.Flatten(),  # lineariza formando um vetor               # entrada: (b, 512, 7, 7) e saida: (b, 512*7*7) = (b, 25088)\n",
        "\n",
        "        ## DenseBlock\n",
        "        nn.Linear(25088, 4096),                                    # entrada: (b, 25088) e saida: (b, 4096)\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4096, 4096),                                     # entrada: (b, 4096) e saida: (b, 4096)\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4096, 10),                                       # entrada: (b, 4096) e saida: (b, 10)\n",
        "        nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "\n",
        "Implementando uma CNN com PyTorch\n",
        "Treinando uma CNN do zero\n",
        "CNN para extração de características\n",
        "Carregando modelos pré-treinados do Torchvision\n",
        "Fine-tuning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PNX6FNlu-hSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iut8ndeM-f6K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "No-tH84S-RAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNWg12Cy-Q2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCHfht8U-Qx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnHwcPqK-Qm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}